\documentclass[11pt,english]{article}

\usepackage{graphicx} % Required for inserting images
\usepackage[margin=2cm,top=2cm,headheight=16pt,headsep=0.2in,heightrounded]{geometry}
\usepackage{fancyhdr} % Required for inserting and customizing page header
    \pagestyle{fancy} % Required for changing page style
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{hyperref}
\usepackage[
backend=biber,
style=ieee,
]{biblatex}
\usepackage{caption}

\usepackage{tikz}
\usepackage{booktabs}
\usetikzlibrary{arrows}
\usetikzlibrary{tikzmark}
\usetikzlibrary{trees}

\usepackage{tabularx}
\usepackage{lscape}
\newcolumntype{b}{>{\hsize=2.80\hsize}X}
\newcolumntype{s}{>{\hsize=.4\hsize}X}


\usepackage{enumitem}
\usepackage{longtable}

\captionsetup[table]{name=Tabla}
\captionsetup[figure]{name=Figura}

\usepackage{amsthm}
\theoremstyle{plain}

\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}

\fancyhead{}
\fancyfoot{}

\fancyhead[L]{Tarea 4: Procesamiento de Lenguaje Natural}
\fancyfoot[L]{\thepage}

\addbibresource{biblio.bib} %Imports bibliography file

\renewcommand{\labelitemi}{{\tiny$\bullet$}}

\renewcommand*{\bibfont}{\normalfont\small}

\title{Procesamiento de Lenguaje Natural\\
Tarea 4
}
\author{
  Rayo Mosquera, Jhon Stewar\\
  \texttt{j.rayom@uniandes.edu.co}
  \and
  De La Rosa Peredo, Carlos Raul\\
  \texttt{c.delarosap@uniandes.edu.co}\and
  Mario Garrido Córdoba\\
   \texttt{m.garrido10@uniandes.edu.co}
  \\ 
}
\date{Octubre 2024}

\begin{document}

\maketitle

\section*{Punto I}

En nuestro notebook, procesamos una colección de libros de Jane Austen, Leo Tolstoy y James Joyce obtenidos del Proyecto Gutenberg, para entrenar embeddings de palabras usando Word2Vec de Gensim. Comenzamos leyendo el texto crudo de cada libro y aplicamos pasos de preprocesamiento como la eliminación de puntuación, la tokenización, la conversión a minúsculas y la eliminación de palabras vacías. Estos pasos aseguran que el corpus esté bien preparado para el entrenamiento de embeddings. Una vez preprocesado, guardamos el texto tokenizado para su uso futuro.

Después de procesar y tokenizar los libros, combinamos todos los textos en un solo corpus y entrenamos modelos Word2Vec con tres tamaños de vector: 50, 100 y 300 dimensiones. Configuramos el entrenamiento con parámetros como el tamaño de la ventana de contexto, la cantidad mínima de apariciones de una palabra y el número de épocas. Cada modelo entrenado se guarda en disco utilizando una convención de nombres específica que incluye nuestro código de grupo.

\section*{Punto II}

Cargamos modelos preentrenados de Word2Vec para los libros, enfocándonos en los personajes principales y recuperando las palabras más semánticamente similares a ellos. Después de extraer los vectores de palabras relevantes, reducimos su dimensionalidad y los graficamos en dos dimensiones para visualizarlos. También exploramos las relaciones temáticas de conceptos clave como ``amor'', ``guerra'', ``paz'', ``muerte'' y ``honor'' mediante razonamiento analógico, obteniendo una comprensión más profunda de los temas en estos libros.

\subsection*{Estrategia para Graficar Embeddings en Dos Dimensiones}

Usamos \textbf{t-SNE} (t-distributed Stochastic Neighbor Embedding) como método principal para reducir la dimensionalidad de los vectores de palabras generados por los modelos Word2Vec. Este método captura estructuras locales en los datos, asegurando que las palabras semánticamente cercanas se mantengan próximas en el gráfico bidimensional.

También hemos implementado \textbf{PCA} (Análisis de Componentes Principales) como alternativa, aunque t-SNE es nuestro método preferido debido a su efectividad en mantener relaciones sutiles entre palabras. Después de la reducción de dimensionalidad, creamos gráficos de dispersión donde cada punto representa una palabra, y su proximidad refleja similitud semántica. Los gráficos están anotados con etiquetas de palabras para interpretar visualmente las relaciones entre personajes y términos relacionados.

Esta estrategia nos permite visualizar cómo los personajes y sus palabras más similares se organizan en un espacio semántico de baja dimensionalidad, ayudándonos a identificar agrupaciones temáticas y analizar relaciones entre diferentes libros.

\subsection*{Discusión de las Relaciones Encontradas Usando Razonamiento Analógico}

Exploramos temas como ``amor'', ``guerra'', ``paz'', ``muerte'' y ``honor'' a través del razonamiento analógico, descubriendo lo siguiente:

\begin{itemize}
    \item \textbf{Amor}: El ``amor'' está asociado con palabras como ``casado'', ``celoso'', ``felicidad'' y ``encanto''. En Tolstói, vemos asociaciones específicas con personajes como ``Sonetchka'', mientras que Joyce se enfoca en aspectos más internos del amor, con términos como ``amante''.
    
    \item \textbf{Guerra}: En Tolstói, la ``guerra'' está vinculada a contextos históricos y nacionales, con palabras como ``patriótico'' y ``heroísmo''. En Austen, la ``guerra'' se usa más metafóricamente para representar conflictos sociales o personales, como ``sociedad'' y ``asunto''.
    
    \item \textbf{Paz}: La ``paz'' en Tolstói está asociada con luchas filosóficas, mientras que en Austen tiene un trasfondo emocional, vinculada a palabras como ``miserable'' y ``confianza''.
    
    \item \textbf{Muerte}: La ``muerte'' evoca asociaciones emocionales y espirituales fuertes, con palabras como ``dolor'', ``ángel'' y ``enfermedad''. En Tolstói, está más relacionada con reflexiones morales, mientras que en Austen se centra más en la pérdida personal.
    
    \item \textbf{Honor}: En Austen, el ``honor'' está relacionado con la reputación social y el comportamiento personal, mientras que en Tolstói está ligado al deber y la rivalidad. En Joyce, se asocia con conflictos internos y la identidad.
\end{itemize}

A través de estas analogías, logramos comprender mejor cómo cada autor trata estos temas universales de manera única, reflejando la riqueza y profundidad de sus obras literarias.

\section*{Punto III}

Entrenamos y evaluamos redes neuronales feed-forward (FFNN) para la atribución de autor utilizando embeddings de palabras preentrenados. Procesamos los datos, construimos tres modelos con diferentes complejidades y evaluamos su rendimiento con tres tamaños de embeddings (50, 100 y 300 dimensiones).

Comenzamos cargando y preprocesando los textos. Se extraen oraciones de 150 a 250 palabras y se almacenan en un DataFrame con las columnas \texttt{autor} y \texttt{oración}. Luego, dividimos los datos en conjuntos de entrenamiento (70\%), validación (15\%) y prueba (15\%). Utilizamos un tokenizador para convertir las oraciones en secuencias de enteros, y las secuencias se rellenan para garantizar una longitud uniforme.

A continuación, cargamos modelos Word2Vec preentrenados con 50, 100 y 300 dimensiones. Para cada tamaño de embedding, creamos una capa de embedding en Keras que mapea las palabras tokenizadas a vectores, y esta capa sirve como la entrada para los modelos.

Definimos tres arquitecturas FFNN:
\begin{itemize}
    \item \textbf{Modelo 1} (El más simple): Capa de embedding $\rightarrow$ Capa de flatten $\rightarrow$ Capa densa (128 unidades) $\rightarrow$ Capa de salida (3 unidades).
    \item \textbf{Modelo 2} (Intermedio): Capa de embedding $\rightarrow$ Capa de flatten $\rightarrow$ Capa densa (256, 128 unidades) $\rightarrow$ Capa de salida.
    \item \textbf{Modelo 3} (El más complejo): Capa de embedding $\rightarrow$ Capa de flatten $\rightarrow$ Capa densa (512, 256, 128 unidades) $\rightarrow$ Capa de salida.
\end{itemize}

Entrenamos y evaluamos cada modelo durante 10 épocas con un tamaño de lote de 32. Evaluamos cada combinación de modelo y tamaño de embedding en el conjunto de prueba, midiendo precisión, exactitud y \textit{recall}. El script muestra la arquitectura del modelo y los resultados de rendimiento final para cada configuración.

Al explorar diferentes arquitecturas FFNN y tamaños de embeddings para la atribución de autor, encontramos que los modelos más simples y los embeddings más pequeños tienden a generalizar mejor, siendo el Modelo 1 con embeddings de 50 dimensiones el que ofrece el mejor rendimiento general.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Modelo} & \textbf{Tamaño del Embedding} & \textbf{Exactitud} & \textbf{Precisión} & \textbf{Recall} \\ \hline
Modelo 1        & 50                            & 91.32\%            & 91.14\%            & 87.16\%         \\ \hline
Modelo 2        & 50                            & 91.32\%            & 90.43\%            & 87.87\%         \\ \hline
Modelo 3        & 50                            & 88.84\%            & 87.15\%            & 84.26\%         \\ \hline
Modelo 1        & 100                           & 90.91\%            & 88.80\%            & 87.41\%         \\ \hline
Modelo 2        & 100                           & 90.08\%            & 89.24\%            & 85.81\%         \\ \hline
Modelo 3        & 100                           & 88.84\%            & 86.52\%            & 85.53\%         \\ \hline
Modelo 1        & 300                           & 88.84\%            & 88.14\%            & 84.29\%         \\ \hline
Modelo 2        & 300                           & 89.67\%            & 88.01\%            & 85.72\%         \\ \hline
Modelo 3        & 300                           & 87.60\%            & 86.11\%            & 84.96\%         \\ \hline
\end{tabular}
\caption{Comparación de resultados de los modelos FFNN con diferentes tamaños de embeddings.}
\end{table}

\subsection*{Resumen de Resultados}

A partir de la evaluación, observamos que el Modelo 1 (la arquitectura más simple) con embeddings de 50 dimensiones logra el mejor rendimiento general, con una exactitud del 91.32\%, precisión del 91.14\% y \textit{recall} del 87.16\%. Esto sugiere que una arquitectura más simple combinada con embeddings de menor dimensionalidad generaliza mejor a los datos.

A medida que aumenta la complejidad del modelo (por ejemplo, el Modelo 3 con 512 unidades), el rendimiento tiende a disminuir. El Modelo 3 con embeddings de 50 dimensiones obtiene una exactitud ligeramente inferior (88.84\%), precisión (87.15\%) y \textit{recall} (84.26\%). Esto indica que las arquitecturas más complejas son más propensas al sobreajuste, especialmente cuando los datos no son lo suficientemente grandes para justificar los parámetros adicionales.

Al aumentar las dimensiones de los embeddings (100 y 300), notamos una ligera disminución en el rendimiento de todos los modelos. Por ejemplo, el Modelo 1 con embeddings de 100 dimensiones alcanza una exactitud del 90.91\%, mientras que el Modelo 3 con embeddings de 300 dimensiones baja al 87.60\%. Esto sugiere que los embeddings de mayor dimensionalidad pueden añadir ruido o complejidad que los modelos luchan por generalizar efectivamente.

\subsection*{Análisis}

Los resultados muestran claramente que los modelos más simples con embeddings de menor dimensionalidad son más adecuados para la tarea de atribución de autor con este conjunto de datos. El Modelo 1 supera consistentemente a las arquitecturas más complejas, y los embeddings de 50 dimensiones proporcionan el rendimiento más equilibrado en términos de exactitud, precisión y \textit{recall}. Aumentar la complejidad del modelo o el tamaño del embedding genera rendimientos decrecientes, probablemente debido al sobreajuste o la dificultad añadida de generalizar con datos de mayor dimensionalidad.

Para mejorar el rendimiento, podríamos explorar técnicas como la regularización (por ejemplo, \textit{dropout} o regularización L2) para reducir el sobreajuste en los modelos más complejos. Además, la validación cruzada proporcionaría una estimación más confiable del rendimiento del modelo en diferentes divisiones de los datos. Sin embargo, basándonos en estos resultados, el Modelo 1 con embeddings de 50 dimensiones es la combinación más efectiva para esta tarea de atribución de autor.

\end{document}
