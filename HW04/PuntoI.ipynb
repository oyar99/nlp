{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9497d0-2845-42c2-9733-81c370db11b7",
   "metadata": {},
   "source": [
    "# Procesamiento de Libros y Entrenamiento de Modelos Word2Vec\n",
    "\n",
    "Este notebook procesa una colección de libros de tres autores (i.e., Jane Austen, Leo Tolstoy y James Joyce), aplicando preprocesamiento de texto y entrenando modelos Word2Vec con diferentes dimensiones vectoriales para capturar relaciones semánticas entre las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebea8bd-3192-4134-a39e-8517c0a2ec6e",
   "metadata": {},
   "source": [
    "## 0. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f60225d-f83c-47bc-a580-6ce6e710db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0b02a-05fc-4057-a178-2d3bd62aa9d4",
   "metadata": {},
   "source": [
    "## 1. Definición de Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e21e6ee-7488-43f5-99cc-f2bca28eb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_gensim(text):\n",
    "    \"\"\"\n",
    "    Limpia y tokeniza el texto mediante:\n",
    "    1. Eliminación de puntuación y caracteres especiales.\n",
    "    2. Convierte el texto a minúsculas.\n",
    "    3. Tokenización del texto en palabras.\n",
    "    4. Eliminación de palabras vacías (stopwords).\n",
    "    \n",
    "    Args:\n",
    "    text (str): Texto de entrada a preprocesar.\n",
    "    \n",
    "    Returns:\n",
    "    list: Una lista de tokens (palabras) del texto limpiado.\n",
    "    \"\"\"\n",
    "    # Eliminar cualquier carácter no alfabético, números, etc.\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenizar y convertir el texto a minúsculas\n",
    "    tokens = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "    \n",
    "    # Eliminar palabras vacías (stopwords)\n",
    "    tokens = [word for word in tokens if word not in STOPWORDS]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def prepare_corpus_gensim(text):\n",
    "    \"\"\"\n",
    "    Divide el texto en oraciones y aplica el preprocesamiento a cada oración.\n",
    "    \n",
    "    Args:\n",
    "    text (str): Texto de entrada para dividir y preprocesar.\n",
    "    \n",
    "    Returns:\n",
    "    lista de listas: Oraciones tokenizadas, donde cada oración es una lista de tokens.\n",
    "    \"\"\"\n",
    "    # Dividir el texto en oraciones (dividiendo por puntos o saltos de línea)\n",
    "    sentences = text.split('.')    # Alternativamente, usar '\\n' para párrafos u otros delimitadores\n",
    "    \n",
    "    # Preprocesar cada oración usando el preprocesamiento de Gensim\n",
    "    tokenized_sentences = [preprocess_text_gensim(sentence) for sentence in sentences if sentence]\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def process_single_book(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Lee un archivo de libro, preprocesa el texto y guarda el corpus tokenizado.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Ruta al archivo del libro sin procesar.\n",
    "    output_path (str): Ruta para guardar los datos preprocesados.\n",
    "    \"\"\"\n",
    "    # Leer el texto del libro\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Preprocesar y preparar el corpus\n",
    "    corpus = prepare_corpus_gensim(text)\n",
    "    \n",
    "    # Guardar el corpus tokenizado en un archivo para uso futuro\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Carga el corpus tokenizado preprocesado desde un archivo pickle.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Ruta a los datos preprocesados.\n",
    "    \n",
    "    Returns:\n",
    "    list of lists: El corpus tokenizado.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    return corpus\n",
    "\n",
    "def train_word2vec(corpus, vector_size, window, min_count, epochs, save_path):\n",
    "    \"\"\"\n",
    "    Entrena un modelo Word2Vec usando Gensim y guarda el modelo entrenado en un archivo.\n",
    "    \n",
    "    Args:\n",
    "    corpus (list of lists): El corpus tokenizado.\n",
    "    vector_size (int): Dimensión de los vectores de palabras.\n",
    "    window (int): Tamaño de la ventana de contexto.\n",
    "    min_count (int): Número mínimo de veces que una palabra debe aparecer para ser incluida.\n",
    "    epochs (int): Número de épocas de entrenamiento.\n",
    "    save_path (str): Ruta para guardar el modelo entrenado.\n",
    "    \n",
    "    Returns:\n",
    "    gensim.models.Word2Vec: Modelo Word2Vec entrenado.\n",
    "    \"\"\"\n",
    "    # Entrenar el modelo Word2Vec\n",
    "    model = Word2Vec(sentences=corpus, vector_size=vector_size, window=window, min_count=min_count, sg=1, epochs=epochs)\n",
    "    \n",
    "    # Guardar el modelo entrenado en un archivo\n",
    "    model.save(save_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50540701-56bb-444f-95b2-b06bc04b7a4c",
   "metadata": {},
   "source": [
    "## 2. Procesamiento de los Libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f90f1b2-bdd1-4d21-a1d9-e322f6e5dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando austen_sense-and-sensibility...\n",
      "Procesando austen_pride-and-prejudice...\n",
      "Procesando austen_emma...\n",
      "Procesando tolstoy_youth...\n",
      "Procesando tolstoy_war-and-peace...\n",
      "Procesando tolstoy_anna-karenina...\n",
      "Procesando joyce_dubliners...\n",
      "Procesando joyce_a-portrait-of-the-artist-as-a-young-man...\n",
      "Procesando joyce_ulysses...\n"
     ]
    }
   ],
   "source": [
    "# Ruta a los libros originales\n",
    "raw_books = {\n",
    "    'austen_sense-and-sensibility': {\n",
    "        'file_path': 'data/raw/austen_sense-and-sensibility.txt',\n",
    "        'output_path': 'data/processed/austen_sense-and-sensibility.pkl'\n",
    "    },\n",
    "    'austen_pride-and-prejudice': {\n",
    "        'file_path': 'data/raw/austen_pride-and-prejudice.txt',\n",
    "        'output_path': 'data/processed/austen_pride-and-prejudice.pkl'\n",
    "    },\n",
    "    'austen_emma': {\n",
    "        'file_path': 'data/raw/austen_emma.txt',\n",
    "        'output_path': 'data/processed/austen_emma.pkl'\n",
    "    },\n",
    "    'tolstoy_youth': {\n",
    "        'file_path': 'data/raw/tolstoy_youth.txt',\n",
    "        'output_path': 'data/processed/tolstoy_youth.pkl'\n",
    "    },\n",
    "    'tolstoy_war-and-peace': {\n",
    "        'file_path': 'data/raw/tolstoy_war-and-peace.txt',\n",
    "        'output_path': 'data/processed/tolstoy_war-and-peace.pkl'\n",
    "    },\n",
    "    'tolstoy_anna-karenina': {\n",
    "        'file_path': 'data/raw/tolstoy_anna-karenina.txt',\n",
    "        'output_path': 'data/processed/tolstoy_anna-karenina.pkl'\n",
    "    },\n",
    "    'joyce_dubliners': {\n",
    "        'file_path': 'data/raw/joyce_dubliners.txt',\n",
    "        'output_path': 'data/processed/joyce_dubliners.pkl'\n",
    "    },\n",
    "    'joyce_a-portrait-of-the-artist-as-a-young-man': {\n",
    "        'file_path': 'data/raw/joyce_a-portrait-of-the-artist-as-a-young-man.txt',\n",
    "        'output_path': 'data/processed/joyce_a-portrait-of-the-artist-as-a-young-man.pkl'\n",
    "    },\n",
    "    'joyce_ulysses': {\n",
    "        'file_path': 'data/raw/joyce_ulysses.txt',\n",
    "        'output_path': 'data/processed/joyce_ulysses.pkl'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Procesar cada libro\n",
    "for book, paths in raw_books.items():\n",
    "    print(f\"Procesando {book}...\")\n",
    "    process_single_book(paths['file_path'], paths['output_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0b7d5-fd7f-4bc2-91a6-ef7e0bb6b90b",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento de los Modelos Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10065cf-5298-4478-aa11-6d5fa223a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo combinado con 50 dimensiones...\n",
      "Entrenando modelo combinado con 100 dimensiones...\n",
      "Entrenando modelo combinado con 300 dimensiones...\n"
     ]
    }
   ],
   "source": [
    "# Ruta a los libros procesados\n",
    "processed_books = {\n",
    "    'austen_sense-and-sensibility': 'data/processed/austen_sense-and-sensibility.pkl',\n",
    "    'austen_pride-and-prejudice': 'data/processed/austen_pride-and-prejudice.pkl',\n",
    "    'austen_emma': 'data/processed/austen_emma.pkl',\n",
    "    'tolstoy_youth': 'data/processed/tolstoy_youth.pkl',\n",
    "    'tolstoy_war-and-peace': 'data/processed/tolstoy_war-and-peace.pkl',\n",
    "    'tolstoy_anna-karenina': 'data/processed/tolstoy_anna-karenina.pkl',\n",
    "    'joyce_dubliners': 'data/processed/joyce_dubliners.pkl',\n",
    "    'joyce_a-portrait-of-the-artist-as-a-young-man': 'data/processed/joyce_a-portrait-of-the-artist-as-a-young-man.pkl',\n",
    "    'joyce_ulysses': 'data/processed/joyce_ulysses.pkl'\n",
    "}\n",
    "\n",
    "# Código de nuestro grupo\n",
    "group_code = 'CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba'\n",
    "\n",
    "# Cargar y combinar todos los corpus de los libros\n",
    "combined_corpus = []\n",
    "for file_path in processed_books.values():\n",
    "    corpus = load_processed_data(file_path)\n",
    "    combined_corpus.extend(corpus)    # Añadir cada corpus al corpus combinado\n",
    "\n",
    "# Entrenamiento del modelo combinado con diferentes tamaños de vectores\n",
    "vector_sizes = [50, 100, 300]  # Dimensiones de los embeddings a probar\n",
    "window_size = 5  # Tamaño de la ventana de contexto\n",
    "min_count = 3    # Solo considerar palabras que aparezcan al menos 3 veces\n",
    "epochs = 10      # Número de épocas de entrenamiento\n",
    "\n",
    "# Entrenar los modelos Word2Vec de 50, 100 y 300 dimensiones de todos los libros\n",
    "for vector_size in vector_sizes:\n",
    "    print(f\"Entrenando modelo combinado con {vector_size} dimensiones...\")\n",
    "    \n",
    "    # Guardar el modelo combinado con el tamaño de vector especificado\n",
    "    model_save_path = f\"data/models/Books_{vector_size}_{group_code}.model\"\n",
    "    \n",
    "    # Entrenar el modelo con el tamaño de vector especificado\n",
    "    model = train_word2vec(combined_corpus, vector_size=vector_size, window=window_size, min_count=min_count, epochs=epochs, save_path=model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
