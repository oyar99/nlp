{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de texto\n",
    "\n",
    "Este notebook presenta la creación de un dataset de oraciones con su respectivo autor, con el que luego se entrenan varios clasificadores usando variaciones de redes feed-forward, y distintos embeddings pre-entrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerías necesarias para el desarrollo del proyecto\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creación de dataset\n",
    "\n",
    "Primero, creamos el dataset de oraciones etiquetadas según el autor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función que carga un archivo de texto y lo devuelve como un string\n",
    "def load_raw_data(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Carga el texto crudo a partir de un archivo de texto\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Ruta del archivo de texto.\n",
    "    \n",
    "    Returns:\n",
    "    str: Texto crudo.\n",
    "    \"\"\"\n",
    "    # Leer el texto crudo\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función que extrae las oraciones del texto según el formato de los libros de Gutenberg\n",
    "def extract_sentences(book: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extrae extractos de un libro asegurando que cumplan con ciertas condiciones de tamaño\n",
    "    \n",
    "    Args:\n",
    "    book (str): Texto crudo.\n",
    "    \n",
    "    Returns:\n",
    "    list[str]: Lista de extractos del libro.\n",
    "    \"\"\"\n",
    "    # Separar el texto en bloques usando líneas completamente vacías como delimitadores\n",
    "    # Seleccionar el tercer bloque que contiene el contenido del libro\n",
    "    lines = book.split('***')[2].split('\\n\\n')\n",
    "\n",
    "    # Eliminar espacios en blanco al inicio y al final de cada línea\n",
    "    lines = [line.strip() for line in lines]\n",
    "\n",
    "    # Eliminar lineas vacias\n",
    "    lines = [line for line in lines if line]\n",
    "\n",
    "    # Eliminar lineas genericas como ilustraciones o titulos de los capitulos de los libros\n",
    "    lines = [line for line in lines if not (\n",
    "        line.startswith('CHAPTER')) or \n",
    "        line.startswith('[Illustration]')\n",
    "    ]\n",
    "\n",
    "    # Eliminar saltos de lineas de las oraciones\n",
    "    lines = [line.replace('\\n', ' ') for line in lines]\n",
    "\n",
    "    # Solo procesar lineas con mas de 150 caracteres\n",
    "    lines = [line for line in lines if len(line) >= 150]\n",
    "\n",
    "    # Separar adicionalmente por . si la oracion es muy larga\n",
    "    sentences = []\n",
    "    for sentence in lines:\n",
    "        if len(sentence) > 250:\n",
    "            sentences.extend(sent_tokenize(sentence))    # Dividir en oraciones usando NLTK\n",
    "        else:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    # Solo procesar lineas con mas de 150 y menos de 250 caracteres\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) >= 150 and len(sentence) <= 250]\n",
    "\n",
    "    # Eliminar espacios en blanco al inicio y al final de cada línea nuevamente\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta a los libros originales junto con su autor\n",
    "raw_books = {\n",
    "    'austen_sense-and-sensibility': {\n",
    "        'file_path': 'data/raw/austen_sense-and-sensibility.txt',\n",
    "        'author': 'Jane Austen',\n",
    "    },\n",
    "    'austen_pride-and-prejudice': {\n",
    "        'file_path': 'data/raw/austen_pride-and-prejudice.txt',\n",
    "        'author': 'Jane Austen',\n",
    "    },\n",
    "    'austen_emma': {\n",
    "        'file_path': 'data/raw/austen_emma.txt',\n",
    "        'author': 'Jane Austen',\n",
    "    },\n",
    "    'tolstoy_youth': {\n",
    "        'file_path': 'data/raw/tolstoy_youth.txt',\n",
    "        'author': 'Leo Tolstoy',\n",
    "    },\n",
    "    'tolstoy_war-and-peace': {\n",
    "        'file_path': 'data/raw/tolstoy_war-and-peace.txt',\n",
    "        'author': 'Leo Tolstoy',\n",
    "    },\n",
    "    'tolstoy_anna-karenina': {\n",
    "        'file_path': 'data/raw/tolstoy_anna-karenina.txt',\n",
    "        'author': 'Leo Tolstoy',\n",
    "    },\n",
    "    'joyce_dubliners': {\n",
    "        'file_path': 'data/raw/joyce_dubliners.txt',\n",
    "        'author': 'James Joyce',\n",
    "    },\n",
    "    'joyce_a-portrait-of-the-artist-as-a-young-man': {\n",
    "        'file_path': 'data/raw/joyce_a-portrait-of-the-artist-as-a-young-man.txt',\n",
    "        'author': 'James Joyce',\n",
    "    },\n",
    "    'joyce_ulysses': {\n",
    "        'file_path': 'data/raw/joyce_ulysses.txt',\n",
    "        'author': 'James Joyce',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Their estate was large, and their residence wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>The late owner of this estate was a single man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>The son, a steady respectable young man, was a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>To him therefore the succession to the Norland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Their mother had nothing, and their father onl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author                                           sentence\n",
       "0  Jane Austen  Their estate was large, and their residence wa...\n",
       "1  Jane Austen  The late owner of this estate was a single man...\n",
       "2  Jane Austen  The son, a steady respectable young man, was a...\n",
       "3  Jane Austen  To him therefore the succession to the Norland...\n",
       "4  Jane Austen  Their mother had nothing, and their father onl..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se crea un dataframe con las oraciones extraídas de los libros\n",
    "df = pd.DataFrame(columns=['author', 'sentence'])\n",
    "\n",
    "# Por cada libro, se extra el texto y se concatenan las oraciones en el dataframe\n",
    "for book in raw_books.values():\n",
    "    corpus = load_raw_data(book['file_path'])\n",
    "    author = book['author']\n",
    "    \n",
    "    # Extraer las oraciones del texto\n",
    "    sentences = extract_sentences(corpus)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'author': author, 'sentence': sentences})], ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataset como un archivo CSV\n",
    "df.to_csv('data/classifier/sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>num_training_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leo Tolstoy</td>\n",
       "      <td>10514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>3745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James Joyce</td>\n",
       "      <td>2815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author  num_training_data\n",
       "0  Leo Tolstoy              10514\n",
       "1  Jane Austen               3745\n",
       "2  James Joyce               2815"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contar el número de datos de entrenamiento por cada autor\n",
    "author_counts = df['author'].value_counts()\n",
    "\n",
    "# Convertir los conteos en un DataFrame\n",
    "summary_df = author_counts.reset_index()\n",
    "summary_df.columns = ['author', 'num_training_data']\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento\n",
    "\n",
    "Preprocesamos el dataset separandolo en entrenamiento y prueba. Adicionalmente, tokenizamos el texto para poder mapear las palabras a los embeddings construidos y usarlos como la capa de entrada de los modelos de redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['sentence'], df['author'],\n",
    "                                                    train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6165    [wine, gluttony, idleness, laziness, irritabil...\n",
       "7583    [old, stateliness, cleanliness, stillness, rei...\n",
       "1052    [letters, town, days, nerve, elinors, body, th...\n",
       "4038    [soft, road, phaeton, pursuing, tenour, way, b...\n",
       "3405    [weston, walked, purpose, tired, sit, time, re...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizacion y preprocesamiento\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Limpia y tokeniza el texto mediante:\n",
    "    1. Eliminación de puntuación y caracteres especiales.\n",
    "    2. Convierte el texto a minúsculas.\n",
    "    3. Tokenización del texto en palabras.\n",
    "    4. Eliminación de palabras vacías (stopwords).\n",
    "    \n",
    "    Args:\n",
    "    text (str): Texto de entrada a preprocesar.\n",
    "    \n",
    "    Returns:\n",
    "    list: Una lista de tokens (palabras) del texto limpiado.\n",
    "    \"\"\"\n",
    "    # Eliminar cualquier carácter no alfabético, números, etc.\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenizar y convertir el texto a minúsculas\n",
    "    tokens = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "    \n",
    "    # Eliminar palabras vacías (stopwords)\n",
    "    tokens = [word for word in tokens if word not in STOPWORDS]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "x_train = x_train.apply(preprocess_text)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamos las palabras a los embeddings de texto pre-entrenados\n",
    "# TODO: Cargar el modelo de embeddings de texto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
