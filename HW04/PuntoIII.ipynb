{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de Texto\n",
    "\n",
    "Este notebook presenta la creación de un dataset de oraciones con su respectivo autor, con el que luego se entrenan varios clasificadores usando variaciones de redes feed-forward y distintos embeddings pre-entrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerías necesarias para el desarrollo del proyecto\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creación del Dataset\n",
    "\n",
    "Primero, creamos el dataset de oraciones etiquetadas según el autor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función que carga un archivo de texto y lo devuelve como un string\n",
    "def load_raw_data(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Carga el texto crudo a partir de un archivo de texto\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Ruta del archivo de texto.\n",
    "    \n",
    "    Returns:\n",
    "    str: Texto crudo.\n",
    "    \"\"\"\n",
    "    # Leer el texto crudo\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función que extrae las oraciones del texto según el formato de los libros de Gutenberg\n",
    "def extract_sentences(book: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extrae extractos de un libro asegurando que cumplan con ciertas condiciones de tamaño\n",
    "    \n",
    "    Args:\n",
    "    book (str): Texto crudo.\n",
    "    \n",
    "    Returns:\n",
    "    list[str]: Lista de extractos del libro.\n",
    "    \"\"\"\n",
    "    # Separar el texto en bloques usando líneas completamente vacías como delimitadores\n",
    "    # Seleccionar el tercer bloque que contiene el contenido del libro\n",
    "    lines = book.split('***')[2].split('\\n\\n')\n",
    "\n",
    "    # Eliminar espacios en blanco al inicio y al final de cada línea\n",
    "    lines = [line.strip() for line in lines]\n",
    "\n",
    "    # Eliminar lineas vacias\n",
    "    lines = [line for line in lines if line]\n",
    "\n",
    "    # Eliminar lineas genericas como ilustraciones o titulos de los capitulos de los libros\n",
    "    lines = [line for line in lines if not (\n",
    "        line.startswith('CHAPTER')) or \n",
    "        line.startswith('[Illustration]')\n",
    "    ]\n",
    "\n",
    "    # Eliminar saltos de lineas de las oraciones\n",
    "    lines = [line.replace('\\n', ' ') for line in lines]\n",
    "\n",
    "    # Solo procesar lineas con mas de 150 caracteres\n",
    "    lines = [line for line in lines if len(line) >= 150]\n",
    "\n",
    "    # Separar adicionalmente por . si la oracion es muy larga\n",
    "    sentences = []\n",
    "    for sentence in lines:\n",
    "        if len(sentence) > 250:\n",
    "            sentences.extend(sent_tokenize(sentence))    # Dividir en oraciones usando NLTK\n",
    "        else:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    # Solo procesar lineas con mas de 150 y menos de 250 caracteres\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) >= 150 and len(sentence) <= 250]\n",
    "\n",
    "    # Eliminar espacios en blanco al inicio y al final de cada línea nuevamente\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta a los libros originales junto con su autor\n",
    "raw_books = {\n",
    "    'austen_sense-and-sensibility': {\n",
    "        'file_path': 'data/raw/austen_sense-and-sensibility.txt',\n",
    "        'author': 'Jane Austen',\n",
    "    },\n",
    "    'austen_pride-and-prejudice': {\n",
    "        'file_path': 'data/raw/austen_pride-and-prejudice.txt',\n",
    "        'author': 'Jane Austen',\n",
    "    },\n",
    "    'austen_emma': {\n",
    "        'file_path': 'data/raw/austen_emma.txt',\n",
    "        'author': 'Jane Austen',\n",
    "    },\n",
    "    'tolstoy_youth': {\n",
    "        'file_path': 'data/raw/tolstoy_youth.txt',\n",
    "        'author': 'Leo Tolstoy',\n",
    "    },\n",
    "    'tolstoy_war-and-peace': {\n",
    "        'file_path': 'data/raw/tolstoy_war-and-peace.txt',\n",
    "        'author': 'Leo Tolstoy',\n",
    "    },\n",
    "    'tolstoy_anna-karenina': {\n",
    "        'file_path': 'data/raw/tolstoy_anna-karenina.txt',\n",
    "        'author': 'Leo Tolstoy',\n",
    "    },\n",
    "    'joyce_dubliners': {\n",
    "        'file_path': 'data/raw/joyce_dubliners.txt',\n",
    "        'author': 'James Joyce',\n",
    "    },\n",
    "    'joyce_a-portrait-of-the-artist-as-a-young-man': {\n",
    "        'file_path': 'data/raw/joyce_a-portrait-of-the-artist-as-a-young-man.txt',\n",
    "        'author': 'James Joyce',\n",
    "    },\n",
    "    'joyce_ulysses': {\n",
    "        'file_path': 'data/raw/joyce_ulysses.txt',\n",
    "        'author': 'James Joyce',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Their estate was large, and their residence wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>The late owner of this estate was a single man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>The son, a steady respectable young man, was a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>To him therefore the succession to the Norland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>Their mother had nothing, and their father onl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author                                           sentence\n",
       "0  Jane Austen  Their estate was large, and their residence wa...\n",
       "1  Jane Austen  The late owner of this estate was a single man...\n",
       "2  Jane Austen  The son, a steady respectable young man, was a...\n",
       "3  Jane Austen  To him therefore the succession to the Norland...\n",
       "4  Jane Austen  Their mother had nothing, and their father onl..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se crea un dataframe con las oraciones extraídas de los libros\n",
    "df = pd.DataFrame(columns=['author', 'sentence'])\n",
    "\n",
    "# Por cada libro, se extra el texto y se concatenan las oraciones en el dataframe\n",
    "for book in raw_books.values():\n",
    "    corpus = load_raw_data(book['file_path'])\n",
    "    author = book['author']\n",
    "    \n",
    "    # Extraer las oraciones del texto\n",
    "    sentences = extract_sentences(corpus)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame({'author': author, 'sentence': sentences})], ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataset como un archivo CSV\n",
    "df.to_csv('data/classifier/sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>num_training_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leo Tolstoy</td>\n",
       "      <td>10514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>3745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James Joyce</td>\n",
       "      <td>2815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author  num_training_data\n",
       "0  Leo Tolstoy              10514\n",
       "1  Jane Austen               3745\n",
       "2  James Joyce               2815"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contar el número de datos de entrenamiento por cada autor\n",
    "author_counts = df['author'].value_counts()\n",
    "\n",
    "# Convertir los conteos en un DataFrame\n",
    "summary_df = author_counts.reset_index()\n",
    "summary_df.columns = ['author', 'num_training_data']\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento del Dataset\n",
    "\n",
    "Preprocesamos el dataset separandolo en entrenamiento y prueba. Adicionalmente, tokenizamos el texto para poder mapear las palabras a los embeddings construidos y usarlos como la capa de entrada de los modelos de redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['sentence'], df['author'],\n",
    "                                                    train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización usando Keras\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Convertir el texto en secuencias de enteros\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Rellenar las secuencias para que tengan la misma longitud\n",
    "max_length = max([len(seq) for seq in x_train_seq])\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=max_length, padding='post')\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definición de los Modelos de Redes Neuronales\n",
    "\n",
    "Cargamos los los embeddings de Word2Vec pre-entrenados, creamos las capas de embeddings a partir de ellos, y definimos los tres tipos de arquitecturas de redes neuronales que usaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta a los modelos Word2Vec combinados con diferentes tamaños de vectores\n",
    "books_models = [\n",
    "    'data/models/Books_50_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba.model',\n",
    "    'data/models/Books_100_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba.model',\n",
    "    'data/models/Books_300_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba.model'\n",
    "]\n",
    "\n",
    "# Cargar los embeddings de Word2Vec pre-entrenados\n",
    "word2vec_model_50 = gensim.models.Word2Vec.load(books_models[0])\n",
    "word2vec_model_100 = gensim.models.Word2Vec.load(books_models[1])\n",
    "word2vec_model_300 = gensim.models.Word2Vec.load(books_models[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word2vec_model, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Crea una capa de embeddings a partir de un modelo Word2Vec y un tokenizer.\n",
    "\n",
    "    Args:\n",
    "    word2vec_model: Modelo Word2Vec preentrenado.\n",
    "    tokenizer: Tokenizer que contiene el índice de palabras.\n",
    "    max_length (int): Longitud máxima de las secuencias de entrada.\n",
    "\n",
    "    Returns:\n",
    "    Embedding: Capa de embedding de Keras que utiliza la matriz de embeddings generada.\n",
    "    \"\"\"\n",
    "    # Crear la matriz de embeddings para el modelo Word2Vec\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, word2vec_model.vector_size))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in word2vec_model.wv:\n",
    "            embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "    # Definir la capa de embedding en Keras\n",
    "    embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                                output_dim=word2vec_model.vector_size,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "# Crear las capas de embeddings a partir de los modelos Word2Vec\n",
    "embedding_layer_50 = create_embedding_layer(word2vec_model_50, tokenizer, max_length)\n",
    "embedding_layer_100 = create_embedding_layer(word2vec_model_100, tokenizer, max_length)\n",
    "embedding_layer_300 = create_embedding_layer(word2vec_model_300, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura 1: Modelo sencillo\n",
    "def create_ffnn_model_1(embedding_layer):\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal feedforward simple.\n",
    "\n",
    "    Args:\n",
    "    embedding_layer: Capa de embeddings de Keras utilizada como entrada.\n",
    "\n",
    "    Returns:\n",
    "    Sequential: Modelo de red neuronal compilado.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))  # Salida con 3 clases (autores)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Arquitectura 2: Modelo con más capas\n",
    "def create_ffnn_model_2(embedding_layer):\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal feedforward con más capas.\n",
    "\n",
    "    Args:\n",
    "    embedding_layer: Capa de embeddings de Keras utilizada como entrada.\n",
    "\n",
    "    Returns:\n",
    "    Sequential: Modelo de red neuronal compilado.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Arquitectura 3: Modelo con más unidades\n",
    "def create_ffnn_model_3(embedding_layer):\n",
    "    \"\"\"\n",
    "    Crea un modelo de red neuronal feedforward con más unidades.\n",
    "\n",
    "    Args:\n",
    "    embedding_layer: Capa de embeddings de Keras utilizada como entrada.\n",
    "\n",
    "    Returns:\n",
    "    Sequential: Modelo de red neuronal compilado.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creación y Evaluación de los Modelos de Redes Neuronales\n",
    "\n",
    "Creamos un modelo con cada tipo de arquitectura y capa de embeddings y evaluamos su accuracy, precision y recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación de las etiquetas (es decir, los autores)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = to_categorical(label_encoder.fit_transform(y_train))\n",
    "y_test_encoded = to_categorical(label_encoder.transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test_pad, y_test_encoded):\n",
    "    \"\"\"\n",
    "    Evalúa el rendimiento de un modelo entrenado calculando accuracy, precision y recall.\n",
    "    \n",
    "    Args:\n",
    "    model (keras.models.Model): El modelo entrenado.\n",
    "    x_test_pad (numpy.ndarray): Conjunto de datos de prueba preprocesados y tokenizados.\n",
    "    y_test_encoded (numpy.ndarray): Etiquetas de prueba codificadas en formato one-hot.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Un tupla que contiene:\n",
    "        - accuracy (float): La proporción de predicciones correctas.\n",
    "        - precision (float): La proporción de predicciones positivas correctas (precisión macro).\n",
    "        - recall (float): La proporción de verdaderos positivos detectados (recall macro).\n",
    "    \"\"\"\n",
    "    # Obtener predicciones del modelo\n",
    "    y_pred = model.predict(x_test_pad)\n",
    "    \n",
    "    # Convertir las predicciones y etiquetas de one-hot a clases\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test_encoded, axis=1)\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    accuracy = np.mean(y_pred_classes == y_test_classes)\n",
    "    \n",
    "    # Calcular precisión y recall usando la métrica macro (promedio entre todas las clases)\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='macro')\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='macro')\n",
    "    \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 5s 9ms/step - loss: 0.4748 - accuracy: 0.7996 - val_loss: 0.3990 - val_accuracy: 0.8347\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 2s 7ms/step - loss: 0.2864 - accuracy: 0.8897 - val_loss: 0.4126 - val_accuracy: 0.8388\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 3s 7ms/step - loss: 0.1882 - accuracy: 0.9331 - val_loss: 0.4514 - val_accuracy: 0.8317\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 2s 7ms/step - loss: 0.1034 - accuracy: 0.9700 - val_loss: 0.5127 - val_accuracy: 0.8282\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 3s 7ms/step - loss: 0.0441 - accuracy: 0.9932 - val_loss: 0.5999 - val_accuracy: 0.8278\n",
      "161/161 [==============================] - 1s 3ms/step\n",
      "Modelo 1 con 50 dimensiones - Accuracy: 0.8278352527815733, Precision: 0.7817381082676237, Recall: 0.787833728041996\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 1 con los embeddings de 50 dimensiones\n",
    "model_1_50 = create_ffnn_model_1(embedding_layer_50)\n",
    "model_1_50.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 1 con los embeddings de 50 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_1_50, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 1 con 50 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 6s 10ms/step - loss: 0.4605 - accuracy: 0.8111 - val_loss: 0.3888 - val_accuracy: 0.8407\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 3s 9ms/step - loss: 0.2378 - accuracy: 0.9125 - val_loss: 0.4380 - val_accuracy: 0.8216\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 3s 9ms/step - loss: 0.1295 - accuracy: 0.9571 - val_loss: 0.4965 - val_accuracy: 0.8259\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 3s 8ms/step - loss: 0.0522 - accuracy: 0.9873 - val_loss: 0.5254 - val_accuracy: 0.8370\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 3s 8ms/step - loss: 0.0178 - accuracy: 0.9982 - val_loss: 0.6229 - val_accuracy: 0.8325\n",
      "161/161 [==============================] - 1s 4ms/step\n",
      "Modelo 1 con 100 dimensiones - Accuracy: 0.832520007807925, Precision: 0.7853074200551647, Recall: 0.8053995782498754\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 1 con los embeddings de 100 dimensiones\n",
    "model_1_100 = create_ffnn_model_1(embedding_layer_100)\n",
    "model_1_100.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 1 con los embeddings de 100 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_1_100, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 1 con 100 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 8s 16ms/step - loss: 0.4632 - accuracy: 0.8064 - val_loss: 0.3863 - val_accuracy: 0.8446\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 5s 14ms/step - loss: 0.2165 - accuracy: 0.9179 - val_loss: 0.4435 - val_accuracy: 0.8427\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 5s 14ms/step - loss: 0.0962 - accuracy: 0.9693 - val_loss: 0.5231 - val_accuracy: 0.8325\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 6s 15ms/step - loss: 0.0298 - accuracy: 0.9948 - val_loss: 0.6097 - val_accuracy: 0.8325\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 6s 15ms/step - loss: 0.0073 - accuracy: 0.9999 - val_loss: 0.6612 - val_accuracy: 0.8337\n",
      "161/161 [==============================] - 1s 6ms/step\n",
      "Modelo 1 con 300 dimensiones - Accuracy: 0.833691196564513, Precision: 0.792418751649858, Recall: 0.793357362294358\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 1 con los embeddings de 300 dimensiones\n",
    "model_1_300 = create_ffnn_model_1(embedding_layer_300)\n",
    "model_1_300.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 1 con los embeddings de 300 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_1_300, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 1 con 300 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 6s 11ms/step - loss: 0.4854 - accuracy: 0.8032 - val_loss: 0.4026 - val_accuracy: 0.8390\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 3s 9ms/step - loss: 0.2699 - accuracy: 0.8965 - val_loss: 0.4244 - val_accuracy: 0.8376\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 3s 9ms/step - loss: 0.1272 - accuracy: 0.9544 - val_loss: 0.5608 - val_accuracy: 0.8187\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 4s 10ms/step - loss: 0.0438 - accuracy: 0.9854 - val_loss: 0.7461 - val_accuracy: 0.8177\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 3s 9ms/step - loss: 0.0327 - accuracy: 0.9896 - val_loss: 0.7964 - val_accuracy: 0.8181\n",
      "161/161 [==============================] - 1s 4ms/step\n",
      "Modelo 2 con 50 dimensiones - Accuracy: 0.8180753464766738, Precision: 0.7711380621294751, Recall: 0.784024899312303\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 2 con los embeddings de 50 dimensiones\n",
    "model_2_50 = create_ffnn_model_2(embedding_layer_50)\n",
    "model_2_50.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 2 con los embeddings de 50 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_2_50, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 2 con 50 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 7s 14ms/step - loss: 0.4654 - accuracy: 0.8074 - val_loss: 0.3887 - val_accuracy: 0.8429\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 4s 12ms/step - loss: 0.2276 - accuracy: 0.9116 - val_loss: 0.4229 - val_accuracy: 0.8368\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 5s 12ms/step - loss: 0.0910 - accuracy: 0.9678 - val_loss: 0.5748 - val_accuracy: 0.8294\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 5s 12ms/step - loss: 0.0357 - accuracy: 0.9875 - val_loss: 0.6926 - val_accuracy: 0.8337\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 4s 12ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.8897 - val_accuracy: 0.8274\n",
      "161/161 [==============================] - 1s 5ms/step\n",
      "Modelo 2 con 100 dimensiones - Accuracy: 0.8274448565293773, Precision: 0.7842484308020725, Recall: 0.7836818455548729\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 2 con los embeddings de 100 dimensiones\n",
    "model_2_100 = create_ffnn_model_2(embedding_layer_100)\n",
    "model_2_100.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 2 con los embeddings de 100 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_2_100, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 2 con 100 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 11s 24ms/step - loss: 0.4770 - accuracy: 0.8030 - val_loss: 0.3888 - val_accuracy: 0.8458\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 8s 23ms/step - loss: 0.2248 - accuracy: 0.9116 - val_loss: 0.4512 - val_accuracy: 0.8232\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 8s 22ms/step - loss: 0.0953 - accuracy: 0.9646 - val_loss: 0.6391 - val_accuracy: 0.8093\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 9s 23ms/step - loss: 0.0384 - accuracy: 0.9870 - val_loss: 0.8056 - val_accuracy: 0.8177\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 9s 23ms/step - loss: 0.0319 - accuracy: 0.9884 - val_loss: 0.8089 - val_accuracy: 0.8327\n",
      "161/161 [==============================] - 2s 9ms/step\n",
      "Modelo 2 con 300 dimensiones - Accuracy: 0.832715205934023, Precision: 0.7943832803036058, Recall: 0.7836270320281654\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 2 con los embeddings de 300 dimensiones\n",
    "model_2_300 = create_ffnn_model_2(embedding_layer_300)\n",
    "model_2_300.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 2 con los embeddings de 300 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_2_300, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 2 con 300 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 7s 15ms/step - loss: 0.4881 - accuracy: 0.8018 - val_loss: 0.4028 - val_accuracy: 0.8333\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 5s 14ms/step - loss: 0.2696 - accuracy: 0.8947 - val_loss: 0.4464 - val_accuracy: 0.8333\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 5s 14ms/step - loss: 0.1260 - accuracy: 0.9516 - val_loss: 0.6076 - val_accuracy: 0.8192\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 5s 14ms/step - loss: 0.0579 - accuracy: 0.9789 - val_loss: 0.8522 - val_accuracy: 0.8198\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 5s 13ms/step - loss: 0.0376 - accuracy: 0.9868 - val_loss: 0.9032 - val_accuracy: 0.8214\n",
      "161/161 [==============================] - 1s 6ms/step\n",
      "Modelo 3 con 50 dimensiones - Accuracy: 0.8213937146203396, Precision: 0.778040777051026, Recall: 0.7647760440333439\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 3 con los embeddings de 50 dimensiones\n",
    "model_3_50 = create_ffnn_model_3(embedding_layer_50)\n",
    "model_3_50.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 3 con los embeddings de 50 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_3_50, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 3 con 50 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 9s 20ms/step - loss: 0.4793 - accuracy: 0.7995 - val_loss: 0.4052 - val_accuracy: 0.8321\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 7s 19ms/step - loss: 0.2338 - accuracy: 0.9119 - val_loss: 0.4476 - val_accuracy: 0.8343\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 7s 19ms/step - loss: 0.1001 - accuracy: 0.9647 - val_loss: 0.5567 - val_accuracy: 0.8267\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 7s 20ms/step - loss: 0.0488 - accuracy: 0.9823 - val_loss: 0.7163 - val_accuracy: 0.8265\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 7s 18ms/step - loss: 0.0339 - accuracy: 0.9879 - val_loss: 0.7781 - val_accuracy: 0.8280\n",
      "161/161 [==============================] - 1s 7ms/step\n",
      "Modelo 3 con 100 dimensiones - Accuracy: 0.8280304509076712, Precision: 0.7887507558187116, Recall: 0.7743649065106641\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 3 con los embeddings de 100 dimensiones\n",
    "model_3_100 = create_ffnn_model_3(embedding_layer_100)\n",
    "model_3_100.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 3 con los embeddings de 100 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_3_100, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 3 con 100 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "374/374 [==============================] - 17s 42ms/step - loss: 0.4795 - accuracy: 0.8014 - val_loss: 0.3988 - val_accuracy: 0.8394\n",
      "Epoch 2/5\n",
      "374/374 [==============================] - 15s 40ms/step - loss: 0.2361 - accuracy: 0.9106 - val_loss: 0.4555 - val_accuracy: 0.8253\n",
      "Epoch 3/5\n",
      "374/374 [==============================] - 15s 40ms/step - loss: 0.0956 - accuracy: 0.9649 - val_loss: 0.5836 - val_accuracy: 0.8247\n",
      "Epoch 4/5\n",
      "374/374 [==============================] - 15s 40ms/step - loss: 0.0508 - accuracy: 0.9815 - val_loss: 0.7419 - val_accuracy: 0.8251\n",
      "Epoch 5/5\n",
      "374/374 [==============================] - 15s 40ms/step - loss: 0.0349 - accuracy: 0.9862 - val_loss: 0.9709 - val_accuracy: 0.8148\n",
      "161/161 [==============================] - 2s 11ms/step\n",
      "Modelo 3 con 300 dimensiones - Accuracy: 0.814756978333008, Precision: 0.7681871129445516, Recall: 0.783474564912253\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo 3 con los embeddings de 300 dimensiones\n",
    "model_3_300 = create_ffnn_model_3(embedding_layer_300)\n",
    "model_3_300.fit(x_train_pad, y_train_encoded, epochs=5, batch_size=32, validation_data=(x_test_pad, y_test_encoded))\n",
    "\n",
    "# Evaluar el modelo 3 con los embeddings de 300 dimensiones\n",
    "accuracy, precision, recall = evaluate_model(model_3_300, x_test_pad, y_test_encoded)\n",
    "print(f\"Modelo 3 con 300 dimensiones - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
