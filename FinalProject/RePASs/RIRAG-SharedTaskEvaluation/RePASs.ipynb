{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "https://github.com/RegNLP/RePASs/blob/main/RIRAG-SharedTaskEvaluation/RePASs.ipynb",
      "authorship_tag": "ABX9TyMCotFCF7vRwakygSY5f01S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/RePASs/blob/main/RIRAG-SharedTaskEvaluation/RePASs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLSL_hr2pIgy"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Folder path input\n",
        "folder_path = \"/content/drive/Othercomputers/MBZUAI/MBZUAI/ADGM-Project/SharedTask/TestSet\"  # Replace with the actual path which include \"ObligationClassificationDataset.json\" and \"Sample.json\""
      ],
      "metadata": {
        "id": "I4cW2FkepZPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    pipeline,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from nltk.tokenize import sent_tokenize as sent_tokenize_uncached\n",
        "import nltk\n",
        "from functools import cache\n",
        "\n",
        "# Set up random seeds and deterministic flags for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Check if CUDA is available and use it if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 2: Load and preprocess the data\n",
        "json_path = os.path.join(folder_path, \"ObligationClassificationDataset.json\")\n",
        "with open(json_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "texts = [item['Text'] for item in data]\n",
        "labels = [1 if item['Obligation'] else 0 for item in data]  # Converting True/False to 1/0\n",
        "\n",
        "# Step 3: Tokenization using LegalBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
        "\n",
        "class ObligationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = ObligationDataset(X_train, y_train, tokenizer)\n",
        "val_dataset = ObligationDataset(X_val, y_val, tokenizer)\n",
        "\n",
        "# Step 4: Fine-tuning LegalBERT for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'nlpaueb/legal-bert-base-uncased', num_labels=2\n",
        ")\n",
        "model.to(device)  # Move model to the GPU\n",
        "\n",
        "# Ensure the directories exist for saving results and logs\n",
        "output_dir = os.path.join(folder_path, 'results')\n",
        "log_dir = os.path.join(folder_path, 'logs')\n",
        "save_dir = os.path.join(folder_path, 'obligation-classifier-legalbert')\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=log_dir,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    seed=42,  # Set seed in TrainingArguments\n",
        ")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average='binary'\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "\n",
        "# Step 5: Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "trainer.evaluate()\n",
        "\n",
        "# Step 7: Save the model and tokenizer for future use\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(\"Model fine-tuning and evaluation completed.\")"
      ],
      "metadata": {
        "id": "40Wm_zvYpW3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# Load the tokenizer and model for obligation detection\n",
        "model_name = os.path.join(folder_path, 'obligation-classifier-legalbert')\n",
        "obligation_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "obligation_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "obligation_model.to(device)\n",
        "obligation_model.eval()\n",
        "\n",
        "# Load NLI model and tokenizer for obligation coverage\n",
        "coverage_nli_model = pipeline(\n",
        "    \"text-classification\", model=\"microsoft/deberta-large-mnli\", device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Load NLI model and tokenizer for entailment and contradiction checks\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-xsmall')\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-xsmall')\n",
        "nli_model.to(device)\n",
        "nli_model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
        "\n",
        "# Define a cached version of sentence tokenization\n",
        "@cache\n",
        "def sent_tokenize(passage: str):\n",
        "    return sent_tokenize_uncached(passage)\n",
        "\n",
        "def softmax(logits):\n",
        "    e_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    return e_logits / np.sum(e_logits, axis=1, keepdims=True)\n",
        "\n",
        "def get_nli_probabilities(premises, hypotheses):\n",
        "    features = nli_tokenizer(\n",
        "        premises,\n",
        "        hypotheses,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    nli_model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = nli_model(**features).logits.cpu().numpy()\n",
        "    probabilities = softmax(logits)\n",
        "    return probabilities\n",
        "\n",
        "def get_nli_matrix(passages, answers):\n",
        "    entailment_matrix = np.zeros((len(passages), len(answers)))\n",
        "    contradiction_matrix = np.zeros((len(passages), len(answers)))\n",
        "\n",
        "    batch_size = 16\n",
        "    for i, pas in enumerate(passages):\n",
        "        for b in range(0, len(answers), batch_size):\n",
        "            e = b + batch_size\n",
        "            probs = get_nli_probabilities(\n",
        "                [pas] * len(answers[b:e]), answers[b:e]\n",
        "            )  # Get NLI probabilities\n",
        "            entailment_matrix[i, b:e] = probs[:, 1]\n",
        "            contradiction_matrix[i, b:e] = probs[:, 0]\n",
        "    return entailment_matrix, contradiction_matrix\n",
        "\n",
        "def calculate_scores_from_matrix(nli_matrix, score_type='entailment'):\n",
        "    if nli_matrix.size == 0:\n",
        "        return 0.0  # or some other default score or handling as appropriate for your use case\n",
        "\n",
        "    if score_type == 'entailment':\n",
        "        reduced_vector = np.max(nli_matrix, axis=0)\n",
        "    elif score_type == 'contradiction':\n",
        "        reduced_vector = np.max(nli_matrix, axis=0)\n",
        "    score = np.round(np.mean(reduced_vector), 5)\n",
        "    return score\n",
        "\n",
        "def classify_obligations(sentences):\n",
        "    inputs = obligation_tokenizer(\n",
        "        sentences, padding=True, truncation=True, return_tensors='pt'\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = obligation_model(**inputs).logits\n",
        "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    return predictions\n",
        "\n",
        "def calculate_obligation_coverage_score(passages, answers):\n",
        "    # Filter obligation sentences from passages\n",
        "    obligation_sentences_source = []\n",
        "    for passage in passages:\n",
        "        sentences = sent_tokenize(passage)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_source.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Filter obligation sentences from answers\n",
        "    obligation_sentences_answer = []\n",
        "    for answer in answers:\n",
        "        sentences = sent_tokenize(answer)\n",
        "        is_obligation = classify_obligations(sentences)\n",
        "        obligation_sentences_answer.extend(\n",
        "            [sent for sent, label in zip(sentences, is_obligation) if label == 1]\n",
        "        )\n",
        "\n",
        "    # Calculate coverage based on NLI entailment\n",
        "    covered_count = 0\n",
        "    for obligation in obligation_sentences_source:\n",
        "        for answer_sentence in obligation_sentences_answer:\n",
        "            nli_result = coverage_nli_model(\n",
        "                f\"{answer_sentence} [SEP] {obligation}\"\n",
        "            )\n",
        "            if nli_result[0]['label'].lower() == 'entailment' and nli_result[0]['score'] > 0.7:\n",
        "                covered_count += 1\n",
        "                break\n",
        "\n",
        "    return (\n",
        "        covered_count / len(obligation_sentences_source)\n",
        "        if obligation_sentences_source\n",
        "        else 0\n",
        "    )\n",
        "\n",
        "def calculate_final_composite_score(passages, answers):\n",
        "    passage_sentences = [sent for passage in passages for sent in sent_tokenize(passage)]\n",
        "    answer_sentences = [sent for answer in answers for sent in sent_tokenize(answer)]\n",
        "\n",
        "    # Calculate NLI matrix for entailment and contradiction\n",
        "    entailment_matrix, contradiction_matrix = get_nli_matrix(\n",
        "        passage_sentences, answer_sentences\n",
        "    )\n",
        "\n",
        "    # Calculate scores\n",
        "    entailment_score = calculate_scores_from_matrix(entailment_matrix, 'entailment')\n",
        "    contradiction_score = calculate_scores_from_matrix(\n",
        "        contradiction_matrix, 'contradiction'\n",
        "    )\n",
        "    obligation_coverage_score = calculate_obligation_coverage_score(passages, answers)\n",
        "\n",
        "    # Final composite score formula\n",
        "    composite_score = (\n",
        "        obligation_coverage_score + entailment_score - contradiction_score + 1\n",
        "    ) / 3\n",
        "\n",
        "    # Return all scores\n",
        "    return (\n",
        "        np.round(composite_score, 5),\n",
        "        entailment_score,\n",
        "        contradiction_score,\n",
        "        obligation_coverage_score,\n",
        "    )\n",
        "\n",
        "def calculate_average_scores_from_csv(output_file_csv):\n",
        "    \"\"\"Calculate average scores from the CSV file.\"\"\"\n",
        "    entailment_scores = []\n",
        "    contradiction_scores = []\n",
        "    obligation_coverage_scores = []\n",
        "    composite_scores = []\n",
        "\n",
        "    with open(output_file_csv, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            try:\n",
        "                entailment_scores.append(float(row['entailment_score']))\n",
        "                contradiction_scores.append(float(row['contradiction_score']))\n",
        "                obligation_coverage_scores.append(float(row['obligation_coverage_score']))\n",
        "                composite_scores.append(float(row['composite_score']))\n",
        "            except ValueError:\n",
        "                # Handle the case where the value cannot be converted to float\n",
        "                print(f\"Skipping invalid row: {row}\")\n",
        "\n",
        "    avg_entailment = np.mean(entailment_scores) if entailment_scores else 0.0\n",
        "    avg_contradiction = np.mean(contradiction_scores) if contradiction_scores else 0.0\n",
        "    avg_obligation_coverage = (\n",
        "        np.mean(obligation_coverage_scores) if obligation_coverage_scores else 0.0\n",
        "    )\n",
        "    avg_composite = np.mean(composite_scores) if composite_scores else 0.0\n",
        "\n",
        "    return avg_entailment, avg_contradiction, avg_obligation_coverage, avg_composite\n",
        "\n",
        "def main(input_file_path, group_method_name):\n",
        "    # Create a directory with the group_method_name in the folder path\n",
        "    output_dir = os.path.join(folder_path, group_method_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Define the paths for result files\n",
        "    output_file_csv = os.path.join(output_dir, 'results.csv')\n",
        "    output_file_txt = os.path.join(output_dir, 'results.txt')\n",
        "\n",
        "    processed_question_ids = set()\n",
        "    saved_items_count = 0\n",
        "\n",
        "    # Check if the output CSV file already exists and read processed QuestionIDs\n",
        "    if os.path.exists(output_file_csv):\n",
        "        with open(output_file_csv, 'r') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                processed_question_ids.add(row['QuestionID'])\n",
        "                saved_items_count += 1\n",
        "\n",
        "    with open(input_file_path, 'r') as file:\n",
        "        test_data = json.load(file)\n",
        "\n",
        "    total_items = len(test_data)\n",
        "\n",
        "    # Open the CSV file for appending results\n",
        "    with open(output_file_csv, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if not processed_question_ids:\n",
        "            # Write the header if the file is empty or new\n",
        "            writer.writerow(\n",
        "                [\n",
        "                    'QuestionID',\n",
        "                    'entailment_score',\n",
        "                    'contradiction_score',\n",
        "                    'obligation_coverage_score',\n",
        "                    'composite_score',\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        for index, item in enumerate(test_data, start=1):\n",
        "            question_id = item['QuestionID']\n",
        "\n",
        "            # Skip if the QuestionID has already been processed\n",
        "            if question_id in processed_question_ids:\n",
        "                continue\n",
        "\n",
        "            # Skip if the \"Answer\" is null or empty\n",
        "            if not item.get('Answer') or not item['Answer'].strip():\n",
        "                continue\n",
        "\n",
        "            # Merge \"RetrievedPassages\" if it's a list\n",
        "            if isinstance(item['RetrievedPassages'], list):\n",
        "                item['RetrievedPassages'] = \" \".join(item['RetrievedPassages'])\n",
        "\n",
        "            passages = [item['RetrievedPassages']]\n",
        "            answers = [item['Answer']]\n",
        "            (\n",
        "                composite_score,\n",
        "                entailment_score,\n",
        "                contradiction_score,\n",
        "                obligation_coverage_score,\n",
        "            ) = calculate_final_composite_score(passages, answers)\n",
        "\n",
        "            # Write the result to the CSV file\n",
        "            writer.writerow(\n",
        "                [\n",
        "                    question_id,\n",
        "                    entailment_score,\n",
        "                    contradiction_score,\n",
        "                    obligation_coverage_score,\n",
        "                    composite_score,\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Increment the saved items count and print status\n",
        "            saved_items_count += 1\n",
        "            print(f\"{saved_items_count}/{total_items}\")\n",
        "\n",
        "    # Calculate average scores from the CSV file\n",
        "    (\n",
        "        avg_entailment,\n",
        "        avg_contradiction,\n",
        "        avg_obligation_coverage,\n",
        "        avg_composite,\n",
        "    ) = calculate_average_scores_from_csv(output_file_csv)\n",
        "\n",
        "    # Print and save results to a text file\n",
        "    results = (\n",
        "        f\"Average Entailment Score: {avg_entailment}\\n\"\n",
        "        f\"Average Contradiction Score: {avg_contradiction}\\n\"\n",
        "        f\"Average Obligation Coverage Score: {avg_obligation_coverage}\\n\"\n",
        "        f\"Average Final Composite Score: {avg_composite}\\n\"\n",
        "    )\n",
        "\n",
        "    print(results)\n",
        "\n",
        "    with open(output_file_txt, 'w') as txtfile:\n",
        "        txtfile.write(results)\n",
        "\n",
        "    print(f\"Processing complete. Results saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "2bXNmKO20dwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    group_methodName = 'SAMPLE'  # First test the code, Runtime A100 GPU\n",
        "    input_file = os.path.join(folder_path, \"sample.json\")\n",
        "    main(input_file, group_methodName)\n",
        "\n",
        "# Expected Scores:\n",
        "# Average Entailment Score: 0.11377999999999999\n",
        "# Average Contradiction Score: 0.09597\n",
        "# Average Obligation Coverage Score: 0.7\n",
        "# Average Final Composite Score: 0.5726030000000001"
      ],
      "metadata": {
        "id": "KeA1Aim70efo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW TEST YOUR RESULTS, FOR RIRAG SHARED TASK EVALUATION WE WILL USE THIS NOTEBOOK\n",
        "if __name__ == \"__main__\":\n",
        "  group_methodName = 'SAMPLE' # Replace with the GroupName Or MethodName\n",
        "  input_file = os.path.join(folder_path, \"sample.json\") # Replace with your system results\n",
        "  main(input_file, group_methodName)"
      ],
      "metadata": {
        "id": "JsHtW9VVrgTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}