{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "This notebook processes the dataset in json format and outputs pickle files for training and validation. Each json file has several data samples where a question is mapped to the relevant passages that answer the question accurately and completely. For instance, this would be a sample from the testing dataset.\n",
    "\n",
    "```json\n",
    "{\n",
    "        \"QuestionID\": \"777e7a14-fea3-4c37-a0e6-9ffb50024d5c\",\n",
    "        \"Question\": \"Can the ADGM provide clarity on the level of detail and documentation that should accompany a report of suspicious activity to ensure it meets regulatory standards?\",\n",
    "        \"Passages\": [\n",
    "            {\n",
    "                \"DocumentID\": 1,\n",
    "                \"PassageID\": \"14.2.3.Guidance.10.\",\n",
    "                \"Passage\": \"Relevant Persons should comply with guidance issued by the EOCN with regard to identifying and reporting suspicious activity and Transactions relating to money laundering, terrorist financing and proliferation financing.\"\n",
    "            }\n",
    "        ],\n",
    "        \"Group\": 2\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Data handling\n",
    "from datasets import Dataset\n",
    "from pandas import to_pickle\n",
    "\n",
    "# Other utils\n",
    "import json\n",
    "from re import compile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform some basic clean up of the query\n",
    "def simple_cleaning(query: str) -> str:\n",
    "    pattern_newline = compile(r'[\\n\\t\\u200e]')  # Remove new lines, tabs, and undesired characters\n",
    "    pattern_multiple_spaces = compile(r' +')  # Remove contiguous blank spaces\n",
    "\n",
    "    cln_query = pattern_newline.sub(' ', query)\n",
    "    cln_query = pattern_multiple_spaces.sub(' ', cln_query).strip()\n",
    "    return cln_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in memory\n",
    "\n",
    "# Training dataset\n",
    "with open('./ObliQADataset/ObliQA_train.json') as f:\n",
    "    data_train = json.load(f)\n",
    "    \n",
    "# Evaluation dataset\n",
    "with open('./ObliQADataset/ObliQA_dev.json') as f:\n",
    "    data_eval = json.load(f)\n",
    "\n",
    "# Testing dataset\n",
    "with open('./ObliQADataset/ObliQA_test.json') as f:\n",
    "    data_test = json.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22295, 2788, 2786)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train), len(data_eval), len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29547/29547 [00:00<00:00, 749409.50 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor_id', 'anchor', 'positive', 'positive_id'],\n",
       "    num_rows: 29547\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training dataset with 4 features\n",
    "train_set = []\n",
    "for q in data_train:\n",
    "    q_id = q['QuestionID']\n",
    "    for rel_doc in q['Passages']:\n",
    "        # The training dataset has 4 features\n",
    "        # - anchor_id: the query id\n",
    "        # - anchor: the query itself\n",
    "        # - positive: the passage itself\n",
    "        # - positive_id: the passage id along with the document id\n",
    "        train_set.append({\n",
    "            'anchor_id': q_id,\n",
    "            'anchor': simple_cleaning(q['Question']),\n",
    "            'positive': simple_cleaning(f\"{rel_doc['PassageID']} {rel_doc['Passage']}\"),\n",
    "            'positive_id': f\"{rel_doc['DocumentID']}-{rel_doc['PassageID']}\",\n",
    "        })\n",
    "        \n",
    "train_dataset = Dataset.from_list(train_set)\n",
    "\n",
    "train_dataset.save_to_disk('./data/train_dataset')\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor_id': 'a10724b5-ad0e-4b69-8b5e-792aef214f86',\n",
       " 'anchor': 'Under Rules 7.3.2 and 7.3.3, what are the two specific conditions related to the maturity of a financial instrument that would trigger a disclosure requirement?',\n",
       " 'positive': '7.3.4 Events that trigger a disclosure. For the purposes of Rules 7.3.2 and 7.3.3, a Person is taken to hold Financial Instruments in or relating to a Reporting Entity, if the Person holds a Financial Instrument that on its maturity will confer on him: (1) an unconditional right to acquire the Financial Instrument; or (2) the discretion as to his right to acquire the Financial Instrument.',\n",
       " 'positive_id': '11-7.3.4'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3677/3677 [00:00<00:00, 477771.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor_id', 'anchor', 'positive', 'positive_id'],\n",
       "    num_rows: 3677\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same features as the training dataset\n",
    "eval_set = []\n",
    "for q in data_eval:\n",
    "    q_id = q['QuestionID']\n",
    "    for rel_doc in q['Passages']:\n",
    "        eval_set.append({\n",
    "            'anchor_id': q_id,\n",
    "            'anchor': simple_cleaning(q['Question']),\n",
    "            'positive': simple_cleaning(f\"{rel_doc['PassageID']} {rel_doc['Passage']}\"),\n",
    "            'positive_id': f\"{rel_doc['DocumentID']}-{rel_doc['PassageID']}\",\n",
    "        })\n",
    "        \n",
    "eval_dataset = Dataset.from_list(eval_set)\n",
    "\n",
    "eval_dataset.save_to_disk('./data/eval_dataset')\n",
    "\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3666/3666 [00:00<00:00, 475634.70 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor_id', 'anchor', 'positive', 'positive_id'],\n",
       "    num_rows: 3666\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same features as the training dataset\n",
    "test_set = []\n",
    "for q in data_test:\n",
    "    q_id = q['QuestionID']\n",
    "    for rel_doc in q['Passages']:\n",
    "        test_set.append({\n",
    "            'anchor_id': q_id,\n",
    "            'anchor': simple_cleaning(q['Question']),\n",
    "            'positive': simple_cleaning(f\"{rel_doc['PassageID']} {rel_doc['Passage']}\"),\n",
    "            'positive_id': f\"{rel_doc['DocumentID']}-{rel_doc['PassageID']}\",\n",
    "        })\n",
    "        \n",
    "test_dataset = Dataset.from_list(test_set)\n",
    "\n",
    "test_dataset.save_to_disk('./data/test_dataset')\n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "\n",
    "Creates a corpus pickle from the list of 40 regulatory documents and saves it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process to load a collection of passage\n",
    "ndocs = 40  # Number of documents to process\n",
    "collection = []  # Collection of documents\n",
    "\n",
    "# Read each document and extracts the relevant passages\n",
    "for i in range(1, ndocs + 1):\n",
    "    with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
    "        doc = json.load(f)  # Load the contents of the json file\n",
    "        for psg in doc:  # For each passage in the document\n",
    "            # Only add a passage to the collection if the length is greater than 100. This is based \n",
    "            # on the assumption that shorther passages may be irrelevant or empty as they may simply refer to sections\n",
    "            # of the document\n",
    "            if len(psg[\"PassageID\"] + \" \" + psg[\"Passage\"])>100:\n",
    "                collection.append(\n",
    "                    dict(\n",
    "                        text=psg[\"PassageID\"] + \" \" + psg[\"Passage\"],  # Joins the passageId and the passage itself\n",
    "                        ID=psg[\"ID\"],  # entry ID\n",
    "                        DocumentId=psg['DocumentID'],  # document ID\n",
    "                        PassageId=psg['PassageID'],  # passage ID\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "corpus = {f\"{doc['DocumentId']}-{doc['PassageId']}\": doc[\"text\"] for doc in collection}\n",
    "# Save the corpus to disk\n",
    "to_pickle(corpus, './data/corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
