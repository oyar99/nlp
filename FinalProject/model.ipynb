{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raul/Escritorio/extra/misis/rl/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import (SentenceTransformer, models, \n",
    "                                   SentenceTransformerTrainingArguments,\n",
    "                                   SentenceTransformerTrainer)\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.util import dot_score\n",
    "\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import create_data_for_evaluator\n",
    "from custom_adapter_module.AdapterModule import AdapterModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom SentenceTransformer model is configured by defining and configuring several components, including a word embedding model, a grouping model, a normalization layer, and an adapter module. The word embedding model is initialized with a pre-trained transformer model from Sentence Transformers, with specific settings for a maximum sequence length and case-sensitivity. The pooling model is configured to use token averaging for pooling, with other pooling modes disabled. The normalization layer is defined to standardize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): AdapterModule(\n",
       "    (dense1): Linear(in_features=384, out_features=768, bias=True)\n",
       "    (dense2): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (output): Linear(in_features=512, out_features=384, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (3): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga del modelo de embeddings de palabras\n",
    "word_embedding_model = models.Transformer(\n",
    "    model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",  # Modelo base de Sentence Transformers\n",
    "    max_seq_length=512,  # Longitud máxima de la secuencia\n",
    "    do_lower_case=False,  # No convertir a minúsculas\n",
    ")\n",
    "\n",
    "# Definición de los parámetros del modelo de pooling\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_dimension=384,  # Dimensión de los embeddings de palabras\n",
    "    pooling_mode_cls_token=False,  # No usar el token CLS para el pooling\n",
    "    pooling_mode_mean_tokens=True,  # Usar el promedio de los tokens para el pooling\n",
    "    pooling_mode_max_tokens=False,  # No usar el máximo de los tokens para el pooling\n",
    "    pooling_mode_mean_sqrt_len_tokens=False,  # No usar el promedio de la raíz cuadrada de la longitud para el pooling\n",
    "    pooling_mode_weightedmean_tokens=False,  # No usar el promedio ponderado de los tokens para el pooling\n",
    "    pooling_mode_lasttoken=False,  # No usar el último token para el pooling\n",
    "    include_prompt=True  # Incluir el prompt en el pooling\n",
    ")\n",
    "\n",
    "# Definición del modelo de normalización\n",
    "normalize = models.Normalize()\n",
    "\n",
    "# Congelar los pesos del modelo de embeddings de palabras para que no se entrenen\n",
    "for param in word_embedding_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Configuración del dispositivo para usar GPU si está disponible, de lo contrario usar CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Definir el módulo adaptador con las dimensiones de entrada y salida\n",
    "adapter = AdapterModule(384, 384).to(device)\n",
    "\n",
    "# Definir el modelo base de Sentence Transformer con las capas de embedding, pooling y normalización\n",
    "base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, normalize], \n",
    "                                 device=device,\n",
    "                                 model_kwargs={\"torch_dtype\": \"float16\"}\n",
    "                                 )\n",
    "\n",
    "# Definir el modelo personalizado de Sentence Transformer que incluye el adaptador\n",
    "custom_domain_model = SentenceTransformer(\n",
    "    modules=[word_embedding_model, pooling_model, adapter, normalize], device=device,\n",
    ")\n",
    "\n",
    "custom_domain_model  # Mostrar la arquitectura del modelo personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent the weights of the word embedding model from being updated during training, all its parameters are frozen. Device configuration ensures that the model uses a GPU if available; otherwise it will default to CPU usage. An AdapterModule instance with defined input and output dimensions is created and moved to the specified device.\n",
    "\n",
    "Two SentenceTransformer models are instantiated: the base model, which includes the word embedding, pooling, and normalization layers; and the custom domain model, which additionally incorporates the adapter module. This configuration allows flexible adaptation of embeddings tailored to specific tasks or domains. The final architecture of the custom domain model is shown for your review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and evaluation datasets for question answering tasks from the respective pickled files stored in the 'data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_train = load_from_disk('./data/train_dataset')\n",
    "qa_eval = load_from_disk('./data/eval_dataset')\n",
    "qa_test = load_from_disk('./data/test_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training examples using the question-answer pairs from the dataset `qa`, where each example consists of a question (`qa[0]`) and its corresponding answer (`qa[1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lenght:  29547\n",
      "Validation lenght:  3677\n",
      "Test lenght:  3666\n"
     ]
    }
   ],
   "source": [
    "print(\"Training lenght: \", len(qa_train))\n",
    "print(\"Validation lenght: \", len(qa_eval))\n",
    "print(\"Test lenght: \", len(qa_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepares and configures the training and evaluation process for a custom SentenceTransformer model. Initially, a training data set is created by generating a list of `InputExample` instances, where each instance consists of a pair of texts (question and answer). This data set is then loaded into a \"DataLoader\", which shuffles the data at each epoch and sets the batch size to 256.\n",
    "\n",
    "The training loss is defined using \"MultipleNegativesSymmetricRankingLoss\", which is suitable for information retrieval tasks involving positive text pairs. An evaluator is configured using \"InformationRetrievalEvaluator\", which evaluates the performance of the model on a set of queries and corpora, with the main scoring function specified as \"dot_score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_evaluator = create_data_for_evaluator(qa_eval)\n",
    "test_dataset_evaluator = create_data_for_evaluator(qa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=eval_dataset_evaluator['queries'],\n",
    "        corpus=eval_dataset_evaluator['corpus'],\n",
    "        relevant_docs=eval_dataset_evaluator['relevant_docs'],\n",
    "        name='qa_eval', \n",
    "        map_at_k=[10],\n",
    "        accuracy_at_k = [10],\n",
    "        precision_recall_at_k = [10],\n",
    "        score_functions={'dot_score':dot_score}\n",
    "    )\n",
    "\n",
    "test_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=test_dataset_evaluator['queries'],\n",
    "        corpus=test_dataset_evaluator['corpus'],\n",
    "        relevant_docs=test_dataset_evaluator['relevant_docs'],\n",
    "        name='qa_test', \n",
    "        map_at_k=[10],\n",
    "        accuracy_at_k = [10],\n",
    "        precision_recall_at_k = [10],\n",
    "        score_functions={'dot_score':dot_score}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_eval_dot_score_accuracy@10': 0.7819225251076041,\n",
       " 'qa_eval_dot_score_precision@10': 0.08371592539454806,\n",
       " 'qa_eval_dot_score_recall@10': 0.705266618842659,\n",
       " 'qa_eval_dot_score_ndcg@10': 0.5763616901461027,\n",
       " 'qa_eval_dot_score_mrr@10': 0.5725218339368267,\n",
       " 'qa_eval_dot_score_map@10': 0.5162513355613247}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Base model evaluation\n",
    "\n",
    "results = dev_evaluator(base_model)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368e6a7a9a934a74a6ceadc2a314b4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raul/Escritorio/extra/misis/rl/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss = MultipleNegativesRankingLoss(custom_domain_model,\n",
    "                                    similarity_fct=dot_score)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"./results/domain_adaptation_model\",  \n",
    "    num_train_epochs=5,  # Entrenar por al menos 50 épocas\n",
    "    per_device_train_batch_size=128,  # Ajustar según la memoria disponible\n",
    "    gradient_accumulation_steps=2,  # 32 * 4 = 128\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,  # Reducir uso de memoria\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer más eficiente\n",
    "    lr_scheduler_type=\"cosine\",  # Planificador de tasa de aprendizaje\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"epoch\",  # Evaluar al final de cada época\n",
    "    save_strategy=\"epoch\",  # Guardar al final de cada época\n",
    "    save_total_limit=1,  # Mantener los últimos 3 checkpoints\n",
    "    logging_steps=100,  # Ajustar según la frecuencia deseada\n",
    "    metric_for_best_model=\"qa_eval_dot_score_map@10\",\n",
    "    greater_is_better=True,  # Si un mayor MAP es mejor\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=custom_domain_model,\n",
    "    args=args,\n",
    "    train_dataset=qa_train.select_columns([\"anchor\", \"positive\", \"negative\"]),\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of training epochs is set to 500 and the warm-up steps are calculated as 10% of the total training steps, determined by the length of the DataLoader and the number of epochs. This setup ensures that the model is properly prepared and evaluated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the base model & the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raul/Escritorio/extra/misis/nlp/nlp/FinalProject/custom_adapter_module/AdapterModule.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  config = torch.load(os.path.join(input_path, 'config.pt'))\n",
      "/home/raul/Escritorio/extra/misis/nlp/nlp/FinalProject/custom_adapter_module/AdapterModule.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'adapter_module.pt')))\n"
     ]
    }
   ],
   "source": [
    "custom_domain_model = SentenceTransformer('./results/domain_adaptation_model',\n",
    "                                          device=device,\n",
    "                                          model_kwargs={\"torch_dtype\": \"float16\"}\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Mean Average Precision (MAP) at k=10 for both the base and custom domain models using the evaluator, and print the results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model:  {'qa_test_dot_score_accuracy@10': 0.7692031586503948, 'qa_test_dot_score_precision@10': 0.08155061019382627, 'qa_test_dot_score_recall@10': 0.6938023450586264, 'qa_test_dot_score_ndcg@10': 0.5686011104188272, 'qa_test_dot_score_mrr@10': 0.5632566175548943, 'qa_test_dot_score_map@10': 0.5110383265534019}\n",
      "Custom model:  {'qa_test_dot_score_accuracy@10': 0.7114142139267767, 'qa_test_dot_score_precision@10': 0.0754845656855707, 'qa_test_dot_score_recall@10': 0.6434912658530749, 'qa_test_dot_score_ndcg@10': 0.5122504366271634, 'qa_test_dot_score_mrr@10': 0.5012685308629311, 'qa_test_dot_score_map@10': 0.45437743327370034}\n"
     ]
    }
   ],
   "source": [
    "eva_base_model = test_evaluator(base_model, output_path='results/base_model/')\n",
    "print(\"Base model: \", eva_base_model)\n",
    "\n",
    "eva_custom_model = test_evaluator(custom_domain_model, output_path='results/custom_model/')\n",
    "print(\"Custom model: \", eva_custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load evaluation results from CSV files for both the base and custom domain models, add a column to indicate the model type, and concatenate the results into a single DataFrame for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>steps</th>\n",
       "      <th>cosine-Accuracy@1</th>\n",
       "      <th>cosine-Accuracy@3</th>\n",
       "      <th>cosine-Accuracy@5</th>\n",
       "      <th>cosine-Accuracy@10</th>\n",
       "      <th>cosine-Precision@1</th>\n",
       "      <th>cosine-Recall@1</th>\n",
       "      <th>cosine-Precision@3</th>\n",
       "      <th>cosine-Recall@3</th>\n",
       "      <th>...</th>\n",
       "      <th>dot-Precision@3</th>\n",
       "      <th>dot-Recall@3</th>\n",
       "      <th>dot-Precision@5</th>\n",
       "      <th>dot-Recall@5</th>\n",
       "      <th>dot-Precision@10</th>\n",
       "      <th>dot-Recall@10</th>\n",
       "      <th>dot-MRR@10</th>\n",
       "      <th>dot-NDCG@10</th>\n",
       "      <th>dot-MAP@100</th>\n",
       "      <th>tipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.339880</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.529460</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.339880</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.105892</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.421933</td>\n",
       "      <td>0.102059</td>\n",
       "      <td>0.011930</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.339880</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.529460</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.339880</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.105892</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.421933</td>\n",
       "      <td>0.102059</td>\n",
       "      <td>0.011930</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.339880</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.529460</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.339880</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.105892</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.421933</td>\n",
       "      <td>0.102059</td>\n",
       "      <td>0.011930</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.339880</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.529460</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.339880</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.105892</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.421933</td>\n",
       "      <td>0.102059</td>\n",
       "      <td>0.011930</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.340698</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.529460</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.340698</td>\n",
       "      <td>0.009464</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158847</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.105892</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.422303</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0.042230</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.422303</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0.042230</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.607474</td>\n",
       "      <td>0.060747</td>\n",
       "      <td>0.016874</td>\n",
       "      <td>0.428965</td>\n",
       "      <td>0.103823</td>\n",
       "      <td>0.042897</td>\n",
       "      <td>0.607474</td>\n",
       "      <td>0.060747</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.597109</td>\n",
       "      <td>0.059711</td>\n",
       "      <td>0.016586</td>\n",
       "      <td>0.428617</td>\n",
       "      <td>0.103230</td>\n",
       "      <td>0.042862</td>\n",
       "      <td>0.597109</td>\n",
       "      <td>0.059711</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.422303</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0.042230</td>\n",
       "      <td>0.596017</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.691673</td>\n",
       "      <td>0.069167</td>\n",
       "      <td>0.019213</td>\n",
       "      <td>0.503166</td>\n",
       "      <td>0.120688</td>\n",
       "      <td>0.050317</td>\n",
       "      <td>0.691673</td>\n",
       "      <td>0.069167</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.691673</td>\n",
       "      <td>0.069167</td>\n",
       "      <td>0.019213</td>\n",
       "      <td>0.503166</td>\n",
       "      <td>0.120688</td>\n",
       "      <td>0.050317</td>\n",
       "      <td>0.691673</td>\n",
       "      <td>0.069167</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>base_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.273595</td>\n",
       "      <td>0.394163</td>\n",
       "      <td>0.450627</td>\n",
       "      <td>0.509820</td>\n",
       "      <td>0.273595</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.131388</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131388</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.090125</td>\n",
       "      <td>0.012517</td>\n",
       "      <td>0.050982</td>\n",
       "      <td>0.014162</td>\n",
       "      <td>0.348023</td>\n",
       "      <td>0.085147</td>\n",
       "      <td>0.009879</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.324604</td>\n",
       "      <td>0.447081</td>\n",
       "      <td>0.505728</td>\n",
       "      <td>0.571740</td>\n",
       "      <td>0.324604</td>\n",
       "      <td>0.009017</td>\n",
       "      <td>0.149027</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149027</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.101146</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.057174</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.401730</td>\n",
       "      <td>0.097379</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.320786</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.500273</td>\n",
       "      <td>0.570104</td>\n",
       "      <td>0.320786</td>\n",
       "      <td>0.008911</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.100055</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.398395</td>\n",
       "      <td>0.096727</td>\n",
       "      <td>0.011291</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.319967</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.500273</td>\n",
       "      <td>0.570104</td>\n",
       "      <td>0.319967</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.100055</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.397986</td>\n",
       "      <td>0.096660</td>\n",
       "      <td>0.039799</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.570104</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.398117</td>\n",
       "      <td>0.096681</td>\n",
       "      <td>0.039812</td>\n",
       "      <td>0.570104</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.571740</td>\n",
       "      <td>0.057174</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.389870</td>\n",
       "      <td>0.095375</td>\n",
       "      <td>0.038987</td>\n",
       "      <td>0.571740</td>\n",
       "      <td>0.057174</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.511184</td>\n",
       "      <td>0.051118</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.352464</td>\n",
       "      <td>0.085920</td>\n",
       "      <td>0.035246</td>\n",
       "      <td>0.511184</td>\n",
       "      <td>0.051118</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.556738</td>\n",
       "      <td>0.055674</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>0.381735</td>\n",
       "      <td>0.093198</td>\n",
       "      <td>0.038173</td>\n",
       "      <td>0.556738</td>\n",
       "      <td>0.055674</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.663676</td>\n",
       "      <td>0.066368</td>\n",
       "      <td>0.018435</td>\n",
       "      <td>0.471482</td>\n",
       "      <td>0.113914</td>\n",
       "      <td>0.047148</td>\n",
       "      <td>0.663676</td>\n",
       "      <td>0.066368</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.594760</td>\n",
       "      <td>0.059476</td>\n",
       "      <td>0.016521</td>\n",
       "      <td>0.409857</td>\n",
       "      <td>0.099921</td>\n",
       "      <td>0.040986</td>\n",
       "      <td>0.594760</td>\n",
       "      <td>0.059476</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>custom_model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  steps  cosine-Accuracy@1  cosine-Accuracy@3  cosine-Accuracy@5  \\\n",
       "0      -1     -1           0.339880           0.476541           0.529460   \n",
       "1      -1     -1           0.339880           0.476541           0.529460   \n",
       "2      -1     -1           0.339880           0.476541           0.529460   \n",
       "3      -1     -1           0.339880           0.476541           0.529460   \n",
       "4      -1     -1           0.340698           0.476541           0.529460   \n",
       "5      -1     -1           0.596017           0.059602           0.016556   \n",
       "6      -1     -1           0.607474           0.060747           0.016874   \n",
       "7      -1     -1           0.597109           0.059711           0.016586   \n",
       "8      -1     -1           0.596017           0.059602           0.016556   \n",
       "9      -1     -1           0.691673           0.069167           0.019213   \n",
       "10     -1     -1           0.691673           0.069167           0.019213   \n",
       "0      -1     -1           0.273595           0.394163           0.450627   \n",
       "1      -1     -1           0.324604           0.447081           0.505728   \n",
       "2      -1     -1           0.320786           0.446809           0.500273   \n",
       "3      -1     -1           0.319967           0.446809           0.500273   \n",
       "4      -1     -1           0.570104           0.057010           0.015836   \n",
       "5      -1     -1           0.571740           0.057174           0.015882   \n",
       "6      -1     -1           0.511184           0.051118           0.014200   \n",
       "7      -1     -1           0.556738           0.055674           0.015465   \n",
       "8      -1     -1           0.663676           0.066368           0.018435   \n",
       "9      -1     -1           0.594760           0.059476           0.016521   \n",
       "\n",
       "    cosine-Accuracy@10  cosine-Precision@1  cosine-Recall@1  \\\n",
       "0             0.596017            0.339880         0.009441   \n",
       "1             0.596017            0.339880         0.009441   \n",
       "2             0.596017            0.339880         0.009441   \n",
       "3             0.596017            0.339880         0.009441   \n",
       "4             0.596017            0.340698         0.009464   \n",
       "5             0.422303            0.102119         0.042230   \n",
       "6             0.428965            0.103823         0.042897   \n",
       "7             0.428617            0.103230         0.042862   \n",
       "8             0.422303            0.102119         0.042230   \n",
       "9             0.503166            0.120688         0.050317   \n",
       "10            0.503166            0.120688         0.050317   \n",
       "0             0.509820            0.273595         0.007600   \n",
       "1             0.571740            0.324604         0.009017   \n",
       "2             0.570104            0.320786         0.008911   \n",
       "3             0.570104            0.319967         0.008888   \n",
       "4             0.398117            0.096681         0.039812   \n",
       "5             0.389870            0.095375         0.038987   \n",
       "6             0.352464            0.085920         0.035246   \n",
       "7             0.381735            0.093198         0.038173   \n",
       "8             0.471482            0.113914         0.047148   \n",
       "9             0.409857            0.099921         0.040986   \n",
       "\n",
       "    cosine-Precision@3  cosine-Recall@3  ...  dot-Precision@3  dot-Recall@3  \\\n",
       "0             0.158847         0.013237  ...         0.158847      0.013237   \n",
       "1             0.158847         0.013237  ...         0.158847      0.013237   \n",
       "2             0.158847         0.013237  ...         0.158847      0.013237   \n",
       "3             0.158847         0.013237  ...         0.158847      0.013237   \n",
       "4             0.158847         0.013237  ...         0.158847      0.013237   \n",
       "5             0.596017         0.059602  ...              NaN           NaN   \n",
       "6             0.607474         0.060747  ...              NaN           NaN   \n",
       "7             0.597109         0.059711  ...              NaN           NaN   \n",
       "8             0.596017         0.059602  ...              NaN           NaN   \n",
       "9             0.691673         0.069167  ...              NaN           NaN   \n",
       "10            0.691673         0.069167  ...              NaN           NaN   \n",
       "0             0.131388         0.010949  ...         0.131388      0.010949   \n",
       "1             0.149027         0.012419  ...         0.149027      0.012419   \n",
       "2             0.148936         0.012411  ...         0.148936      0.012411   \n",
       "3             0.148936         0.012411  ...         0.148936      0.012411   \n",
       "4             0.570104         0.057010  ...              NaN           NaN   \n",
       "5             0.571740         0.057174  ...              NaN           NaN   \n",
       "6             0.511184         0.051118  ...              NaN           NaN   \n",
       "7             0.556738         0.055674  ...              NaN           NaN   \n",
       "8             0.663676         0.066368  ...              NaN           NaN   \n",
       "9             0.594760         0.059476  ...              NaN           NaN   \n",
       "\n",
       "    dot-Precision@5  dot-Recall@5  dot-Precision@10  dot-Recall@10  \\\n",
       "0          0.105892      0.014707          0.059602       0.016556   \n",
       "1          0.105892      0.014707          0.059602       0.016556   \n",
       "2          0.105892      0.014707          0.059602       0.016556   \n",
       "3          0.105892      0.014707          0.059602       0.016556   \n",
       "4          0.105892      0.014707          0.059602       0.016556   \n",
       "5               NaN           NaN               NaN            NaN   \n",
       "6               NaN           NaN               NaN            NaN   \n",
       "7               NaN           NaN               NaN            NaN   \n",
       "8               NaN           NaN               NaN            NaN   \n",
       "9               NaN           NaN               NaN            NaN   \n",
       "10              NaN           NaN               NaN            NaN   \n",
       "0          0.090125      0.012517          0.050982       0.014162   \n",
       "1          0.101146      0.014048          0.057174       0.015882   \n",
       "2          0.100055      0.013896          0.057010       0.015836   \n",
       "3          0.100055      0.013896          0.057010       0.015836   \n",
       "4               NaN           NaN               NaN            NaN   \n",
       "5               NaN           NaN               NaN            NaN   \n",
       "6               NaN           NaN               NaN            NaN   \n",
       "7               NaN           NaN               NaN            NaN   \n",
       "8               NaN           NaN               NaN            NaN   \n",
       "9               NaN           NaN               NaN            NaN   \n",
       "\n",
       "    dot-MRR@10  dot-NDCG@10  dot-MAP@100          tipo  \n",
       "0     0.421933     0.102059     0.011930    base_model  \n",
       "1     0.421933     0.102059     0.011930    base_model  \n",
       "2     0.421933     0.102059     0.011930    base_model  \n",
       "3     0.421933     0.102059     0.011930    base_model  \n",
       "4     0.422303     0.102119     0.042230    base_model  \n",
       "5          NaN          NaN          NaN    base_model  \n",
       "6          NaN          NaN          NaN    base_model  \n",
       "7          NaN          NaN          NaN    base_model  \n",
       "8          NaN          NaN          NaN    base_model  \n",
       "9          NaN          NaN          NaN    base_model  \n",
       "10         NaN          NaN          NaN    base_model  \n",
       "0     0.348023     0.085147     0.009879  custom_model  \n",
       "1     0.401730     0.097379     0.011379  custom_model  \n",
       "2     0.398395     0.096727     0.011291  custom_model  \n",
       "3     0.397986     0.096660     0.039799  custom_model  \n",
       "4          NaN          NaN          NaN  custom_model  \n",
       "5          NaN          NaN          NaN  custom_model  \n",
       "6          NaN          NaN          NaN  custom_model  \n",
       "7          NaN          NaN          NaN  custom_model  \n",
       "8          NaN          NaN          NaN  custom_model  \n",
       "9          NaN          NaN          NaN  custom_model  \n",
       "\n",
       "[21 rows x 33 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_eval = pd.read_csv('results/base_model/Information-Retrieval_evaluation_qa_eval_results.csv')\n",
    "base_model_eval['tipo'] = 'base_model'\n",
    "custom_model_eval = pd.read_csv('results/custom_model/Information-Retrieval_evaluation_qa_eval_results.csv')\n",
    "custom_model_eval['tipo'] = 'custom_model'\n",
    "\n",
    "pd.concat([base_model_eval, custom_model_eval]).to_csv('results/eval_comparation.csv', index=False)\n",
    "\n",
    "pd.concat([base_model_eval, custom_model_eval])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1 0.8037261 (answer1) -- [0.761284] (answer2)\n",
      "q2 0.7965807 (answer1) -- [0.8650476] (answer2)\n",
      "------ Base Model ------\n",
      "q1 0.6166147 (answer1) -- [0.4293761] (answer2)\n",
      "q2 0.55304617 (answer1) -- [0.7028685] (answer2)\n"
     ]
    }
   ],
   "source": [
    "# Asumiendo que los embeddings están normalizados\n",
    "question1 = \"How does the FSRA define and evaluate 'principal risks and uncertainties' for a Petroleum Reporting Entity, particularly for the remaining six months of the financial year?\"\n",
    "answer1 =  \"A Reporting Entity must: (a) prepare such report: (i) for the first six months of each financial year or period, and if there is a change to the accounting reference date, prepare such report in respect of the period up to the old accounting reference date; and (ii) in accordance with the applicable IFRS standards or other standards acceptable to the Regulator; (b) ensure the financial statements have either been audited or reviewed by auditors, and the audit or review by the auditor is included within the report; and (c) ensure that the report includes: (i) except in the case of a Mining Exploration Reporting Entity or a Petroleum Exploration Reporting Entity, an indication of important events that have occurred during the first six months of the financial year, and their impact on the financial statements; (ii) except in the case of a Mining Exploration Reporting Entity or a Petroleum Exploration Reporting Entity, a description of the principal risks and uncertainties for the remaining six months of the financial year; and (iii) a condensed set of financial statements, an interim management report and associated responsibility statements.\"\n",
    "\n",
    "question2 = 'Under Rules 7.3.2 and 7.3.3, what are the two specific conditions related to the maturity of a financial instrument that would trigger a disclosure requirement?'\n",
    "answer2 =  'Events that trigger a disclosure. For the purposes of Rules 7.3.2 and 7.3.3, a Person is taken to hold Financial Instruments in or relating to a Reporting Entity, if the Person holds a Financial Instrument that on its maturity will confer on him: (1) an unconditional right to acquire the Financial Instrument; or (2) the discretion as to his right to acquire the Financial Instrument.',\n",
    "\n",
    "\n",
    "emb_q1 = custom_domain_model.encode(question1)  # el embedding está normalizado\n",
    "emb_q2 = custom_domain_model.encode(question2)  # el embedding está normalizado\n",
    "ans_1 = custom_domain_model.encode(answer1)\n",
    "ans_2 = custom_domain_model.encode(answer2)\n",
    "\n",
    "\n",
    "print(\"q1\", ans_1 @ emb_q1,\"(answer1) --\", ans_2 @ emb_q1, \"(answer2)\")\n",
    "print(\"q2\", ans_1 @ emb_q2, \"(answer1) --\", ans_2 @ emb_q2, \"(answer2)\")\n",
    "\n",
    "\n",
    "print(\"------ Base Model ------\")\n",
    "\n",
    "emb_q1 = base_model.encode(question1)  # el embedding está normalizado\n",
    "emb_q2 = base_model.encode(question2)  # el embedding está normalizado\n",
    "ans_1 = base_model.encode(answer1)\n",
    "ans_2 = base_model.encode(answer2)\n",
    "\n",
    "\n",
    "print(\"q1\", ans_1 @ emb_q1,\"(answer1) --\", ans_2 @ emb_q1, \"(answer2)\")\n",
    "print(\"q2\", ans_1 @ emb_q2, \"(answer1) --\", ans_2 @ emb_q2, \"(answer2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The custom model mantain original capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodes sample text inputs, including the title of an article, author names, and various concepts, using both the custom domain model and the base model. Also, the dot product between the coded vectors is calculated to measure the similarity between different pairs of concepts and between the paper and a concept. Print the similarity scores for each comparison to see the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = \"Composable Lightweight Processors\"\n",
    "\n",
    "concept1 = \"shark\"\n",
    "concept2 = \"ocean\"\n",
    "concept3 = \"strawberry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producto punto entre dos conceptos (shark y ocean): 0.7008720636367798\n",
      "Producto punto entre dos conceptos (shark y strawberry): 0.6190972328186035\n",
      "Producto punto entre el documento y un concepto (ocean): 0.4101462662220001\n"
     ]
    }
   ],
   "source": [
    "custom_paper = custom_domain_model.encode(paper)\n",
    "\n",
    "custom_concept1 = custom_domain_model.encode(concept1)\n",
    "custom_concept2 = custom_domain_model.encode(concept2)\n",
    "custom_concept3 = custom_domain_model.encode(concept3)\n",
    "\n",
    "# Imprimir los resultados y explicaciones\n",
    "print(f\"Producto punto entre dos conceptos (shark y ocean): {np.dot(custom_concept1, custom_concept2)}\")\n",
    "print(f\"Producto punto entre dos conceptos (shark y strawberry): {np.dot(custom_concept1, custom_concept3)}\")\n",
    "print(f\"Producto punto entre el documento y un concepto (ocean): {np.dot(custom_paper, custom_concept2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producto punto entre dos conceptos (shark y ocean): 0.5527569055557251\n",
      "Producto punto entre dos conceptos (shark y strawberry): 0.27426061034202576\n",
      "Producto punto entre el documento y un concepto (ocean): -0.05138666182756424\n"
     ]
    }
   ],
   "source": [
    "base_paper = base_model.encode(paper)\n",
    "\n",
    "base_concept1 = base_model.encode(concept1)\n",
    "base_concept2 = base_model.encode(concept2)\n",
    "base_concept3 = base_model.encode(concept3)  \n",
    "\n",
    "# Imprimir los resultados y explicaciones\n",
    "print(f\"Producto punto entre dos conceptos (shark y ocean): {np.dot(base_concept1, base_concept2)}\")\n",
    "print(f\"Producto punto entre dos conceptos (shark y strawberry): {np.dot(base_concept1, base_concept3)}\")\n",
    "print(f\"Producto punto entre el documento y un concepto (ocean): {np.dot(base_paper, base_concept2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
