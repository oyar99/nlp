{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 24 13:47:31 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40-24Q                 On  |   00000000:00:10.0 Off |                  N/A |\n",
      "| N/A   N/A    P0             N/A /  N/A  |     684MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       820      G   /usr/lib/xorg/Xorg                             23MiB |\n",
      "|    0   N/A  N/A     98171      C   /home/estudiante/venv/bin/python3             660MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/estudiante/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-10-24 13:47:33.781481: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-24 13:47:33.823167: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-24 13:47:33.823200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-24 13:47:33.824216: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-24 13:47:33.831225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-24 13:47:34.802070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import (SentenceTransformer, models, \n",
    "                                   SentenceTransformerTrainingArguments,\n",
    "                                   SentenceTransformerTrainer)\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.util import dot_score\n",
    "\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss,MultipleNegativesSymmetricRankingLoss\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import create_data_for_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom SentenceTransformer model is configured by defining and configuring several components, including a word embedding model, a grouping model, a normalization layer, and an adapter module. The word embedding model is initialized with a pre-trained transformer model from Sentence Transformers, with specific settings for a maximum sequence length and case-sensitivity. The pooling model is configured to use token averaging for pooling, with other pooling modes disabled. The normalization layer is defined to standardize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga del modelo base de embeddings de palabras Sentence Transformer\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "base_model = SentenceTransformer(\n",
    "    model_name,                  # Nombre del modelo preentrenado de Sentence Transformers\n",
    "    device=\"cuda\",               # Utilizar GPU para entrenar (dispositivo CUDA)\n",
    "    model_kwargs={\"torch_dtype\": \"float16\"},  # Configurar el modelo para usar FP16 (precisión reducida) para optimizar la memoria\n",
    ")\n",
    "\n",
    "# Mostrar la arquitectura del modelo cargado\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 512, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear la capa de embeddings de palabras usando el mismo modelo preentrenado\n",
    "word_embedding_model = models.Transformer(\n",
    "    model_name_or_path=model_name,  # Usar el mismo modelo preentrenado (sentence-transformers/all-MiniLM-L6-v2)\n",
    "    max_seq_length=512,  \n",
    "    do_lower_case=True,  # No convertir las palabras a minúsculas, mantener la capitalización original\n",
    ")\n",
    "\n",
    "# Definir los parámetros del modelo de pooling\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_dimension=512,  # Dimensión de los embeddings (incrementada a 768 para el nuevo modelo)\n",
    "    pooling_mode_cls_token=True,  # Usar el token [CLS] para el pooling\n",
    "    pooling_mode_mean_tokens=False,  # No usar el promedio de los tokens como estrategia de pooling\n",
    "    pooling_mode_max_tokens=False,  # No usar el token máximo para el pooling\n",
    "    pooling_mode_mean_sqrt_len_tokens=False,  # No usar la raíz cuadrada de la longitud de los tokens para el pooling\n",
    "    pooling_mode_weightedmean_tokens=False,  # No usar un promedio ponderado de los tokens\n",
    "    pooling_mode_lasttoken=False,  # No usar el último token para el pooling\n",
    "    include_prompt=True  # Incluir el \"prompt\" en el proceso de pooling\n",
    ")\n",
    "\n",
    "# Definir una capa de normalización para el modelo personalizado\n",
    "normalize = models.Normalize()\n",
    "\n",
    "\n",
    "# Definir el modelo personalizado de Sentence Transformer que incluye:\n",
    "# - word_embedding_model: La capa de embeddings de palabras con el modelo MiniLM\n",
    "# - pooling_model: El modelo de pooling personalizado\n",
    "# - normalize: La capa de normalización para mantener la estabilidad del espacio de los embeddings\n",
    "custom_domain_model = SentenceTransformer(\n",
    "    modules=[word_embedding_model, pooling_model, normalize],  # Definir las capas del modelo\n",
    "    device=\"cuda\"  # Usar GPU para entrenar\n",
    ")\n",
    "\n",
    "# Mostrar la arquitectura del modelo personalizado\n",
    "custom_domain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and evaluation datasets for question answering tasks from the respective pickled files stored in the 'data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_train = load_from_disk('./data/train_dataset')\n",
    "qa_eval = load_from_disk('./data/eval_dataset')\n",
    "qa_test = load_from_disk('./data/test_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training examples using the question-answer pairs from the dataset `qa`, where each example consists of a question (`qa[0]`) and its corresponding answer (`qa[1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lenght:  29547\n",
      "Validation lenght:  3677\n",
      "Test lenght:  3666\n"
     ]
    }
   ],
   "source": [
    "print(\"Training lenght: \", len(qa_train))\n",
    "print(\"Validation lenght: \", len(qa_eval))\n",
    "print(\"Test lenght: \", len(qa_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepares and configures the training and evaluation process for a custom SentenceTransformer model. Initially, a training data set is created by generating a list of `InputExample` instances, where each instance consists of a pair of texts (question and answer). This data set is then loaded into a \"DataLoader\", which shuffles the data at each epoch and sets the batch size to 256.\n",
    "\n",
    "The training loss is defined using \"MultipleNegativesSymmetricRankingLoss\", which is suitable for information retrieval tasks involving positive text pairs. An evaluator is configured using \"InformationRetrievalEvaluator\", which evaluates the performance of the model on a set of queries and corpora, with the main scoring function specified as \"dot_score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_evaluator = create_data_for_evaluator(qa_eval)\n",
    "test_dataset_evaluator = create_data_for_evaluator(qa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=eval_dataset_evaluator['queries'],\n",
    "        corpus=eval_dataset_evaluator['corpus'],\n",
    "        relevant_docs=eval_dataset_evaluator['relevant_docs'],\n",
    "        name='qa_eval', \n",
    "        map_at_k=[10],\n",
    "        accuracy_at_k = [10],\n",
    "        precision_recall_at_k = [10],\n",
    "        score_functions={'dot_score':dot_score}\n",
    "    )\n",
    "\n",
    "test_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=test_dataset_evaluator['queries'],\n",
    "        corpus=test_dataset_evaluator['corpus'],\n",
    "        relevant_docs=test_dataset_evaluator['relevant_docs'],\n",
    "        name='qa_test', \n",
    "        map_at_k=[10],\n",
    "        accuracy_at_k = [10],\n",
    "        precision_recall_at_k = [10],\n",
    "        score_functions={'dot_score':dot_score}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_eval_dot_score_accuracy@10': 0.7908895265423243,\n",
       " 'qa_eval_dot_score_precision@10': 0.08418220946915352,\n",
       " 'qa_eval_dot_score_recall@10': 0.7134684361549498,\n",
       " 'qa_eval_dot_score_ndcg@10': 0.6009030811596184,\n",
       " 'qa_eval_dot_score_mrr@10': 0.6015105839083594,\n",
       " 'qa_eval_dot_score_map@10': 0.5462275469130742}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Base model evaluation\n",
    "\n",
    "results = dev_evaluator(base_model)\n",
    "\n",
    "results #'qa_eval_dot_score_recall@10': 0.7134684361549498,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3450' max='3450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3450/3450 42:17, Epoch 29/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Qa Eval Dot Score Accuracy@10</th>\n",
       "      <th>Qa Eval Dot Score Precision@10</th>\n",
       "      <th>Qa Eval Dot Score Recall@10</th>\n",
       "      <th>Qa Eval Dot Score Ndcg@10</th>\n",
       "      <th>Qa Eval Dot Score Mrr@10</th>\n",
       "      <th>Qa Eval Dot Score Map@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.849354</td>\n",
       "      <td>0.091679</td>\n",
       "      <td>0.769566</td>\n",
       "      <td>0.651601</td>\n",
       "      <td>0.656308</td>\n",
       "      <td>0.592498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.916900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.879842</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>0.797884</td>\n",
       "      <td>0.676039</td>\n",
       "      <td>0.680668</td>\n",
       "      <td>0.614976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.943600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.887733</td>\n",
       "      <td>0.096521</td>\n",
       "      <td>0.807030</td>\n",
       "      <td>0.684091</td>\n",
       "      <td>0.687833</td>\n",
       "      <td>0.622670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.887016</td>\n",
       "      <td>0.097095</td>\n",
       "      <td>0.808178</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.693476</td>\n",
       "      <td>0.629333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.887374</td>\n",
       "      <td>0.097418</td>\n",
       "      <td>0.808871</td>\n",
       "      <td>0.690048</td>\n",
       "      <td>0.694033</td>\n",
       "      <td>0.630086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.890603</td>\n",
       "      <td>0.097956</td>\n",
       "      <td>0.812231</td>\n",
       "      <td>0.693309</td>\n",
       "      <td>0.696979</td>\n",
       "      <td>0.633546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.888809</td>\n",
       "      <td>0.097884</td>\n",
       "      <td>0.811633</td>\n",
       "      <td>0.688877</td>\n",
       "      <td>0.691145</td>\n",
       "      <td>0.627882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.889885</td>\n",
       "      <td>0.097884</td>\n",
       "      <td>0.811819</td>\n",
       "      <td>0.691072</td>\n",
       "      <td>0.694041</td>\n",
       "      <td>0.630691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.888809</td>\n",
       "      <td>0.097884</td>\n",
       "      <td>0.811693</td>\n",
       "      <td>0.689904</td>\n",
       "      <td>0.693029</td>\n",
       "      <td>0.629179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.368400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.891320</td>\n",
       "      <td>0.098207</td>\n",
       "      <td>0.814198</td>\n",
       "      <td>0.690082</td>\n",
       "      <td>0.691825</td>\n",
       "      <td>0.628774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.490900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.884864</td>\n",
       "      <td>0.097597</td>\n",
       "      <td>0.808967</td>\n",
       "      <td>0.684204</td>\n",
       "      <td>0.685868</td>\n",
       "      <td>0.622761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883788</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.808327</td>\n",
       "      <td>0.685450</td>\n",
       "      <td>0.686925</td>\n",
       "      <td>0.624807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.886298</td>\n",
       "      <td>0.097848</td>\n",
       "      <td>0.810868</td>\n",
       "      <td>0.682975</td>\n",
       "      <td>0.683086</td>\n",
       "      <td>0.620868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.885222</td>\n",
       "      <td>0.097956</td>\n",
       "      <td>0.810420</td>\n",
       "      <td>0.680777</td>\n",
       "      <td>0.679438</td>\n",
       "      <td>0.618201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.886298</td>\n",
       "      <td>0.098171</td>\n",
       "      <td>0.811884</td>\n",
       "      <td>0.681333</td>\n",
       "      <td>0.680379</td>\n",
       "      <td>0.618383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.884864</td>\n",
       "      <td>0.097956</td>\n",
       "      <td>0.810491</td>\n",
       "      <td>0.679206</td>\n",
       "      <td>0.677640</td>\n",
       "      <td>0.616284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.887733</td>\n",
       "      <td>0.098135</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>0.678929</td>\n",
       "      <td>0.676950</td>\n",
       "      <td>0.615169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.243600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883429</td>\n",
       "      <td>0.097740</td>\n",
       "      <td>0.808608</td>\n",
       "      <td>0.676505</td>\n",
       "      <td>0.674428</td>\n",
       "      <td>0.613390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883070</td>\n",
       "      <td>0.097525</td>\n",
       "      <td>0.807329</td>\n",
       "      <td>0.674405</td>\n",
       "      <td>0.672212</td>\n",
       "      <td>0.610772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.097525</td>\n",
       "      <td>0.807162</td>\n",
       "      <td>0.673338</td>\n",
       "      <td>0.670688</td>\n",
       "      <td>0.609602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881994</td>\n",
       "      <td>0.097597</td>\n",
       "      <td>0.806791</td>\n",
       "      <td>0.675685</td>\n",
       "      <td>0.673350</td>\n",
       "      <td>0.612751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.097489</td>\n",
       "      <td>0.806731</td>\n",
       "      <td>0.676534</td>\n",
       "      <td>0.674854</td>\n",
       "      <td>0.613918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.303300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881277</td>\n",
       "      <td>0.097597</td>\n",
       "      <td>0.807371</td>\n",
       "      <td>0.677276</td>\n",
       "      <td>0.675343</td>\n",
       "      <td>0.614782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883429</td>\n",
       "      <td>0.097884</td>\n",
       "      <td>0.809164</td>\n",
       "      <td>0.677877</td>\n",
       "      <td>0.675549</td>\n",
       "      <td>0.615024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881636</td>\n",
       "      <td>0.097633</td>\n",
       "      <td>0.807520</td>\n",
       "      <td>0.676593</td>\n",
       "      <td>0.674329</td>\n",
       "      <td>0.613831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881994</td>\n",
       "      <td>0.097776</td>\n",
       "      <td>0.808447</td>\n",
       "      <td>0.676659</td>\n",
       "      <td>0.674046</td>\n",
       "      <td>0.613658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881636</td>\n",
       "      <td>0.097812</td>\n",
       "      <td>0.808357</td>\n",
       "      <td>0.676391</td>\n",
       "      <td>0.673565</td>\n",
       "      <td>0.613336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.097884</td>\n",
       "      <td>0.808835</td>\n",
       "      <td>0.676861</td>\n",
       "      <td>0.674220</td>\n",
       "      <td>0.613788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881636</td>\n",
       "      <td>0.097812</td>\n",
       "      <td>0.808118</td>\n",
       "      <td>0.676283</td>\n",
       "      <td>0.673611</td>\n",
       "      <td>0.613229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "# Definición de la función de pérdida usando MultipleNegativesSymmetricRankingLoss\n",
    "# Esta pérdida está diseñada para el entrenamiento de modelos de recuperación de información donde \n",
    "# se tienen pares de preguntas y respuestas, y se desea maximizar la similitud entre ellos.\n",
    "# 'similarity_fct=dot_score' especifica que se utiliza el producto punto (dot product) como función de similitud\n",
    "loss = MultipleNegativesRankingLoss(custom_domain_model,\n",
    "                                    similarity_fct=dot_score)\n",
    "\n",
    "# Configuración de los parámetros de entrenamiento mediante SentenceTransformerTrainingArguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"./results/domain_adaptation_model\",  # Directorio donde se guardarán los resultados y checkpoints del modelo\n",
    "    num_train_epochs=15,  # Número de épocas para el entrenamiento\n",
    "    per_device_train_batch_size=64,  # Tamaño del batch para entrenamiento por dispositivo\n",
    "    gradient_accumulation_steps=4,  # Acumulación de gradientes para simular un batch de tamaño más grande (4 acumulaciones para tener un batch equivalente a 512)\n",
    "    per_device_eval_batch_size=512,  # Tamaño del batch para evaluación\n",
    "    learning_rate=2e-5,  # Tasa de aprendizaje inicial\n",
    "    warmup_ratio=0.1,  # Proporción de pasos de calentamiento (10% de los pasos de entrenamiento)\n",
    "    bf16=True,  # Utiliza el formato bfloat16 para reducir el uso de memoria durante el entrenamiento en hardware compatible\n",
    "    gradient_checkpointing=False,  # Checkpointing de gradientes para reducir el uso de memoria al costo de mayor cómputo\n",
    "    optim=\"adamw_torch_fused\",  # Optimizador AdamW con fusión de operaciones, más eficiente en hardware compatible\n",
    "    lr_scheduler_type=\"cosine\",  # Planificador de tasa de aprendizaje con decaimiento cosenoidal\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Utiliza un muestreo de batches que garantiza que no haya duplicados\n",
    "    eval_strategy=\"epoch\",  # Realiza evaluación al final de cada época\n",
    "    save_strategy=\"epoch\",  # Guarda un checkpoint al final de cada época\n",
    "    save_total_limit=1,  # Mantiene un límite de 3 checkpoints, eliminando los más antiguos\n",
    "    logging_steps=1,  # Registra los resultados del entrenamiento cada 25 pasos\n",
    "    metric_for_best_model=\"eval_qa_eval_dot_score_recall@10\",  # Métrica clave utilizada para determinar el mejor modelo (MAP@10 en este caso)\n",
    "    greater_is_better=True,  # Indica que un valor mayor de la métrica es mejor (se utiliza para seleccionar el mejor modelo)\n",
    "    load_best_model_at_end=True,  # Carga automáticamente el mejor modelo al final del entrenamiento\n",
    ")\n",
    "\n",
    "# Creación del entrenador con los parámetros definidos\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=custom_domain_model,  # Modelo que se va a entrenar (previamente definido)\n",
    "    args=args,  # Argumentos de entrenamiento definidos previamente\n",
    "    train_dataset=qa_train.select_columns([\"anchor\", \"positive\"]),  # Dataset de entrenamiento, solo las columnas \"anchor\" y \"positive\"\n",
    "    loss=loss,  # Función de pérdida definida para el entrenamiento\n",
    "    evaluator=dev_evaluator,  # Evaluador que se encargará de evaluar el rendimiento en cada época\n",
    ")\n",
    "\n",
    "# Inicia el proceso de entrenamiento\n",
    "trainer.train()\n",
    "\n",
    "# Guarda el modelo final entrenado\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of training epochs is set to 30 and the warm-up steps are calculated as 10% of the total training steps, determined by the length of the DataLoader and the number of epochs. This setup ensures that the model is properly prepared and evaluated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the base model & the fine tunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_domain_model = SentenceTransformer('./results/domain_adaptation_model',\n",
    "                                          device=\"cuda\",\n",
    "                                          model_kwargs={\"torch_dtype\": \"float16\"},\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_eval_dot_score_accuracy@10': 0.8902439024390244,\n",
       " 'qa_eval_dot_score_precision@10': 0.09809899569583931,\n",
       " 'qa_eval_dot_score_recall@10': 0.8133010521281683,\n",
       " 'qa_eval_dot_score_ndcg@10': 0.6904537357707828,\n",
       " 'qa_eval_dot_score_mrr@10': 0.6927355901710266,\n",
       " 'qa_eval_dot_score_map@10': 0.6294921830520825}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_evaluator(custom_domain_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Mean Average Precision (MAP) at k=10 for both the base and custom domain models using the evaluator, and print the results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model:  {'qa_test_dot_score_accuracy@10': 0.7763819095477387, 'qa_test_dot_score_precision@10': 0.08248384781048097, 'qa_test_dot_score_recall@10': 0.7017109356305337, 'qa_test_dot_score_ndcg@10': 0.5896317890507138, 'qa_test_dot_score_mrr@10': 0.5893142013924492, 'qa_test_dot_score_map@10': 0.5357515444759702}\n",
      "\n",
      "-----------\n",
      "\n",
      "Custom model:  {'qa_test_dot_score_accuracy@10': 0.8822684852835606, 'qa_test_dot_score_precision@10': 0.09720028715003588, 'qa_test_dot_score_recall@10': 0.808698253170615, 'qa_test_dot_score_ndcg@10': 0.6882911157662472, 'qa_test_dot_score_mrr@10': 0.6910618853906713, 'qa_test_dot_score_map@10': 0.6290388972451069}\n"
     ]
    }
   ],
   "source": [
    "eva_base_model = test_evaluator(base_model, output_path='results/base_model/')\n",
    "print(\"Base model: \", eva_base_model)\n",
    "print(\"\\n-----------\\n\")\n",
    "eva_custom_model = test_evaluator(custom_domain_model, output_path='results/custom_model/')\n",
    "print(\"Custom model: \", eva_custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_domain_model.push_to_hub('raul-delarosa99/bge-small-en-v1.5-RIRAG_ObliQA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1 0.6978 (answer1) -- -0.02693 (answer2)\n",
      "q2 0.09546 (answer1) -- 0.8774 (answer2)\n",
      "------ Base Model ------\n",
      "q1 0.8325 (answer1) -- 0.56 (answer2)\n",
      "q2 0.581 (answer1) -- 0.9155 (answer2)\n"
     ]
    }
   ],
   "source": [
    "# Asumiendo que los embeddings están normalizados\n",
    "question1 = \"In the context of selling Direct Long-Term Insurance to Retail Clients, can you identify the rule that mandates insurers and insurance intermediaries to ensure that the insurance products are suitable for their clients?\"\n",
    "answer1 =  \"An Insurer or an Insurance Intermediary must comply with the suitability requirement set out in Rule ‎3.4 when conducting any Insurance or Insurance Intermediation Business with or for a Retail Client in respect of Direct Long-Term Insurance.\"\n",
    "\n",
    "question2 = 'Under what circumstances, as outlined in Rule ‎12.3.2, is a Fund Manager of a Domestic Fund not mandated to engage the services of an Eligible Custodian?'\n",
    "answer2 =  'A Fund Manager of a Domestic Fund is not required to appoint an Eligible Custodian for the Fund pursuant to Rule ‎12.3.2 where it meets the requirements in either (2) and (3), or (4).'\n",
    "\n",
    "\n",
    "emb_q1 = custom_domain_model.encode(question1)  # el embedding está normalizado\n",
    "emb_q2 = custom_domain_model.encode(question2)  # el embedding está normalizado\n",
    "ans_1 = custom_domain_model.encode(answer1)\n",
    "ans_2 = custom_domain_model.encode(answer2)\n",
    "\n",
    "\n",
    "print(\"q1\", ans_1 @ emb_q1,\"(answer1) --\", ans_2 @ emb_q1, \"(answer2)\")\n",
    "print(\"q2\", ans_1 @ emb_q2, \"(answer1) --\", ans_2 @ emb_q2, \"(answer2)\")\n",
    "\n",
    "\n",
    "print(\"------ Base Model ------\")\n",
    "\n",
    "emb_q1 = base_model.encode(question1)  # el embedding está normalizado\n",
    "emb_q2 = base_model.encode(question2)  # el embedding está normalizado\n",
    "ans_1 = base_model.encode(answer1)\n",
    "ans_2 = base_model.encode(answer2)\n",
    "\n",
    "\n",
    "print(\"q1\", ans_1 @ emb_q1,\"(answer1) --\", ans_2 @ emb_q1, \"(answer2)\")\n",
    "print(\"q2\", ans_1 @ emb_q2, \"(answer1) --\", ans_2 @ emb_q2, \"(answer2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The custom model mantain original capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodes sample text inputs, including the title of an article, author names, and various concepts, using both the custom domain model and the base model. Also, the dot product between the coded vectors is calculated to measure the similarity between different pairs of concepts and between the paper and a concept. Print the similarity scores for each comparison to see the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = \"Composable Lightweight Processors\"\n",
    "\n",
    "concept1 = \"shark\"\n",
    "concept2 = \"ocean\"\n",
    "concept3 = \"strawberry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Fine Tunned Model ------\n",
      "Producto punto entre dos conceptos (shark y ocean): 0.5361328125\n",
      "Producto punto entre dos conceptos (shark y strawberry): 0.55419921875\n",
      "Producto punto entre el documento y un concepto (ocean): 0.2333984375\n"
     ]
    }
   ],
   "source": [
    "custom_paper = custom_domain_model.encode(paper)\n",
    "\n",
    "custom_concept1 = custom_domain_model.encode(concept1)\n",
    "custom_concept2 = custom_domain_model.encode(concept2)\n",
    "custom_concept3 = custom_domain_model.encode(concept3)\n",
    "\n",
    "# Imprimir los resultados y explicaciones\n",
    "print(\"------ Fine Tunned Model ------\")\n",
    "print(f\"Producto punto entre dos conceptos (shark y ocean): {np.dot(custom_concept1, custom_concept2)}\")\n",
    "print(f\"Producto punto entre dos conceptos (shark y strawberry): {np.dot(custom_concept1, custom_concept3)}\")\n",
    "print(f\"Producto punto entre el documento y un concepto (ocean): {np.dot(custom_paper, custom_concept2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Base Model ------\n",
      "Producto punto entre dos conceptos (shark y ocean): 0.72607421875\n",
      "Producto punto entre dos conceptos (shark y strawberry): 0.65625\n",
      "Producto punto entre el documento y un concepto (ocean): 0.47900390625\n"
     ]
    }
   ],
   "source": [
    "base_paper = base_model.encode(paper)\n",
    "\n",
    "base_concept1 = base_model.encode(concept1)\n",
    "base_concept2 = base_model.encode(concept2)\n",
    "base_concept3 = base_model.encode(concept3)  \n",
    "\n",
    "# Imprimir los resultados y explicaciones\n",
    "print(\"------ Base Model ------\")\n",
    "print(f\"Producto punto entre dos conceptos (shark y ocean): {np.dot(base_concept1, base_concept2)}\")\n",
    "print(f\"Producto punto entre dos conceptos (shark y strawberry): {np.dot(base_concept1, base_concept3)}\")\n",
    "print(f\"Producto punto entre el documento y un concepto (ocean): {np.dot(base_paper, base_concept2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
