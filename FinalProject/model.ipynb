{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegNLP - Fine tuned sentence transformer model\n",
    "\n",
    "This notebook details the training process of the text embedding model [raul-delarosa99/bge-small-en-v1.5-RIRAG_ObliQA](https://huggingface.co/raul-delarosa99/bge-small-en-v1.5-RIRAG_ObliQA). The model is derived by fine-tuning the  model [ BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) using a dataset that contains regulatory passages that require specialized knowledge to accurately interpret compliance requirements. This model aims to support academic research in the field of Regulatory Natural Language Processing (RegNLP). \n",
    "\n",
    "This notebook depends on the `data_processing.ipynb` notebook. Please ensure that notebook is run before training the model so the raw data is preprocessed as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "First, let us validate the hardware specs that will be used to train this model by using the NVIDIA command tool `nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 31 13:02:26 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA A40-24Q                 On  |   00000000:00:10.0 Off |                  N/A |\r\n",
      "| N/A   N/A    P0             N/A /  N/A  |      24MiB /  24576MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    0   N/A  N/A       820      G   /usr/lib/xorg/Xorg                             23MiB |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Sentence-transformer related utils - See https://www.sbert.net/\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    models, \n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerTrainer\n",
    ")\n",
    "from sentence_transformers.losses import (\n",
    "    MultipleNegativesRankingLoss,\n",
    "    MultipleNegativesSymmetricRankingLoss\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.util import dot_score\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "# Other libs for data handling\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_from_disk\n",
    ")\n",
    "from pandas import read_pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "We will configure a custom sentence transformer model by defining several components, including a word embedding model, a pooling layer and a normalization layer. The word embedding model is initialized with a pre-trained transformer model ([BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)), with specific settings for a maximum sequence length and case-sensitivity. The normalization layer is defined to standardize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base model\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "base_model = SentenceTransformer(\n",
    "    model_name,                  # Name of the base model based on BERT\n",
    "    device=\"cuda\",               # Use GPU for training\n",
    "    model_kwargs={\"torch_dtype\": \"float16\"},  # Use FP16 precision to reduce memory consumption\n",
    ")\n",
    "\n",
    "# Display the base model architecture\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 512, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we will set up the word embedding model using the base model\n",
    "word_embedding_model = models.Transformer(\n",
    "    model_name_or_path=model_name,  # Use BAAI/bge-small-en-v1.5\n",
    "    max_seq_length=512,\n",
    "    do_lower_case=True,  # Convert all text to lowercase\n",
    ")\n",
    "\n",
    "# Define the pooling layer\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_dimension=512,  # word embeddings dimensional space\n",
    "    pooling_mode_cls_token=True,  # Use [CLS] token\n",
    "    pooling_mode_mean_tokens=False,  # Do not use mean tokens for pooling\n",
    "    pooling_mode_max_tokens=False,  # Do not use max token for pooling\n",
    "    pooling_mode_mean_sqrt_len_tokens=False,  # Do not use sqr len token for pooling\n",
    "    pooling_mode_weightedmean_tokens=False,  # Do not use the weighted mean token for pooling\n",
    "    pooling_mode_lasttoken=False,  # Do not use last token for pooling\n",
    "    include_prompt=True  # Include prompt during pooling\n",
    ")\n",
    "\n",
    "# Define the normalization layer\n",
    "normalize = models.Normalize()\n",
    "\n",
    "# Define our custom model which consists of\n",
    "# - word_embedding_model: The word embedding layer using BAAI/bge-small-en-v1.5\n",
    "# - pooling_model: The pooling layer\n",
    "# - normalize: The normalization layer\n",
    "custom_domain_model = SentenceTransformer(\n",
    "    modules=[word_embedding_model, pooling_model, normalize],  # our model layers\n",
    "    device=\"cuda\"  # Use GPU for training\n",
    ")\n",
    "\n",
    "# Display the custom model architecture\n",
    "custom_domain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and evaluation datasets for question answering tasks from the respective pickle files stored in the `data` directory. (These pickle files are obtained after running the `data_processing` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_train = load_from_disk('./data/train_dataset')\n",
    "qa_eval = load_from_disk('./data/eval_dataset')\n",
    "qa_test = load_from_disk('./data/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lenght:  29547\n",
      "Validation lenght:  3677\n",
      "Test lenght:  3666\n"
     ]
    }
   ],
   "source": [
    "print(\"Training lenght: \", len(qa_train))\n",
    "print(\"Validation lenght: \", len(qa_eval))\n",
    "print(\"Test lenght: \", len(qa_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a function to process the dataset so we can use it with the sentence transformer lib\n",
    "corpus = read_pickle('./data/corpus.pkl') # Our corpus (cid => document)\n",
    "\n",
    "def create_data_for_evaluator(dataset:Dataset) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a data structure for the evaluator from the given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset containing 'anchor_id', 'anchor', and 'positive_id' fields.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the corpus, queries, and relevant documents.\n",
    "              - 'corpus': The corpus of documents\n",
    "              - 'queries': A dictionary mapping query IDs to queries.\n",
    "              - 'relevant_docs': A dictionary mapping query IDs to lists of relevant document IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    queries = dict(\n",
    "        zip(dataset['anchor_id'], \n",
    "            dataset['anchor'])\n",
    "    )  # Our queries (qid => question)\n",
    "\n",
    "    # Create a mapping of relevant document (1 in our case) for each query\n",
    "    relevant_docs = {qid:[] for qid in dataset['anchor_id']}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "    for qid, cid  in zip(dataset['anchor_id'], dataset['positive_id']):\n",
    "        relevant_docs[qid].append(cid)\n",
    "        \n",
    "    return dict(corpus=corpus, queries=queries, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepares and configures the evaluation process for the base model using the `InformationRetrievalEvaluator` using the `dot_score` function. This will serve as our baseline once we train our custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for the validation dataset\n",
    "eval_dataset_evaluator = create_data_for_evaluator(qa_eval)\n",
    "# evaluator for the testing dataset\n",
    "test_dataset_evaluator = create_data_for_evaluator(qa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the information retrieval evaluator which given a set of queries and a corpus, retrieves for each query the top k\n",
    "# most similar docs. We use k=10 and the dot score function (Dot product)\n",
    "dev_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=eval_dataset_evaluator['queries'],\n",
    "        corpus=eval_dataset_evaluator['corpus'],\n",
    "        relevant_docs=eval_dataset_evaluator['relevant_docs'],\n",
    "        name='qa_eval', \n",
    "        map_at_k=[10],\n",
    "        accuracy_at_k = [10],\n",
    "        precision_recall_at_k = [10],\n",
    "        score_functions={'dot_score':dot_score}\n",
    "    )\n",
    "\n",
    "test_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=test_dataset_evaluator['queries'],\n",
    "        corpus=test_dataset_evaluator['corpus'],\n",
    "        relevant_docs=test_dataset_evaluator['relevant_docs'],\n",
    "        name='qa_test', \n",
    "        map_at_k=[10],\n",
    "        accuracy_at_k = [10],\n",
    "        precision_recall_at_k = [10],\n",
    "        score_functions={'dot_score':dot_score}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_eval_dot_score_accuracy@10': 0.7908895265423243,\n",
       " 'qa_eval_dot_score_precision@10': 0.08418220946915352,\n",
       " 'qa_eval_dot_score_recall@10': 0.7134684361549498,\n",
       " 'qa_eval_dot_score_ndcg@10': 0.6009030811596184,\n",
       " 'qa_eval_dot_score_mrr@10': 0.6015105839083594,\n",
       " 'qa_eval_dot_score_map@10': 0.5462275469130742}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Base model evaluation\n",
    "results = dev_evaluator(base_model)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 13:42, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Qa Eval Dot Score Accuracy@10</th>\n",
       "      <th>Qa Eval Dot Score Precision@10</th>\n",
       "      <th>Qa Eval Dot Score Recall@10</th>\n",
       "      <th>Qa Eval Dot Score Ndcg@10</th>\n",
       "      <th>Qa Eval Dot Score Mrr@10</th>\n",
       "      <th>Qa Eval Dot Score Map@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.875897</td>\n",
       "      <td>0.094548</td>\n",
       "      <td>0.793430</td>\n",
       "      <td>0.669875</td>\n",
       "      <td>0.674248</td>\n",
       "      <td>0.608332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.846700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.887016</td>\n",
       "      <td>0.096521</td>\n",
       "      <td>0.806576</td>\n",
       "      <td>0.683009</td>\n",
       "      <td>0.687008</td>\n",
       "      <td>0.621320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.874700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.888092</td>\n",
       "      <td>0.097095</td>\n",
       "      <td>0.809356</td>\n",
       "      <td>0.686894</td>\n",
       "      <td>0.689791</td>\n",
       "      <td>0.625929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.888092</td>\n",
       "      <td>0.097525</td>\n",
       "      <td>0.810390</td>\n",
       "      <td>0.689621</td>\n",
       "      <td>0.692547</td>\n",
       "      <td>0.629218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.417300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.892037</td>\n",
       "      <td>0.097920</td>\n",
       "      <td>0.812267</td>\n",
       "      <td>0.691733</td>\n",
       "      <td>0.695240</td>\n",
       "      <td>0.631234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.554600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.892396</td>\n",
       "      <td>0.097920</td>\n",
       "      <td>0.812321</td>\n",
       "      <td>0.691182</td>\n",
       "      <td>0.694321</td>\n",
       "      <td>0.630474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.892755</td>\n",
       "      <td>0.098135</td>\n",
       "      <td>0.813887</td>\n",
       "      <td>0.691763</td>\n",
       "      <td>0.694289</td>\n",
       "      <td>0.630956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.893831</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>0.814981</td>\n",
       "      <td>0.692382</td>\n",
       "      <td>0.694677</td>\n",
       "      <td>0.631357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.894907</td>\n",
       "      <td>0.098458</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>0.692973</td>\n",
       "      <td>0.695109</td>\n",
       "      <td>0.631785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                             \r"
     ]
    }
   ],
   "source": [
    "# We use the loss function MultipleNegativesSymmetricRankingLoss - \n",
    "# See https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html?highlight=multiplenegativessymmetricrankingloss#multiplenegativessymmetricrankingloss\n",
    "# \n",
    "# This loss is desgined for information retrieval use cases where we have pairs of questions/answers and \n",
    "# we want to maximize the similarity between those.\n",
    "# 'similarity_fct=dot_score' means we use the dot product as the similarity function\n",
    "loss = MultipleNegativesRankingLoss(custom_domain_model,\n",
    "                                    similarity_fct=dot_score)\n",
    "\n",
    "# Configure training parameters\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"./results/custom_domain_model\",  # Directory to store the results/checkpoints of the model\n",
    "    num_train_epochs=10,  # Numer of epochs\n",
    "    per_device_train_batch_size=64,  # Batch size for training\n",
    "    gradient_accumulation_steps=4, # Accumulation steps\n",
    "    per_device_eval_batch_size=512,  # Batch size for evaluation\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    warmup_ratio=0.1,  # Proportion of warmup steps\n",
    "    bf16=True,  # Use bfloat16 to reduce memory usage\n",
    "    gradient_checkpointing=False, \n",
    "    optim=\"adamw_torch_fused\",  # Use a version of adam optmizer for gradient descent\n",
    "    lr_scheduler_type=\"cosine\",  # Learning rate scheduler with cosine decay\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Uses batch sampling that ensures no duplicates\n",
    "    eval_strategy=\"epoch\",  # Evaluates at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Saves a checkpoint at the end of each epoch\n",
    "    save_total_limit=1,  # Keeps a limit of 1 checkpoint, deleting the oldest\n",
    "    logging_steps=1,  # Logs training results every step\n",
    "    metric_for_best_model=\"eval_qa_eval_dot_score_recall@10\",  # Key metric used to determine the best model (MAP@10 in this case)\n",
    "    greater_is_better=True,  # Indicates that a higher value of the metric is better (used to select the best model)\n",
    "    load_best_model_at_end=True,  # Automatically loads the best model at the end of training\n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=custom_domain_model,  # Custom model to train\n",
    "    args=args,  # training args\n",
    "    train_dataset=qa_train.select_columns([\"anchor\", \"positive\"]),  # training dataset using only query and positive samples\n",
    "    loss=loss,  # loss function\n",
    "    evaluator=dev_evaluator,  # evaluator to validate model after each epoch\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model to disk \n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the base model & the fine tunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from disk\n",
    "custom_domain_model = SentenceTransformer('./results/custom_domain_model',\n",
    "                                          device=\"cuda\",\n",
    "                                          model_kwargs={\"torch_dtype\": \"float16\"},\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_eval_dot_score_accuracy@10': 0.8945480631276901,\n",
       " 'qa_eval_dot_score_precision@10': 0.0983500717360115,\n",
       " 'qa_eval_dot_score_recall@10': 0.8158177905308465,\n",
       " 'qa_eval_dot_score_ndcg@10': 0.6926942973725922,\n",
       " 'qa_eval_dot_score_mrr@10': 0.6950025335337378,\n",
       " 'qa_eval_dot_score_map@10': 0.6315333809675632}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Custom model evaluation\n",
    "results = dev_evaluator(custom_domain_model)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Mean Average Precision (MAP) at $k=10$ for both the base and custom domain models using the evaluator, and print the results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model:  {'qa_test_dot_score_accuracy@10': 0.7763819095477387, 'qa_test_dot_score_precision@10': 0.08248384781048097, 'qa_test_dot_score_recall@10': 0.7017109356305337, 'qa_test_dot_score_ndcg@10': 0.5896317890507138, 'qa_test_dot_score_mrr@10': 0.5893142013924492, 'qa_test_dot_score_map@10': 0.5357515444759702}\n",
      "\n",
      "-----------\n",
      "\n",
      "Custom model:  {'qa_test_dot_score_accuracy@10': 0.8872936109117013, 'qa_test_dot_score_precision@10': 0.09727207465900933, 'qa_test_dot_score_recall@10': 0.811180904522613, 'qa_test_dot_score_ndcg@10': 0.6866557656041946, 'qa_test_dot_score_mrr@10': 0.6884060324297208, 'qa_test_dot_score_map@10': 0.6261181450525491}\n"
     ]
    }
   ],
   "source": [
    "eva_base_model = test_evaluator(base_model, output_path='results/base_model/')\n",
    "print(\"Base model: \", eva_base_model)\n",
    "print(\"\\n-----------\\n\")\n",
    "eva_custom_model = test_evaluator(custom_domain_model, output_path='results/custom_model/')\n",
    "print(\"Custom model: \", eva_custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Base Model ------\n",
      "q1 0.8325 (answer1) -- 0.56 (answer2)\n",
      "q2 0.581 (answer1) -- 0.9155 (answer2)\n",
      "------ Custom model -----\n",
      "q1 0.7383 (answer1) -- 0.01709 (answer2)\n",
      "q2 0.1287 (answer1) -- 0.8804 (answer2)\n"
     ]
    }
   ],
   "source": [
    "# Asumiendo que los embeddings están normalizados\n",
    "question1 = \"In the context of selling Direct Long-Term Insurance to Retail Clients, can you identify the rule that mandates insurers and insurance intermediaries to ensure that the insurance products are suitable for their clients?\"\n",
    "answer1 =  \"An Insurer or an Insurance Intermediary must comply with the suitability requirement set out in Rule ‎3.4 when conducting any Insurance or Insurance Intermediation Business with or for a Retail Client in respect of Direct Long-Term Insurance.\"\n",
    "\n",
    "question2 = 'Under what circumstances, as outlined in Rule ‎12.3.2, is a Fund Manager of a Domestic Fund not mandated to engage the services of an Eligible Custodian?'\n",
    "answer2 =  'A Fund Manager of a Domestic Fund is not required to appoint an Eligible Custodian for the Fund pursuant to Rule ‎12.3.2 where it meets the requirements in either (2) and (3), or (4).'\n",
    "\n",
    "print(\"------ Base Model ------\")\n",
    "\n",
    "emb_q1 = base_model.encode(question1)  # the embedding is normalized\n",
    "emb_q2 = base_model.encode(question2)  # the embedding is normalized\n",
    "ans_1 = base_model.encode(answer1)\n",
    "ans_2 = base_model.encode(answer2)\n",
    "\n",
    "\n",
    "print(\"q1\", ans_1 @ emb_q1,\"(answer1) --\", ans_2 @ emb_q1, \"(answer2)\")\n",
    "print(\"q2\", ans_1 @ emb_q2, \"(answer1) --\", ans_2 @ emb_q2, \"(answer2)\")\n",
    "\n",
    "print(\"------ Custom model -----\")\n",
    "\n",
    "emb_q1 = custom_domain_model.encode(question1)  # the embedding is normalized\n",
    "emb_q2 = custom_domain_model.encode(question2)  # the embedding is normalized\n",
    "ans_1 = custom_domain_model.encode(answer1)\n",
    "ans_2 = custom_domain_model.encode(answer2)\n",
    "\n",
    "\n",
    "print(\"q1\", ans_1 @ emb_q1,\"(answer1) --\", ans_2 @ emb_q1, \"(answer2)\")\n",
    "print(\"q2\", ans_1 @ emb_q2, \"(answer1) --\", ans_2 @ emb_q2, \"(answer2)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
