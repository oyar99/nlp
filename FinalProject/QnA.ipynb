{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulatory Information Retrieval and Answer Generation (RIRAG)\n",
    "\n",
    "This notebook solves the following task of the Regulatory Information Retrieval and Answer Generation competition.\n",
    "\n",
    "_Using the question and the passages retrieved in Subtask 1 (See ObliQA.ipynb notebook), participants must generate a comprehensive, accurate, and coherent answer. This subtask emphasizes the ability to synthesize information from multiple sources and present it in a clear and logical manner, ensuring that the answer fully addresses the compliance and obligation requirements of the query._\n",
    "\n",
    "The notebook demonstrates how we can leverage _Retrieval Augmented Generation_ and _Large Language Models_ to synthesize the results obtained through the hybrid (lexical and semantic) search to provide an accurate and precise answer to help professionals navigate the regulatory content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy RePASs repo to validate our results - ONLY RUN ONCE\n",
    "#!git clone https://github.com/RegNLP/RePASs.git && cd RePASs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load passages from disk\n",
    "ndocs = 40  # Number of regulatory documents to process\n",
    "passages = defaultdict(str) # List to store all passages extracted from the regulatory documents\n",
    "\n",
    "# Extract the passages in each document\n",
    "for i in range(1, ndocs + 1):\n",
    "    with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
    "        doc = json.load(f)  # Loads the contents of the JSON file\n",
    "        for psg in doc:  # Map each passageId to the actual content\n",
    "            passages[psg[\"ID\"]] = psg[\"Passage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_dict = defaultdict(list) # Maps a question to the relevant passage and its corresponding ranking score\n",
    "\n",
    "# Load the rankings file in memory\n",
    "with open('data/rankings_hybrid.trec', 'r') as f:\n",
    "    # File format: QuestionID Q0 DocumentID Rank Score Method\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        question_id = parts[0]\n",
    "        document_id = parts[2]\n",
    "        rank = int(parts[3])\n",
    "        score = float(parts[4])\n",
    "        rankings_dict[question_id].append({\n",
    "            'doc': document_id,\n",
    "            'score': score\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_passages(question_id: str, rankings_dict: dict = rankings_dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts the passages content that are relevant for answering the given question.\n",
    "    Given a valid question id, it returns at least one passage and up to 10 passages\n",
    "    that surpass a given relevance threshold.\n",
    "    \n",
    "    Args:\n",
    "        question_id: The question id for which we want to extract the relevant passages\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of passages that are relevant for answering the given question\n",
    "    \"\"\"\n",
    "    retrieved_passages = []\n",
    "    should_stop = False\n",
    "    \n",
    "    for i in range(len(rankings_dict[question_id])):\n",
    "        # If there was a significant difference in relevance between two passages, don't extract more passages\n",
    "        # If 10 passages have already been extracted, don't extract more\n",
    "        if should_stop or len(retrieved_passages) == 10:\n",
    "            break\n",
    "            \n",
    "        # If no passage has been extracted, extract at least one\n",
    "        if len(retrieved_passages) == 0:\n",
    "            retrieved_passages.append(rankings_dict[question_id][i][\"doc\"])\n",
    "            continue\n",
    "                \n",
    "        # Check if there is a relevance difference between this and the next passage of more than 10%\n",
    "        if i < len(rankings_dict[question_id]) - 1 and rankings_dict[question_id][i][\"score\"] - rankings_dict[question_id][i+1][\"score\"] > 0.1:\n",
    "                should_stop = True\n",
    "\n",
    "        # Don't include passages with low relevance\n",
    "        if rankings_dict[question_id][i][\"score\"] < 0.72:\n",
    "            break\n",
    "\n",
    "        retrieved_passages.append(rankings_dict[question_id][i][\"doc\"])\n",
    "        \n",
    "    # Extract the plain text\n",
    "    retrieved_passages = [passages[doc] for doc in retrieved_passages]\n",
    "    \n",
    "    return retrieved_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You are a regulatory compliance assistant. Provide a **complete**, **coherent**, and**correct** response to the given question by synthesizing the information from the provided passages. Your answer should **fully integrate all relevant obligations, practices, and insights**, and directlyaddress the question. The passages are presented in order of relevance, so **prioritize the informationaccordingly** and ensure consistency in your response, avoiding any contradictions. Additionally, reference**specific regulations and key compliance requirements** outlined in the regulatory content to support youranswer. **Do not use any extraneous or external knowledge** outside of the provided passages when craftingyour response.',\n",
       " 'Question: question\\n\\nPassage: passage\\n\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_prompt(question: str, relevant_passages: list[str], system_prompt: str = None) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Builds the prompt that will be used to synthesize the passages\n",
    "    \n",
    "    Args:\n",
    "        question: A well formed regulatory question\n",
    "        relevant_passages: A list of relevant passages that should help answer the question\n",
    "        system_prompt: Optional custom system prompt. If None, uses default regulatory compliance prompt\n",
    "    \n",
    "    Returns:\n",
    "        A tuple with both the system prompt that contains instructions on how to answer and\n",
    "        the user prompt that contains the actual question and passages\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default system prompt if none is provided\n",
    "    default_system_prompt = (\"You are a regulatory compliance assistant. Provide a **complete**, **coherent**, and\"\n",
    "    \"**correct** response to the given question by synthesizing the information from the provided passages. \"\n",
    "    \"Your answer should **fully integrate all relevant obligations, practices, and insights**, and directly\"\n",
    "    \"address the question. The passages are presented in order of relevance, so **prioritize the information\"\n",
    "    \"accordingly** and ensure consistency in your response, avoiding any contradictions. Additionally, reference\"\n",
    "    \"**specific regulations and key compliance requirements** outlined in the regulatory content to support your\"\n",
    "    \"answer. **Do not use any extraneous or external knowledge** outside of the provided passages when crafting\"\n",
    "    \"your response.\")\n",
    "    \n",
    "    \n",
    "    # Use provided system prompt or fall back to default\n",
    "    system_prompt = system_prompt if system_prompt is not None else default_system_prompt\n",
    "\n",
    "    user_prompt = f\"Question: {question}\\n\\n\"\n",
    "    for passage in relevant_passages:\n",
    "        user_prompt += f\"Passage: {passage}\\n\\n\"\n",
    "        \n",
    "    return (system_prompt, user_prompt)\n",
    "\n",
    "build_prompt(\"question\", [\"passage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI - Standard deployment\n",
    "\n",
    "First, we use Azure OpenAI standard deployment to synthesize the retrieved passages for each question using `gpt 3.5 turbo`.\n",
    "Since the API has enabled rate limit for both token and requests per minute, we use a decorator to throttle the function in the client to prevent sending requests that will be blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import azure open AI library to use access API-based LLMs. \n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Import data structures used for throttling implementation for standard deployments in Azure.\n",
    "from collections import (\n",
    "    defaultdict,\n",
    "    deque\n",
    ")\n",
    "from threading import Lock\n",
    "import asyncio\n",
    "\n",
    "# Import util libraries\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables to handle access to the openAI API using secrets\n",
    "# Variables to be defined:\n",
    "# QNA_ENDPOINT_URL: Deployment endpoint for inference/chat completion\n",
    "# QNA_OPENAI_API_KEY: Key to access openAI API\n",
    "#\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The environment variable QNA_ENDPOINT_URL is not defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m llm_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-35-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endpoint:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment variable QNA_ENDPOINT_URL is not defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openAIKey:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment variable QNA_OPENAI_API_KEY is not defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The environment variable QNA_ENDPOINT_URL is not defined."
     ]
    }
   ],
   "source": [
    "endpoint = os.getenv('QNA_ENDPOINT_URL')\n",
    "openAIKey = os.getenv('QNA_OPENAI_API_KEY')\n",
    "llm_model = 'gpt-35-turbo'\n",
    "\n",
    "if not endpoint:\n",
    "    raise ValueError(\"The environment variable QNA_ENDPOINT_URL is not defined.\")\n",
    "\n",
    "if not openAIKey:\n",
    "    raise ValueError(\"The environment variable QNA_OPENAI_API_KEY is not defined.\")\n",
    "\n",
    "openAI_client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=openAIKey,\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "# This class limits the number of times a function can be called in a given time interval. It guarantees that\n",
    "# the functions are called all the time, but not in the same order as they are called.\n",
    "# Limits the number of times we can call a function in a given time interval. \n",
    "# Guarantees that a function call will eventually happen, but it does not necessarily respect the order in which \n",
    "# the function was called\n",
    "class Throttle:\n",
    "    def __init__(self, rate_limit, time_window):\n",
    "        self.rate_limit = rate_limit # Max number of calls allowed in the given interval\n",
    "        self.time_window = time_window # The time interval\n",
    "        self.calls = deque() # Stores the function calls so we can track when to remove calls as they become stale\n",
    "        self.lock = Lock() # Locks to prevent concurrent access to the function\n",
    "        self.queue = asyncio.Queue() # Stores function calls that need to be awaited before being executed\n",
    "\n",
    "    def __call__(self, func):\n",
    "        async def wrapped_func(*args, **kwargs):\n",
    "            # Reference the global func\n",
    "            nonlocal func\n",
    "            # Lock concurrent access\n",
    "            with self.lock:\n",
    "                current_time = time.time()\n",
    "                \n",
    "                # Remove function calls that are outside of the time window\n",
    "                while self.calls and self.calls[0] < current_time - self.time_window:\n",
    "                    self.calls.popleft()\n",
    "\n",
    "                # If we can make a call without exceeding the rate limit, then we just call it   \n",
    "                if len(self.calls) < self.rate_limit:\n",
    "                    self.calls.append(current_time)\n",
    "                    return await func(*args, **kwargs)\n",
    "                else:\n",
    "                    # Otherwise, queue the function call for later\n",
    "                    await self.queue.put((func, args, kwargs))\n",
    "                    \n",
    "                    # Process function calls in the queue\n",
    "                    while not self.queue.empty():\n",
    "                        # Dequeue the function call\n",
    "                        func, args, kwargs = await self.queue.get()\n",
    "                        current_time = time.time()\n",
    "\n",
    "                        # Remove function calls that are outside of the time window\n",
    "                        while self.calls and self.calls[0] < current_time - self.time_window:\n",
    "                            self.calls.popleft()\n",
    "\n",
    "                        # If we can make a call without exceeding the rate limit, then we just call it\n",
    "                        if len(self.calls) < self.rate_limit:\n",
    "                            self.calls.append(current_time)\n",
    "                            result = await func(*args, **kwargs)\n",
    "                            self.queue.task_done()\n",
    "                            return result\n",
    "                        else:\n",
    "                            # Otherwise, wait a few seconds before retrying \n",
    "                            await self.queue.put((func, args, kwargs)) # Enqueue the function call again\n",
    "                            await asyncio.sleep(min(10, self.time_window))\n",
    "                    \n",
    "        return wrapped_func\n",
    "\n",
    "# Allow maximum 60 calls every 70 seconds\n",
    "@Throttle(rate_limit=60, time_window=70)\n",
    "async def summarize_answer(question: str, relevant_passages: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes the answer based on the provided passages\n",
    "    \n",
    "    Args:\n",
    "        question: A well formed regulatory question\n",
    "        relevant_passages: A list of relevant passages that should help answer the question\n",
    "    \"\"\"\n",
    "\n",
    "    (system_prompt, user_prompt) = build_prompt(question, relevant_passages)\n",
    "\n",
    "    # Executes the LLM API call\n",
    "    completion = openAI_client.chat.completions.create(\n",
    "        model=llm_model, # we are using gpt-3.5-turbo\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.25, # Controls how deterministic the response is. Higher values result in more creativity. We want more easy-to-reproduce results\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2786/2786 [1:56:08<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "# load the test dataset\n",
    "with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
    "    data = json.load(f)  # Load the JSON file\n",
    "    \n",
    "    # For each question:\n",
    "    for e in tqdm(data):  # tqdm adds a progress bar\n",
    "        query = e['Question']  # Extract the actual question\n",
    "        question_id = e[\"QuestionID\"] # Extract the question id\n",
    "        \n",
    "        retrieved_passages = extract_passages(question_id)\n",
    "\n",
    "        answer = await summarize_answer(query, retrieved_passages)\n",
    "\n",
    "        answers.append({\n",
    "            \"QuestionID\": question_id,\n",
    "            \"RetrievedPassages\": retrieved_passages,\n",
    "            \"Answer\": answer\n",
    "        })\n",
    "\n",
    "# Store the results in a json File\n",
    "with open(\"data/answers.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI - Batch deployment\n",
    "\n",
    "Second, we use Azure OpenAI batch deployment to synthesize the retrieved passages for each question using `gpt-4o-mini`.\n",
    "We leverage the batch API to send all queries at once. The response is retrieved offline from the Azure open AI portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queue_batch_summarization_job(jobs):\n",
    "    \"\"\"\n",
    "    Create a batch Job in Azure open AI to generate the answers for all questions with a single request\n",
    "    \"\"\"\n",
    "    endpoint = os.getenv('QNA_ENDPOINT_URL')\n",
    "    openAIKey = os.getenv('QNA_OPENAI_API_KEY')\n",
    "\n",
    "    if not endpoint:\n",
    "        raise ValueError(\"No se ha definido la variable de entorno QNA_ENDPOINT_URL\")\n",
    "\n",
    "    if not openAIKey:\n",
    "        raise ValueError(\"No se ha definido la variable de entorno QNA_OPENAI_API_KEY\")\n",
    "\n",
    "    openAI_client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=openAIKey,\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "\n",
    "    # File that can either be uploaded manually or programatically\n",
    "    file_name = \"data/batch_questions.jsonl\"\n",
    "\n",
    "    # Save file contents using json lines format\n",
    "    with open(file_name, 'w') as file:\n",
    "        for job in jobs:\n",
    "            file.write(json.dumps(job) + '\\n')\n",
    "\n",
    "    # Upload the file programatically\n",
    "    batch_file = openAI_client.files.create(\n",
    "      file=open(file_name, \"rb\"),\n",
    "      purpose=\"batch\"\n",
    "    )\n",
    "    \n",
    "    # Wait until the file upload is done\n",
    "    while True:\n",
    "        file = openAI_client.files.retrieve(batch_file.id)\n",
    "        if file.status == \"processed\" or file.status == \"error\":\n",
    "            break\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Trigger the batch job using the uploaded file\n",
    "    # Result should terminate in less than 24 hours\n",
    "    batch_job = openAI_client.batches.create(\n",
    "      input_file_id=batch_file.id,\n",
    "      endpoint=\"/v1/chat/completions\",\n",
    "      completion_window=\"24h\"\n",
    "    )\n",
    "    \n",
    "    return batch_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_one_shot = (\"As a regulatory compliance assistant. Provide a **complete**, **coherent**, and \"\n",
    "\"**correct** response to the given question by synthesizing the information from the provided passages. \"\n",
    "\"Your answer should **fully integrate all relevant obligations, practices, and insights**, and directly \"\n",
    "\"address the question. The passages are presented in order of relevance, so **prioritize the information \"\n",
    "\"accordingly** and ensure consistency in your response, avoiding any contradictions. Additionally, reference \"\n",
    "\"**specific regulations and key compliance requirements** outlined in the regulatory content to support your \"\n",
    "\"answer. **Do not use any extraneous or external knowledge** outside of the provided passages when crafting \"\n",
    "\"your response.\"\n",
    "\"/n/nHere are a few examples.\"\n",
    "\"/n/nQuestion: What specific areas of inventory and delivery infrastructure should be covered in the independent third-party audits to satisfy the requirements of COBS Rule 22.4.2(d)?\"\n",
    "\"/n/nPassage: REGULATORY REQUIREMENTS - SPOT COMMODITY ACTIVITIES\\nDelivery & Storage\\nWhen applying COBS Rule 22.4.2(d), an Authorised Person should have independent third party audits carried out at appropriate times, for the inventories and deliveries undertaken at the storage facility, as well as the facilities infrastructure itself.  Where necessary, further third-party audits will be required for the obligations of Accepted Spot Commodities, as outlined in paragraph 26 above.\\n\"\n",
    "\"/n/nPassage: REGULATORY REQUIREMENTS - SPOT COMMODITY ACTIVITIES\\nDelivery & Storage\\nPursuant to COBS Rule 22.4.1, a delivery and/or storage facility used by an Authorised Person can be operated from within ADGM or outside ADGM.  Specifically, for the purposes of COBS Rules 22.4.1, an Authorised Person will need to submit to the FSRA the details of how each delivery and storage facility that it proposes to use, whether located inside or outside ADGM, meets the requirements set out in Rule 22.4.2(a) to (e).\\n\"\n",
    "\"/n/nYour response should read:\"\n",
    "\"/n/nTo satisfy the requirements of COBS Rule 22.4.2(d) for independent third-party audits, an Authorised Person should ensure that the audits cover the inventories and deliveries undertaken at the storage facility, as well as the infrastructure of the facility itself. Additionally, if the Authorised Person deals with Accepted Spot Commodities, further third-party audits will be necessary to fulfill their obligations as outlined in paragraph 26. As per COBS Rule 22.4.1, the delivery and storage facility used by the Authorised Person can be located within or outside ADGM, and the Authorised Person must submit details to the FSRA on how each facility meets the requirements set out in Rule 22.4.2(a) to (e). Therefore, the independent third-party audits should cover the areas of inventory, delivery, and infrastructure of the storage facility, as well as any obligations related to Accepted Spot Commodities and compliance with the requirements set out in COBS Rule 22.4.2(a) to (e)\"\n",
    "\"/n/nQuestion: What percentage of the Insurer's Net Written Premium is used to determine the non-proportional reinsurance element?\"\n",
    "\"/n/nPassage: The non proportional reinsurance element is calculated as 52% of the Insurer's Net Written Premium\"\n",
    "\"/n/nYour response should read:\"\n",
    "\"/n/nThe non-proportional reinsurance element is determined by calculating 52% of the Insurer's Net Written Premium.\"\n",
    "\"/n/nQuestion: Who is responsible for ensuring compliance with the obligations that apply to the Reporting Entity of a Fund under the provisions of this chapter, unless explicitly stated otherwise?\"\n",
    "\"/n/nPassage: Where an obligation applies to a Reporting Entity of a Fund under a provision of this chapter, except where expressly provided otherwise, the Governing Body of the Listed Fund must ensure compliance with that obligation.\"\n",
    "\"/n/nYour response should read:\"\n",
    "\"/n/nThe responsibility for ensuring compliance with the obligations that apply to the Reporting Entity of a Fund under the provisions of this chapter lies with the Governing Body of the Listed Fund. This is explicitly stated in the passage, which indicates that unless otherwise specified, it is the Governing Body that must ensure adherence to these obligations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2786/2786 [00:00<00:00, 92037.29it/s]\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "\n",
    "# Load the test dataset\n",
    "with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
    "    data = json.load(f)  # Load the JSON file\n",
    "    \n",
    "    # For each question:\n",
    "    for e in tqdm(data):  # tqdm adds a progress bar\n",
    "        query = e['Question']  # Extract the actual question\n",
    "        question_id = e[\"QuestionID\"] # Extract the question id\n",
    "\n",
    "        retrieved_passages = extract_passages(question_id)\n",
    "\n",
    "        (system_prompt, user_prompt) = build_prompt(query, retrieved_passages, system_prompt_one_shot)\n",
    "        \n",
    "        jobs.append({\n",
    "            \"custom_id\": question_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                \"temperature\": 0,\n",
    "                \"frequency_penalty\": 0.0,\n",
    "                \"presence_penalty\": 0.0,\n",
    "                \"max_tokens\": 1000,\n",
    "            }\n",
    "        })\n",
    "        \n",
    "batch_job = queue_batch_summarization_job(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point the result has been downloaded offline and uploaded to the data folder\n",
    "answers = []\n",
    "\n",
    "with open(\"data/batch_result_4o.jsonl\") as f:\n",
    "    # Parse each line of the file as a JSON\n",
    "    results = [json.loads(line) for line in f]\n",
    "    \n",
    "    for result in results:\n",
    "        # For each result, create an entry in answers array to later create the output file\n",
    "        question_id = result[\"custom_id\"]\n",
    "        # Since this function is deterministic, we can just call it again\n",
    "        # Clearly, there's an optimization we can do to avoid calling this twice\n",
    "        retrieved_passages = extract_passages(question_id) \n",
    "        answer = result[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        answers.append({\n",
    "            \"QuestionID\": question_id,\n",
    "            \"RetrievedPassages\": retrieved_passages,\n",
    "            \"Answer\": answer\n",
    "        })\n",
    "        \n",
    "# Save the results as a JSON file\n",
    "with open(\"data/answers-4o-new.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq API - Regular deployment\n",
    "\n",
    "Third, we use Groq's API to synthesize the retrieved passages for each question using `llama-3.1-70b-versatile`. We leverage Groq's high-performance infrastructure to process queries with minimal latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Third-party imports\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "\n",
    "# Configure minimal logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "@dataclass\n",
    "class Question:\n",
    "    \"\"\"Represents a question with its ID and content\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class ProcessedAnswer:\n",
    "    \"\"\"Represents a processed answer with metadata\"\"\"\n",
    "    question_id: str\n",
    "    retrieved_passages: List[str]\n",
    "    answer: str\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class GroqRateLimiter:\n",
    "    def __init__(self):\n",
    "        # Rate limits\n",
    "        self.tokens_per_minute = 30000\n",
    "        self.requests_per_minute = 1000\n",
    "        self.requests_per_day = 50000\n",
    "        \n",
    "        # Window durations\n",
    "        self.minute_window = timedelta(minutes=1)\n",
    "        self.day_window = timedelta(days=1)\n",
    "        \n",
    "        # Track requests with timestamps and token counts\n",
    "        self.requests = []  # List of (timestamp, tokens) tuples\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def wait_for_tokens(self, tokens_needed: int):\n",
    "        async with self.lock:\n",
    "            while True:\n",
    "                now = datetime.now()\n",
    "                minute_start = now - self.minute_window\n",
    "                day_start = now - self.day_window\n",
    "                \n",
    "                # Remove expired requests\n",
    "                self.requests = [(ts, tokens) for ts, tokens in self.requests \n",
    "                               if ts > day_start]\n",
    "                \n",
    "                # Calculate current usage\n",
    "                minute_requests = [(ts, tokens) for ts, tokens in self.requests \n",
    "                                 if ts > minute_start]\n",
    "                \n",
    "                minute_request_count = len(minute_requests)\n",
    "                day_request_count = len(self.requests)\n",
    "                minute_token_usage = sum(tokens for _, tokens in minute_requests)\n",
    "                \n",
    "                # Check all limits\n",
    "                if (minute_token_usage + tokens_needed <= self.tokens_per_minute and\n",
    "                    minute_request_count < self.requests_per_minute and\n",
    "                    day_request_count < self.requests_per_day):\n",
    "                    # Add new request\n",
    "                    self.requests.append((now, tokens_needed))\n",
    "                    return True\n",
    "                \n",
    "                # Calculate wait time based on the most restrictive limit\n",
    "                wait_times = []\n",
    "                \n",
    "                # Token limit check\n",
    "                if minute_token_usage + tokens_needed > self.tokens_per_minute and minute_requests:\n",
    "                    oldest_in_minute = min(ts for ts, _ in minute_requests)\n",
    "                    wait_times.append((oldest_in_minute + self.minute_window - now).total_seconds())\n",
    "                \n",
    "                # Requests per minute check\n",
    "                if minute_request_count >= self.requests_per_minute and minute_requests:\n",
    "                    oldest_in_minute = min(ts for ts, _ in minute_requests)\n",
    "                    wait_times.append((oldest_in_minute + self.minute_window - now).total_seconds())\n",
    "                \n",
    "                # Requests per day check\n",
    "                if day_request_count >= self.requests_per_day and self.requests:\n",
    "                    oldest_in_day = min(ts for ts, _ in self.requests)\n",
    "                    wait_times.append((oldest_in_day + self.day_window - now).total_seconds())\n",
    "                \n",
    "                # Wait for the shortest required time\n",
    "                if wait_times:\n",
    "                    wait_time = max(0.1, min(wait_times))  # Ensure minimum wait of 0.1s\n",
    "                    print(f\"Rate limit reached. Waiting {wait_time:.2f} seconds...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                else:\n",
    "                    # If no wait times calculated but still hitting limits, wait a small amount\n",
    "                    await asyncio.sleep(0.1)\n",
    "\n",
    "class GroqProcessor:\n",
    "    \"\"\"Handles processing of questions using Groq's API with rate limiting\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.groq_client = self._initialize_groq()\n",
    "        self.rate_limiter = GroqRateLimiter()\n",
    "        \n",
    "    def _initialize_groq(self) -> Groq:\n",
    "        \"\"\"Initialize Groq client with error handling\"\"\"\n",
    "        groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "        if not groq_api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY environment variable is not defined\")\n",
    "        return Groq(api_key=groq_api_key)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_passages(question_id: str, passages: dict, rankings_dict: dict) -> list[str]:\n",
    "        \"\"\"Extract relevant passages for a question\"\"\"\n",
    "        retrieved_passages = []\n",
    "        should_stop = False\n",
    "        \n",
    "        for i in range(len(rankings_dict[question_id])):\n",
    "            if should_stop or len(retrieved_passages) == 10:\n",
    "                break\n",
    "                \n",
    "            if len(retrieved_passages) == 0:\n",
    "                retrieved_passages.append(rankings_dict[question_id][i][\"doc\"])\n",
    "                continue\n",
    "                    \n",
    "            if i < len(rankings_dict[question_id]) - 1 and rankings_dict[question_id][i][\"score\"] - rankings_dict[question_id][i+1][\"score\"] > 0.1:\n",
    "                    should_stop = True\n",
    "\n",
    "            if rankings_dict[question_id][i][\"score\"] < 0.72:\n",
    "                break\n",
    "\n",
    "            retrieved_passages.append(rankings_dict[question_id][i][\"doc\"])\n",
    "            \n",
    "        return [passages[doc] for doc in retrieved_passages]\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        Exception,\n",
    "        max_tries=5,\n",
    "        giveup=lambda e: \"rate limit\" not in str(e).lower()\n",
    "    )\n",
    "    async def process_question(self, question: str, passages: List[str]) -> str:\n",
    "        \"\"\"Process a single question with rate limiting\"\"\"\n",
    "        system_prompt = \"\"\"As a regulatory compliance assistant, analyze the provided passages\n",
    "        and answer the accompanying question. Synthesize information from all passages, which\n",
    "        are presented in order of relevance.\n",
    "\n",
    "        Key Requirements:\n",
    "        1. Extract and prioritize mandatory requirements (\"shall\" statements)\n",
    "        2. Distinguish between required vs recommended practices\n",
    "        3. Identify specific deadlines and documentation needs\n",
    "        4. Use ONLY information from provided passages - no external knowledge\n",
    "        5. Integrate all obligations and requirements to ensure consistency\n",
    "\n",
    "        Structure your response as follows:\n",
    "        1. Brief executive summary (2-3 sentences)\n",
    "        2. Core requirements with citations [Section XX]\n",
    "        3. Implementation steps and timeline\n",
    "        4. Documentation requirements\n",
    "           - Required reports\n",
    "           - Retention periods\n",
    "        5. Special considerations\n",
    "           - Exemptions\n",
    "           - Jurisdictional variations\n",
    "           - Implementation challenges\n",
    "           - Potential contradictions or conflicts\n",
    "\n",
    "        Guidelines:\n",
    "        - Keep responses under 500 words unless complexity requires more\n",
    "        - Use bullet points for clarity\n",
    "        - Highlight critical deadlines\n",
    "        - Flag any ambiguities or conflicts between requirements\n",
    "        - Process passages in order of presentation\n",
    "        - Ensure full integration of all relevant requirements\n",
    "        - Maintain consistency across all recommendations\n",
    "        \"\"\"\n",
    "\n",
    "        user_prompt = f\"Question: {question}\\n\\nPassages:\\n\\n\" + \"\\n\\n\".join(passages)\n",
    "        \n",
    "        # Estimate tokens (rough approximation)\n",
    "        estimated_tokens = len(system_prompt.split()) + len(user_prompt.split()) + 800\n",
    "        \n",
    "        # Wait for available tokens\n",
    "        await self.rate_limiter.wait_for_tokens(estimated_tokens)\n",
    "        \n",
    "        try:\n",
    "            completion = await asyncio.to_thread(\n",
    "                self.groq_client.chat.completions.create,\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.25,\n",
    "                max_tokens=800,\n",
    "                top_p=1,\n",
    "                stream=False\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                retry_after = self._extract_retry_after(str(e))\n",
    "                print(f\"Rate limit hit, waiting {retry_after} seconds...\")\n",
    "                await asyncio.sleep(retry_after + 1)\n",
    "                return await self.process_question(question, passages)\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_retry_after(error_message: str) -> float:\n",
    "        \"\"\"Extract retry-after time from error message\"\"\"\n",
    "        try:\n",
    "            if \"try again in\" in error_message:\n",
    "                time_str = error_message.split(\"try again in\")[1].split(\"s\")[0].strip()\n",
    "                if \"m\" in time_str:\n",
    "                    minutes, seconds = time_str.split(\"m\")\n",
    "                    return float(minutes) * 60 + float(seconds)\n",
    "                return float(time_str)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return 60  # Default to 60 seconds if we can't parse the time\n",
    "\n",
    "async def main():\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = GroqProcessor()\n",
    "    answers = []\n",
    "    \n",
    "    try:\n",
    "        # Load necessary data\n",
    "        print(\"Loading passages...\")\n",
    "        passages = {}\n",
    "        for i in range(1, 41):\n",
    "            with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
    "                doc = json.load(f)\n",
    "                for psg in doc:\n",
    "                    passages[psg[\"ID\"]] = psg[\"Passage\"]\n",
    "\n",
    "        print(\"Loading rankings...\")\n",
    "        rankings_dict = defaultdict(list)\n",
    "        with open('data/rankings_hybrid.trec', 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                rankings_dict[parts[0]].append({\n",
    "                    'doc': parts[2],\n",
    "                    'score': float(parts[4])\n",
    "                })\n",
    "\n",
    "        # Process questions\n",
    "        print(\"Processing questions...\")\n",
    "        with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
    "            questions = json.load(f)\n",
    "            \n",
    "            for q in tqdm_asyncio(questions):\n",
    "                try:\n",
    "                    retrieved_passages = processor.extract_passages(\n",
    "                        q[\"QuestionID\"], \n",
    "                        passages, \n",
    "                        rankings_dict\n",
    "                    )\n",
    "                    \n",
    "                    answer = await processor.process_question(\n",
    "                        q[\"Question\"], \n",
    "                        retrieved_passages\n",
    "                    )\n",
    "                    \n",
    "                    answers.append({\n",
    "                        \"QuestionID\": q[\"QuestionID\"],\n",
    "                        \"RetrievedPassages\": retrieved_passages,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "                    \n",
    "                    # Save progress every 10 questions\n",
    "                    if len(answers) % 10 == 0:\n",
    "                        with open(\"data/answers-llama3.1.json\", \"w\") as f:\n",
    "                            json.dump(answers, f, indent=2)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing question {q['QuestionID']}: {e}\")\n",
    "                    # Save progress on error\n",
    "                    if answers:\n",
    "                        with open(\"data/answers-llama3.1-partial.json\", \"w\") as f:\n",
    "                            json.dump(answers, f, indent=2)\n",
    "\n",
    "        # Save final results\n",
    "        print(\"Saving final results...\")\n",
    "        with open(\"data/answers-llama3.1.json\", \"w\") as f:\n",
    "            json.dump(answers, f, indent=2)\n",
    "            \n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error during processing: {e}\")\n",
    "        # Save partial results if available\n",
    "        if answers:\n",
    "            with open(\"data/answers-llama3.1-partial.json\", \"w\") as f:\n",
    "                json.dump(answers, f, indent=2)\n",
    "        raise\n",
    "\n",
    "# Run the processor\n",
    "nest_asyncio.apply()\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Evaluation\n",
    "\n",
    "To evaluate and compare the results obtained from our three different processing methods:\n",
    "1. Standard Azure OpenAI deployment with GPT-3.5-Turbo\n",
    "2. Batch Azure OpenAI deployment with GPT-4O-Mini\n",
    "3. Groq deployment with Llama-3.1-70B-Versatile\n",
    "\n",
    "Run the following scripts using the RePASs virtual environment to evaluate each model's performance. Make sure you have activated the correct environment before running these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script to evaluate the results. Results are placed in /RePASs/data/hybrid or /RePASs/data/hybrid-4o\n",
    "## These scripts must be run using the virtual env in RePASs\n",
    "\n",
    "#python scripts/evaluate_model.py --input_file ./../data/answers.json --group_method_name hybrid\n",
    "#python scripts/evaluate_model.py --input_file ./../data/answers-4o.json --group_method_name hybrid-4o\n",
    "#python scripts/evaluate_model.py --input_file ./../data/answers-llama3.1.json --group_method_name hybrid-llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_dict_unseen = defaultdict(list) # Maps a question to the relevant passage and its corresponding ranking score\n",
    "\n",
    "# Load the rankings file in memory\n",
    "with open('data/rankings_hybrid_unseen_test.trec', 'r') as f:\n",
    "    # File format: QuestionID Q0 DocumentID Rank Score Method\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        question_id = parts[0]\n",
    "        document_id = parts[2]\n",
    "        rank = int(parts[3])\n",
    "        score = float(parts[4])\n",
    "        rankings_dict_unseen[question_id].append({\n",
    "            'doc': document_id,\n",
    "            'score': score\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 446/446 [00:00<00:00, 56871.05it/s]\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "\n",
    "# Load the test dataset\n",
    "with open(\"ObliQADataset/RIRAG_Unseen_Questions.json\") as f:\n",
    "    data = json.load(f)  # Load the JSON file\n",
    "    \n",
    "    # For each question:\n",
    "    for e in tqdm(data):  # tqdm adds a progress bar\n",
    "        query = e['Question']  # Extract the actual question\n",
    "        question_id = e[\"QuestionID\"] # Extract the question id\n",
    "\n",
    "        retrieved_passages = extract_passages(question_id, rankings_dict_unseen)\n",
    "\n",
    "        (system_prompt, user_prompt) = build_prompt(query, retrieved_passages, system_prompt_one_shot)\n",
    "        \n",
    "        jobs.append({\n",
    "            \"custom_id\": question_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-35-turbo-2\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                \"temperature\": 0,\n",
    "                \"frequency_penalty\": 0.0,\n",
    "                \"presence_penalty\": 0.0,\n",
    "                \"max_tokens\": 1000,\n",
    "            }\n",
    "        })\n",
    "        \n",
    "batch_job = queue_batch_summarization_job(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 446/446 [00:00<00:00, 955881.24it/s]\n"
     ]
    }
   ],
   "source": [
    "questions = {}\n",
    "\n",
    "# Load the test dataset\n",
    "with open(\"ObliQADataset/RIRAG_Unseen_Questions.json\") as f:\n",
    "    data = json.load(f)  # Load the JSON file\n",
    "    \n",
    "    # For each question:\n",
    "    for e in tqdm(data):  # tqdm adds a progress bar\n",
    "        query = e['Question']  # Extract the actual question\n",
    "        question_id = e[\"QuestionID\"] # Extract the question id\n",
    "        \n",
    "        questions[question_id] = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point the result has been downloaded offline and uploaded to the data folder\n",
    "answers = []\n",
    "\n",
    "with open(\"data/batch_result_unseen.jsonl\") as f:\n",
    "    # Parse each line of the file as a JSON\n",
    "    results = [json.loads(line) for line in f]\n",
    "    \n",
    "    for result in results:\n",
    "        # For each result, create an entry in answers array to later create the output file\n",
    "        question_id = result[\"custom_id\"]\n",
    "        # Since this function is deterministic, we can just call it again\n",
    "        # Clearly, there's an optimization we can do to avoid calling this twice\n",
    "        retrieved_passages = extract_passages(question_id, rankings_dict_unseen) \n",
    "        answer = result[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        answers.append({\n",
    "            \"QuestionID\": question_id,\n",
    "            \"Question\": questions[question_id],\n",
    "            \"RetrievedPassages\": retrieved_passages,\n",
    "            \"Answer\": answer\n",
    "        })\n",
    "        \n",
    "# Save the results as a JSON file\n",
    "with open(\"data/answers-unseen.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
