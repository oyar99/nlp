{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from collections import deque\n",
    "from threading import Lock\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy RePASs repo to validate our results - ONLY RUN ONCE\n",
    "#!git clone https://github.com/RegNLP/RePASs.git && cd RePASs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceso para cargar una colección de pasajes desde el directorio de documentos estructurados\n",
    "ndocs = 40  # Número de documentos a procesar\n",
    "passages = defaultdict(str) # Lista para almacenar la colección de pasajes\n",
    "\n",
    "# Lee cada documento y extrae los pasajes relevantes\n",
    "for i in range(1, ndocs + 1):\n",
    "    with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
    "        doc = json.load(f)  # Carga el contenido del documento JSON\n",
    "        for psg in doc:  # Recorre cada pasaje del documento\n",
    "            passages[psg[\"ID\"]] = psg[\"Passage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_dict = defaultdict(list)\n",
    "\n",
    "# Abrimos el archivo de rankings y lo cargamos en memoria\n",
    "with open('data/rankings_hybrid.trec', 'r') as f:\n",
    "    # Formato de línea: QuestionID Q0 DocumentID Rank Score Método\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        question_id = parts[0]\n",
    "        document_id = parts[2]\n",
    "        rank = int(parts[3])\n",
    "        rankings_dict[question_id].append(document_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv('QNA_ENDPOINT_URL')\n",
    "openAIKey = os.getenv('QNA_OPENAI_API_KEY')\n",
    "llm_model = 'gpt-35-turbo'\n",
    "\n",
    "if not endpoint:\n",
    "    raise ValueError(\"No se ha definido la variable de entorno QNA_ENDPOINT_URL\")\n",
    "\n",
    "if not openAIKey:\n",
    "    raise ValueError(\"No se ha definido la variable de entorno QNA_OPENAI_API_KEY\")\n",
    "\n",
    "openAI_client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=openAIKey,\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")\n",
    "\n",
    "class Throttle:\n",
    "    def __init__(self, rate_limit, time_window):\n",
    "        self.rate_limit = rate_limit # Numero de llamadas permitidas por fraccion de tiempo\n",
    "        self.time_window = time_window # Fraccion de tiempo en segundos\n",
    "        self.calls = deque() # Almacena los tiempos de las llamadas en un deque (Array que permite agregar y remover elementos de ambos extremos en O(1))\n",
    "        self.lock = Lock() # Lock para asegurar que solo un hilo accede a la lista de llamadas a la vez\n",
    "        self.queue = asyncio.Queue() # Una cola para almacenar las llamadas que se hicieron mientras se estaba esperando e intentar nuevamente\n",
    "\n",
    "    def __call__(self, func):\n",
    "        async def wrapped_func(*args, **kwargs):\n",
    "            # Referenciar la funcion fuera del context local\n",
    "            nonlocal func\n",
    "            # Bloquear el acceso a la lista de llamadas\n",
    "            with self.lock:\n",
    "                current_time = time.time()\n",
    "                \n",
    "                # Remover llamadas que ya no están en la ventana de tiempo\n",
    "                while self.calls and self.calls[0] < current_time - self.time_window:\n",
    "                    self.calls.popleft()\n",
    "\n",
    "                # Si todavia hay espacio en la ventana de tiempo, agregar la llamada actual y ejecutar la función    \n",
    "                if len(self.calls) < self.rate_limit:\n",
    "                    self.calls.append(current_time)\n",
    "                    return await func(*args, **kwargs)\n",
    "                else:\n",
    "                    # De lo contrario, agregar la llamada a la cola y esperar\n",
    "                    await self.queue.put((func, args, kwargs)) # Agregar la llamada a la cola\n",
    "                    \n",
    "                    # Procesa las llamadas en la cola\n",
    "                    while not self.queue.empty():\n",
    "                        # Sacar la llamada de la cola\n",
    "                        func, args, kwargs = await self.queue.get()\n",
    "                        async with self.lock:\n",
    "                            current_time = time.time()\n",
    "\n",
    "                            # Remover llamadas que ya no están en la ventana de tiempo\n",
    "                            while self.calls and self.calls[0] < current_time - self.time_window:\n",
    "                                self.calls.popleft()\n",
    "                            \n",
    "                            # Si ya hay espacio en la ventana de tiempo, ejecutar la función y sacar de la cola\n",
    "                            if len(self.calls) < self.rate_limit:\n",
    "                                self.calls.append(current_time)\n",
    "                                result = await func(*args, **kwargs)\n",
    "                                self.queue.task_done()\n",
    "                                return result\n",
    "                            else:\n",
    "                                # De lo contrario, esperar 10 segundos antes de intentar nuevamente\n",
    "                                await asyncio.sleep(10)\n",
    "                    \n",
    "        return wrapped_func\n",
    "\n",
    "# Permite maximo 60 llamadas cada 50 segundos. Las llamadas se van encolando y se ejecutaran a medida que se libere espacio\n",
    "@Throttle(rate_limit=60, time_window=50)\n",
    "async def summarize_answer(question: str, relevant_passages: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Utilizar un modelo LLM para crear una respuesta a la pregunta a partir de la lista\n",
    "    de pasajes relevantes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construir el prompt para el modelo LLM\n",
    "    system_prompt = \"\"\"\n",
    "You are an expert legal assistant specializing in compliance and regulatory documents, with a particular focus on the regulations and rules issued by Abu Dhabi Global Markets (ADGM). When responding to a query, your primary objective is to provide a **comprehensive**, **accurate**, and **coherent** answer that fully addresses the compliance and obligation requirements of the question. \n",
    "\n",
    "### Key Guidelines for Response:\n",
    "\n",
    "1. **Relevance**:  \n",
    "   Start with the most relevant passage (Passage 1). This should contain the majority of the information needed to answer the query. If the answer is not fully addressed in Passage 1, proceed to the next relevant passage (Passage 2), and so on, until the query is answered in full. Avoid using information that is not contained in the regulatory content.\n",
    "\n",
    "2. **Accuracy and Completeness**:  \n",
    "   Ensure that your response is fully aligned with the compliance requirements, rules, and obligations outlined in the regulatory passages. If necessary, infer logical conclusions based on the provided text, but do not make up or introduce any information that is not explicitly stated in the provided regulatory content.\n",
    "\n",
    "3. **Clarity and Coherence**:  \n",
    "   Your answer should be clear, well-organized, and free of contradictions. Present information in a structured manner, making sure each point is addressed in sequence, referencing specific rules, regulations, or obligations as relevant.\n",
    "\n",
    "5. **Regulatory References**:  \n",
    "   Whenever possible, cite the specific regulation, rule, or article number from the relevant passages that supports your answer. This will enhance the accuracy and reliability of your response.\n",
    "\n",
    "6. **No Contradictions**:  \n",
    "   Ensure that there are no contradictions in your answer. All information provided should align with the regulatory content from the relevant passages.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Question: {question}\\n\\nPassages:\\n\\n\"\n",
    "    for idx, passage in enumerate(relevant_passages, 1):\n",
    "        user_prompt += f\"{idx}. {passage}\\n\\n\"\n",
    "\n",
    "    completion = openAI_client.chat.completions.create(\n",
    "        model=llm_model, # Estamos usando el modelo gpt-3.5-turbo\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3, # Controla la aleatoriedad de las respuestas generadas - Menor valor, respuestas más deterministicas\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2786/2786 [1:37:42<00:00,  2.10s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "it = 0\n",
    "\n",
    "# Abrimos el archivo JSON que contiene las consultas de prueba (ObliQA_test.json)\n",
    "with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
    "    data = json.load(f)  # Cargamos el contenido del archivo JSON\n",
    "    \n",
    "    # Iteramos sobre cada entrada (pregunta) en el archivo de datos\n",
    "    for e in tqdm(data):  # tqdm agrega una barra de progreso durante la iteración\n",
    "        query = e['Question']  # Extraemos la pregunta o consulta desde el campo 'Question'\n",
    "        questionId = e[\"QuestionID\"] # Extraemos el ID de la pregunta\n",
    "\n",
    "        # TODO: Extract more relevant passages up to a given threshold (e.g., 0.7) and consider abrupt \n",
    "        # changes in relevance score\n",
    "        retrieved_passages = [passages[doc] for doc in rankings_dict[questionId][:3]]\n",
    "\n",
    "        answer = await summarize_answer(query, retrieved_passages)\n",
    "\n",
    "        answers.append({\n",
    "            \"QuestionID\": questionId,\n",
    "            \"RetrievedPassages\": retrieved_passages,\n",
    "            \"Answer\": answer\n",
    "        })\n",
    "\n",
    "# Guardamos las respuestas en un archivo JSON\n",
    "with open(\"data/answers.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script para evaluar los resultados que se guardaran en /RePASs/data/hybrid\n",
    "## Se deben correr activando el ambiente virtual definido en RePASs\n",
    "\n",
    "#python scripts/evaluate_model.py --input_file ./../data/answers.json --group_method_name hybrid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
