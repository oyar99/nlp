{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!git clone https://github.com/usnistgov/trec_eval.git && cd trec_eval && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/raul/Escritorio/extra/misis/rl/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "from tqdm import tqdm\n",
        "from re import compile\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#import unicodedata\n",
        "from contractions import fix as fix_contractions\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n1K8SXctD0Uw"
      },
      "outputs": [],
      "source": [
        "# Cargar un modelo preentrenado para embeddings semánticos, utilizado en consultas semánticas\n",
        "sentence_transformer_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Conjunto de stopwords en inglés y un stemmer basado en la lengua inglesa\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "# Expresiones regulares precompiladas para limpieza de texto\n",
        "pattern_newline = compile(r'[\\n\\t\\u200e]')  # Elimina saltos de línea, tabulaciones y caracteres no deseados\n",
        "pattern_non_alphanumeric = compile(r'[^a-z0-9]')  # Elimina caracteres no alfanuméricos\n",
        "pattern_multiple_spaces = compile(r' +')  # Elimina múltiples espacios consecutivos\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Limpia y normaliza el texto, expandiendo contracciones, eliminando caracteres no deseados,\n",
        "    y aplicando stemming y eliminación de stopwords.\n",
        "\n",
        "    Args:\n",
        "        text (str): El texto de entrada a ser limpiado.\n",
        "\n",
        "    Returns:\n",
        "        str: Texto limpio y normalizado.\n",
        "    \"\"\"    \n",
        "    # Expande las contracciones para evitar inconsistencias (ej. \"don't\" -> \"do not\")\n",
        "    cln_text = fix_contractions(text)\n",
        "    \n",
        "    # Convierte el texto a minúsculas\n",
        "    cln_text = cln_text.lower()\n",
        "    \n",
        "    # Normalización Unicode (comentado por el momento)\n",
        "    #cln_text = normalize('NFKD', cln_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    \n",
        "    # Elimina saltos de línea y otros caracteres no deseados\n",
        "    cln_text = pattern_newline.sub(' ', cln_text)\n",
        "    \n",
        "    # Elimina todos los caracteres no alfanuméricos (ej. puntuación)\n",
        "    cln_text = pattern_non_alphanumeric.sub(' ', cln_text)\n",
        "    \n",
        "    # Tokeniza el texto y aplica stemming a cada palabra, eliminando las stopwords\n",
        "    tokens = [stemmer.stem(word) for word in word_tokenize(cln_text) if word not in stop_words]\n",
        "    \n",
        "    # Junta las palabras en una sola cadena de texto\n",
        "    cln_text = ' '.join(tokens)\n",
        "    \n",
        "    # Elimina múltiples espacios consecutivos y recorta espacios en los extremos\n",
        "    cln_text = pattern_multiple_spaces.sub(' ', cln_text).strip()\n",
        "    \n",
        "    return cln_text\n",
        "\n",
        "def tokenizer(text:str)-> list:\n",
        "    \"\"\"\n",
        "    Tokeniza el texto en unigramas y bigramas para enriquecer la representación del texto.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Texto de entrada.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de unigramas y bigramas.\n",
        "    \"\"\"    \n",
        "    tokens = text.split()\n",
        "    \n",
        "    # Crear unigramas\n",
        "    unigrams = tokens\n",
        "    \n",
        "    # Crear bigramas (combinaciones de palabras consecutivas)\n",
        "    bigrams = [f\"{tokens[i]} {tokens[i + 1]}\" for i in range(len(tokens) - 1)]\n",
        "    \n",
        "    # Retorna la lista de unigramas y bigramas juntos\n",
        "    return unigrams + bigrams\n",
        "\n",
        "def sintactic_query_bm5(query: str, bm5_instance: BM25Okapi) -> np.array:\n",
        "    \"\"\"\n",
        "    Realiza una consulta sintáctica utilizando el algoritmo BM25 para recuperar documentos relevantes.\n",
        "\n",
        "    Args:\n",
        "        query (str): Consulta de entrada.\n",
        "        bm5_instance (BM25Okapi): Instancia de BM25 previamente inicializada con el corpus.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Array de puntajes de BM25 para cada documento en el corpus.\n",
        "    \"\"\"    \n",
        "    \n",
        "    # Limpia y tokeniza la consulta usando el tokenizer definido (unigramas y bigramas)\n",
        "    tokenized_query = tokenizer(clean_text(query))\n",
        "    \n",
        "    # Obtiene los puntajes sintácticos usando el modelo BM25\n",
        "    scores = bm5_instance.get_scores(tokenized_query)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "def sintactic_query_tfidf(query: str, corpus_tfidf_matrix: np.array, \n",
        "                          tfidf_vectorizer: TfidfVectorizer) -> np.array:\n",
        "    \"\"\"\n",
        "    Realiza una consulta sintáctica usando el modelo TF-IDF y calcula la similitud del coseno\n",
        "    entre la consulta y el corpus.\n",
        "\n",
        "    Args:\n",
        "        query (str): Consulta de entrada.\n",
        "        corpus_tfidf_matrix (np.array): Matriz TF-IDF del corpus.\n",
        "        tfidf_vectorizer (TfidfVectorizer): Vectorizador TF-IDF previamente entrenado.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Array de puntajes de similitud del coseno para cada documento.\n",
        "    \"\"\"    \n",
        "    \n",
        "    # Limpia la consulta (sin stemming ni stopwords, porque el modelo TF-IDF se entrenó con el texto original)\n",
        "    cln_query = pattern_newline.sub(' ', query)\n",
        "    cln_query = pattern_multiple_spaces.sub(' ', cln_query).strip()\n",
        "    \n",
        "    # Transforma la consulta en un vector TF-IDF\n",
        "    query_tfidf = tfidf_vectorizer.transform([cln_query])\n",
        "    \n",
        "    # Calcula la similitud del coseno entre la consulta y cada documento en el corpus\n",
        "    scores = (query_tfidf @ corpus_tfidf_matrix.T).toarray()[0]\n",
        "    \n",
        "    return scores\n",
        "\n",
        "def semantic_query(query: str, corpus_embeddings_matrix: np.array, \n",
        "                   sentence_transformer_model: SentenceTransformer) -> np.array:\n",
        "    \"\"\"\n",
        "    Realiza una consulta semántica utilizando embeddings preentrenados con un modelo de \n",
        "    transformer y calcula la similitud entre los embeddings de la consulta y el corpus.\n",
        "\n",
        "    Args:\n",
        "        query (str): Consulta de entrada.\n",
        "        corpus_embeddings_matrix (np.array): Matriz de embeddings del corpus.\n",
        "        sentence_transformer_model (SentenceTransformer): Modelo preentrenado para generar embeddings.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Array de puntajes de similitud del coseno para cada documento.\n",
        "    \"\"\"    \n",
        "    \n",
        "    # Limpia la consulta (sin stemming ni eliminación de stopwords para mantener el contexto)\n",
        "    cln_query = pattern_newline.sub(' ', query)\n",
        "    cln_query = pattern_multiple_spaces.sub(' ', cln_query).strip()\n",
        "    \n",
        "    # Genera el embedding de la consulta utilizando un modelo transformer\n",
        "    query_emb = sentence_transformer_model.encode([cln_query], \n",
        "                                                  device='cuda',  # Aceleración con GPU\n",
        "                                                  normalize_embeddings=True)  # Normaliza los embeddings\n",
        "    \n",
        "    # Calcula la similitud del coseno entre la consulta y cada documento en el corpus (mediante embeddings)\n",
        "    scores = (query_emb @ corpus_embeddings_matrix.T)[0]\n",
        "    \n",
        "    return scores\n",
        "\n",
        "def hybrid_query_rrf(query: str, sintactic_retriever: partial, semantic_retriever: partial, k: int = 60) -> np.array:\n",
        "    \"\"\"\n",
        "    Combina los puntajes de consultas sintácticas y semánticas utilizando la técnica de \n",
        "    Reciprocal Rank Fusion (RRF), que asigna puntajes inversos a las posiciones de los documentos\n",
        "    en las listas de resultados.\n",
        "\n",
        "    Args:\n",
        "        query (str): Consulta de entrada.\n",
        "        sintactic_retriever (partial): Función de consulta sintáctica (BM25 o TF-IDF).\n",
        "        semantic_retriever (partial): Función de consulta semántica (embeddings).\n",
        "        k (int, optional): Constante que controla la magnitud de los puntajes RRF. Default: 60.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Puntajes finales fusionados de los documentos.\n",
        "    \"\"\"    \n",
        "    \n",
        "    # Recupera puntajes sintácticos y semánticos\n",
        "    sintactic_scores = sintactic_retriever(query)\n",
        "    semantic_scores = semantic_retriever(query)\n",
        "    \n",
        "    # Calcula los rangos inversos (mayor puntaje -> rango 1) para ambas consultas\n",
        "    sintactic_ranks = sintactic_scores.argsort()[::-1].argsort() + 1  # Rango de 1 para el más relevante\n",
        "    semantic_ranks = semantic_scores.argsort()[::-1].argsort() + 1\n",
        "    \n",
        "    # Fusiona los puntajes utilizando la fórmula RRF\n",
        "    rrf_scores = (1 / (k + sintactic_ranks)) + (1 / (k + semantic_ranks))\n",
        "    \n",
        "    return rrf_scores\n",
        "\n",
        "def hybrid_query_avg(query: str, sintactic_retriever: partial, semantic_retriever: partial, \n",
        "                     alpha: float = 0.5) -> np.array:\n",
        "    \"\"\"\n",
        "    Combina los puntajes sintácticos y semánticos utilizando un promedio ponderado, donde el\n",
        "    parámetro `alpha` controla el peso dado a cada tipo de consulta.\n",
        "\n",
        "    Args:\n",
        "        query (str): Consulta de entrada.\n",
        "        sintactic_retriever (partial): Función de consulta sintáctica.\n",
        "        semantic_retriever (partial): Función de consulta semántica.\n",
        "        alpha (float, optional): Peso asignado a la consulta semántica en el promedio. Defaults to 0.5.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Puntajes finales combinados.\n",
        "    \"\"\"      \n",
        "    \n",
        "    # Recupera puntajes sintácticos y normaliza entre 0 y 1\n",
        "    sintactic_scores = sintactic_retriever(query)\n",
        "    sintactic_scores = (sintactic_scores - sintactic_scores.min()) / (sintactic_scores.max() - sintactic_scores.min())\n",
        "    \n",
        "    # Recupera puntajes semánticos y normaliza entre 0 y 1\n",
        "    semantic_scores = semantic_retriever(query)\n",
        "    semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min())\n",
        "    \n",
        "    # Calcula el puntaje combinado usando un promedio ponderado\n",
        "    scores = alpha * semantic_scores + (1 - alpha) * sintactic_scores\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga de archivos (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RSVrz2DPD4Sz"
      },
      "outputs": [],
      "source": [
        "# Mismo script que viene en el nb del baseline \n",
        "\n",
        "# Función para cargar los relevamientos (qrels) y relacionar pasajes con identificadores de documentos\n",
        "def load_qrels(docs_dir: str, fqrels: str) -> Dict[str, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Carga los relevamientos de consultas y pasajes desde archivos JSON.\n",
        "\n",
        "    Esta función asocia cada ID de pasaje a un documento y luego genera una estructura\n",
        "    que permite vincular preguntas (QuestionID) con pasajes relevantes (PassageID).\n",
        "    \n",
        "    Args:\n",
        "        docs_dir (str): Directorio donde se encuentran los documentos estructurados (en formato JSON).\n",
        "        fqrels (str): Archivo JSON que contiene el mapeo de las preguntas y sus pasajes relevantes.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, int]]: Un diccionario con el siguiente formato:\n",
        "            {\n",
        "                \"QuestionID\": {\n",
        "                    \"PassageID\": 1\n",
        "                }\n",
        "            }\n",
        "            Donde 1 indica que el pasaje es relevante para esa pregunta.\n",
        "    \"\"\"    \n",
        "    # Número de documentos a procesar (asumido en 40, puede ser parametrizado)\n",
        "    ndocs = 40\n",
        "    docs = []  # Lista para almacenar los documentos leídos\n",
        "\n",
        "    # Lee cada uno de los archivos de documentos JSON\n",
        "    for i in range(1, ndocs + 1):\n",
        "        with open(os.path.join(docs_dir, f\"{i}.json\")) as f:\n",
        "            doc = json.load(f)  # Carga el contenido JSON del archivo\n",
        "            docs.append(doc)  # Añade el documento a la lista\n",
        "\n",
        "    # Mapea DocumentID y PassageID a IDs de pasajes individuales\n",
        "    did2pid2id: Dict[str, Dict[str, str]] = {}\n",
        "    \n",
        "    # Itera sobre cada documento y mapea los pasajes\n",
        "    for doc in docs:\n",
        "        for psg in doc:\n",
        "            # Si el DocumentID no está en el diccionario, lo inicializa\n",
        "            did2pid2id.setdefault(psg[\"DocumentID\"], {})\n",
        "            # Asegura que no haya duplicados de ID en el DocumentID\n",
        "            assert psg[\"ID\"] not in did2pid2id[psg[\"DocumentID\"]]\n",
        "            # Asocia el PassageID con el ID dentro del diccionario\n",
        "            did2pid2id[psg[\"DocumentID\"]].setdefault(psg[\"PassageID\"], psg[\"ID\"])\n",
        "\n",
        "    # Carga el archivo de relevancia (qrels) para asociar preguntas con pasajes relevantes\n",
        "    with open(fqrels) as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    qrels = {}  # Diccionario para almacenar los relevamientos (query relevance)\n",
        "    \n",
        "    # Recorre las preguntas y los pasajes asociados\n",
        "    for e in data:\n",
        "        qid = e[\"QuestionID\"]  # Obtiene el QuestionID de la consulta\n",
        "        for psg in e[\"Passages\"]:  # Recorre los pasajes relevantes para esa pregunta\n",
        "            qrels.setdefault(qid, {})  # Inicializa el diccionario para el QuestionID si no existe\n",
        "            # Mapea el PassageID al ID del pasaje real, usando did2pid2id\n",
        "            pid = did2pid2id[psg[\"DocumentID\"]][psg[\"PassageID\"]]\n",
        "            # Marca el pasaje como relevante (1) para esa consulta\n",
        "            qrels[qid][pid] = 1\n",
        "    \n",
        "    return qrels  # Devuelve la estructura de relevamientos\n",
        "\n",
        "\n",
        "# Carga los relevamientos desde el directorio de documentos y el archivo de preguntas\n",
        "file_type = 'test'\n",
        "qrels = load_qrels(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"ObliQADataset/ObliQA_{file_type}.json\")\n",
        "\n",
        "# Escribe los relevamientos en un archivo en formato de texto con cada línea representando\n",
        "# un QuestionID, Q0, PassageID y relevancia (1) para el formato TREC\n",
        "with open(\"qrels\", \"w\") as f:\n",
        "    for qid, rels in qrels.items():\n",
        "        for pid, rel in rels.items():\n",
        "            # Formatea cada línea en el formato estándar: QuestionID, Q0, PassageID, Relevance\n",
        "            line = f\"{qid} Q0 {pid} {rel}\"\n",
        "            f.write(line + \"\\n\")  # Escribe la línea en el archivo de salida\n",
        "\n",
        "\n",
        "# Proceso para cargar una colección de pasajes desde el directorio de documentos estructurados\n",
        "ndocs = 40  # Número de documentos a procesar\n",
        "collection = []  # Lista para almacenar la colección de pasajes\n",
        "\n",
        "# Lee cada documento y extrae los pasajes relevantes\n",
        "for i in range(1, ndocs + 1):\n",
        "    with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
        "        doc = json.load(f)  # Carga el contenido del documento JSON\n",
        "        for psg in doc:  # Recorre cada pasaje del documento\n",
        "            # Añade cada pasaje a la colección como un diccionario con los campos relevantes si\n",
        "            # la longitud del texto es mayor a 50 caracteres (para evitar pasajes vacíos o irrelevantes por\n",
        "            # su corta longitud) \n",
        "            if len(psg[\"PassageID\"] + \" \" + psg[\"Passage\"])>100: # Mejora propuesta por el grupo\n",
        "                collection.append(\n",
        "                    dict(\n",
        "                        text=psg[\"PassageID\"] + \" \" + psg[\"Passage\"],  # Combina el PassageID y el texto del pasaje\n",
        "                        ID=psg[\"ID\"],  # ID del pasaje\n",
        "                        DocumentId=psg['DocumentID'],  # ID del documento\n",
        "                        PassageId=psg['PassageID'],  # ID del pasaje\n",
        "                    )\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sparse sintactic representation: BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10592"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenizamos y limpiamos el corpus (colección de documentos)\n",
        "# Cada documento en 'collection' se limpia usando la función clean_text y luego se tokeniza.\n",
        "# La función tokenizer genera unigramas y bigramas para cada documento.\n",
        "tokenized_corpus = [tokenizer(clean_text(doc['text'])) for doc in collection]\n",
        "\n",
        "# Inicializamos el modelo BM25 con el corpus tokenizado\n",
        "# BM25 es un modelo de recuperación de información que pondera la relevancia de los documentos basándose\n",
        "# en la frecuencia de términos, la longitud promedio de documentos y dos hiperparámetros k1 y b:\n",
        "# - k1 controla qué tan fuertemente se pondera la frecuencia del término (TF).\n",
        "# - b controla cuánto impacto tiene la longitud del documento.\n",
        "bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)\n",
        "\n",
        "# Convertimos la colección original de documentos en un array de NumPy para facilitar su manejo posterior.\n",
        "# Este array es más eficiente para operaciones que requieren acceso por índice o que involucran grandes conjuntos de datos.\n",
        "collection_array = np.array(collection)\n",
        "\n",
        "# Verificamos la longitud del corpus tokenizado (número de documentos procesados).\n",
        "# Esto es útil para asegurarnos de que todos los documentos hayan sido correctamente tokenizados.\n",
        "len(tokenized_corpus) # 10592 (originalmente 13732)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sparse sintactic representation: TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/raul/Escritorio/extra/misis/rl/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Inicializamos un vectorizador TF-IDF con bigramas y un conjunto de parámetros para optimizar la representación.\n",
        "# Parámetros:\n",
        "# - ngram_range=(1, 2): Utiliza tanto unigramas como bigramas.\n",
        "# - max_features=30000: Limita la matriz TF-IDF a las 30,000 características (términos) más frecuentes.\n",
        "# - max_df=0.9: Ignora términos que aparezcan en más del 90% de los documentos (considerados como demasiado comunes).\n",
        "# - min_df=3: Ignora términos que aparezcan en menos de 3 documentos (considerados como demasiado raros).\n",
        "# - stop_words='english': Utiliza la lista de stopwords en inglés para eliminar términos irrelevantes.\n",
        "# - preprocessor=clean_text: Aplica la función de limpieza del texto antes de vectorizar los documentos.\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=30000, \n",
        "                                   max_df=0.9, min_df=3, stop_words='english', \n",
        "                                   preprocessor=clean_text\n",
        "                                   )\n",
        "\n",
        "# Transformamos el corpus en una matriz TF-IDF, donde cada documento se representa como un vector de características\n",
        "# basadas en las frecuencias ponderadas de términos (utilizando tanto unigramas como bigramas).\n",
        "corpus_tfidf_matrix = tfidf_vectorizer.fit_transform([doc['text'] for doc in collection_array])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dense semantic representation: sentence-transformers/all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41038fea69ce455f90e303b7da81cbe2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/331 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Codificamos los documentos en embeddings semánticos utilizando un modelo preentrenado basado en transformers.\n",
        "# Parámetros:\n",
        "# - device='cuda': Utiliza la GPU para acelerar el cálculo de embeddings.\n",
        "# - normalize_embeddings=True: Normaliza los embeddings para asegurar que las magnitudes de los vectores no dominen\n",
        "#   las comparaciones, lo cual es útil para cálculos de similitud del coseno.\n",
        "# - show_progress_bar=True: Muestra una barra de progreso durante el proceso de codificación.\n",
        "# - max_length=512: Limita la longitud máxima de los textos a 512 tokens (esto es útil para modelos de transformers,\n",
        "#   que suelen tener límites de longitud en la entrada).\n",
        "corpus_embeddings_matrix = sentence_transformer_model.encode([i['text'] for i in collection_array],\n",
        "                          device='cuda',\n",
        "                          normalize_embeddings=True,\n",
        "                          show_progress_bar=True,\n",
        "                          max_length=512,\n",
        "                          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuramos los recuperadores semántico y sintáctico\n",
        "\n",
        "# Utilizamos `partial` para crear una función de recuperación semántica personalizada.\n",
        "# El objetivo es crear una función que ya esté configurada con la matriz de embeddings precomputada y el modelo de embeddings.\n",
        "# Esto permite pasar directamente la consulta sin tener que recalcular o volver a cargar estos recursos.\n",
        "semantic_retriever = partial(semantic_query, corpus_embeddings_matrix=corpus_embeddings_matrix,\n",
        "                             sentence_transformer_model=sentence_transformer_model)\n",
        "\n",
        "# De manera similar, creamos una función de recuperación sintáctica basada en el modelo TF-IDF.\n",
        "# Esta función toma la matriz de TF-IDF del corpus y el vectorizador TF-IDF, permitiendo ejecutar consultas \n",
        "# directamente sin necesidad de recalcular la matriz o el vectorizador.\n",
        "sintactic_tfidf_retriever = partial(sintactic_query_tfidf, corpus_tfidf_matrix=corpus_tfidf_matrix,\n",
        "                                   tfidf_vectorizer=tfidf_vectorizer)\n",
        "\n",
        "# Creamos una función de recuperación sintáctica utilizando el modelo BM25.\n",
        "# En este caso, el `bm5_instance` ya está configurado con el corpus tokenizado, permitiendo realizar consultas\n",
        "# sobre el modelo BM25 directamente.\n",
        "sintactic_bm25_retriever = partial(sintactic_query_bm5, bm5_instance=bm25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22295/22295 [05:01<00:00, 73.87it/s]\n"
          ]
        }
      ],
      "source": [
        "# Casi el Mismo script que viene en el nb del baseline \n",
        "\n",
        "# Diccionario para almacenar los resultados recuperados para cada consulta (QuestionID)\n",
        "retrieved = {}\n",
        "# Número de documentos a recuperar por cada consulta\n",
        "top_n = 10\n",
        "\n",
        "# Abrimos el archivo JSON que contiene las consultas de prueba (ObliQA_test.json)\n",
        "with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
        "    data = json.load(f)  # Cargamos el contenido del archivo JSON\n",
        "    \n",
        "    # Iteramos sobre cada entrada (pregunta) en el archivo de datos\n",
        "    for e in tqdm(data):  # tqdm agrega una barra de progreso durante la iteración\n",
        "        query = e['Question']  # Extraemos la pregunta o consulta desde el campo 'Question'\n",
        "        \n",
        "        # Realizamos una consulta híbrida combinando la recuperación sintáctica (BM25) y la semántica (embeddings)\n",
        "        # El valor de alpha controla el peso que le damos a la consulta semántica vs. sintáctica. En este caso,\n",
        "        # alpha=0.6 implica que el score semántico es ligeramente superior.\n",
        "        scores = hybrid_query_avg(\n",
        "                                query,\n",
        "                                sintactic_retriever=sintactic_tfidf_retriever,  # Recuperador BM25 para consultas sintácticas\n",
        "                                semantic_retriever=semantic_retriever,  # Recuperador de embeddings para consultas semánticas\n",
        "                                alpha=0.6  # Peso ligeramente superior para la consulta semántica\n",
        "                                )\n",
        "        \n",
        "        # Obtenemos los índices de los `top_n` documentos con mayor puntaje utilizando np.argpartition.\n",
        "        # np.argpartition es más eficiente que ordenar completamente todos los documentos cuando solo necesitamos\n",
        "        # los primeros `n` documentos.\n",
        "        top_k = np.argpartition(-scores, top_n)[:top_n]\n",
        "        \n",
        "        # Ordenamos los índices de los `top_n` documentos de acuerdo con sus puntajes.\n",
        "        # Esto asegura que los documentos se devuelvan en el orden correcto según sus puntuaciones.\n",
        "        top_k = top_k[np.argsort(-scores[top_k])]\n",
        "\n",
        "        # Recuperamos los documentos correspondientes a los índices seleccionados.\n",
        "        # collection_array contiene el texto y los metadatos de todos los documentos, por lo que usamos top_k para\n",
        "        # seleccionar solo los documentos más relevantes para la consulta.\n",
        "        top_docs = collection_array[top_k]\n",
        "\n",
        "        # También extraemos los puntajes correspondientes a estos documentos\n",
        "        top_scores = scores[top_k]\n",
        "\n",
        "        # Creamos una lista de diccionarios, donde cada diccionario contiene los detalles del documento\n",
        "        # y su puntaje de relevancia.\n",
        "        top_results = [{**doc, 'score': score} for doc, score in zip(top_docs, top_scores)]\n",
        "        \n",
        "        # Guardamos los resultados para la consulta actual en el diccionario 'retrieved'\n",
        "        # donde la clave es el QuestionID y el valor es la lista de los documentos más relevantes.\n",
        "        retrieved[e[\"QuestionID\"]] = top_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pd.to_pickle(retrieved, \"data/retrieved_train_hard_negatives.pkl\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mismo script que viene en el nb del baseline \n",
        "\n",
        "# Escribimos los resultados recuperados en un archivo en formato TREC para evaluación\n",
        "with open(\"rankings.trec\", \"w\") as f:\n",
        "    for qid, hits in retrieved.items():  # Iteramos sobre cada QuestionID y su lista de documentos relevantes\n",
        "        for i, hit in enumerate(hits):  # Para cada documento recuperado, generamos una línea en formato TREC\n",
        "            # Formato de línea: QuestionID, Q0, DocumentID, Rank, Score, Método\n",
        "            line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} bm25\"\n",
        "            f.write(line + \"\\n\")  # Escribimos la línea en el archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVWcEMRRQfWZ",
        "outputId": "8ec08829-4340-4167-8e0d-fe6c9de74505"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall_10             \tall\t0.8062\n",
            "map_cut_10            \tall\t0.6698\n"
          ]
        }
      ],
      "source": [
        "# Evaluación de la recuperación utilizando 'trec_eval'\n",
        "# Utilizamos la herramienta 'trec_eval' para medir el rendimiento del sistema de recuperación en términos de\n",
        "# métricas como MAP (Mean Average Precision) y Recall.\n",
        "# Las opciones -m recall.10 y -m map_cut.10 indican que estamos evaluando el recall en los primeros 10 documentos\n",
        "# y el MAP (precision promedio) en los primeros 10 documentos.\n",
        "\n",
        "!trec_eval/trec_eval -m recall.10 -m map_cut.10 ./qrels ./rankings.trec"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "alpha = 0.6\n",
        "recall_10             \tall\t0.8062\n",
        "map_cut_10            \tall\t0.6698"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## BaseLine \n",
        "\n",
        "recall_10             \tall\t0.7611\n",
        "map_cut_10            \tall\t0.6237"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
