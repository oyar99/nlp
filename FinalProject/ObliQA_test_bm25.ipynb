{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!git clone https://github.com/usnistgov/trec_eval.git && cd trec_eval && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/raul/Escritorio/extra/misis/rl/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "from tqdm import tqdm\n",
        "from re import compile\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#import unicodedata\n",
        "import contractions\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n1K8SXctD0Uw"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "pattern_newline = compile(r'[\\n\\t\\u200e]')\n",
        "pattern_non_alphanumeric = compile(r'[^a-z0-9]')\n",
        "pattern_multiple_spaces = compile(r' +')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Expande contracciones\n",
        "    cln_text = contractions.fix(text)\n",
        "    \n",
        "    # Convierte el texto a minúsculas\n",
        "    cln_text = cln_text.lower()\n",
        "    \n",
        "    # Normalización Unicode\n",
        "    #cln_text = unicodedata.normalize('NFKD', cln_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    \n",
        "    #  Elimina saltos de línea y caracteres no deseados\n",
        "    cln_text = pattern_newline.sub(' ', cln_text)\n",
        "    \n",
        "    # Elimina caracteres no alfanuméricos\n",
        "    cln_text = pattern_non_alphanumeric.sub(' ', cln_text)\n",
        "    \n",
        "    #tokens \n",
        "    tokens = [stemmer.stem(word) for word in word_tokenize(cln_text) if word not in stop_words]\n",
        "    # Filtra las stopwords usando un conjunto\n",
        "    cln_text = ' '.join(tokens)\n",
        "    \n",
        "    # Elimina múltiples espacios consecutivos\n",
        "    cln_text = pattern_multiple_spaces.sub(' ', cln_text).strip()\n",
        "    \n",
        "    return cln_text\n",
        "\n",
        "# create funcion that tokenize in unigrams and bigrams\n",
        "def tokenizer(text):\n",
        "    tokens = text.split()\n",
        "    unigrams = tokens\n",
        "    bigrams = [f\"{tokens[i]} {tokens[i + 1]}\" for i in range(len(tokens) - 1)]\n",
        "    #tigrams = [f\"{tokens[i]} {tokens[i + 1]} {tokens[i + 2]}\" for i in range(len(tokens) - 2)]\n",
        "    return unigrams + bigrams\n",
        "\n",
        "\n",
        "def load_qrels(docs_dir: str, fqrels: str) -> Dict[str, Dict[str, int]]:\n",
        "    ndocs = 40\n",
        "    docs = []\n",
        "    for i in range(1, ndocs + 1):\n",
        "        with open(os.path.join(docs_dir, f\"{i}.json\")) as f:\n",
        "            doc = json.load(f)\n",
        "            docs.append(doc)\n",
        "\n",
        "    did2pid2id: Dict[str, Dict[str, str]] = {}\n",
        "    for doc in docs:\n",
        "        for psg in doc:\n",
        "            did2pid2id.setdefault(psg[\"DocumentID\"], {})\n",
        "            assert psg[\"ID\"] not in did2pid2id[psg[\"DocumentID\"]]\n",
        "            did2pid2id[psg[\"DocumentID\"]].setdefault(psg[\"PassageID\"], psg[\"ID\"])\n",
        "\n",
        "    with open(fqrels) as f:\n",
        "        data = json.load(f)\n",
        "    qrels = {}\n",
        "    for e in data:\n",
        "        qid = e[\"QuestionID\"]\n",
        "        for psg in e[\"Passages\"]:\n",
        "            qrels.setdefault(qid, {})\n",
        "            pid = did2pid2id[psg[\"DocumentID\"]][psg[\"PassageID\"]]\n",
        "            qrels[qid][pid] = 1\n",
        "    return qrels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RSVrz2DPD4Sz"
      },
      "outputs": [],
      "source": [
        "qrels = load_qrels(\"ObliQADataset/StructuredRegulatoryDocuments\", \"ObliQADataset/ObliQA_test.json\")\n",
        "with open(\"qrels\", \"w\") as f:\n",
        "    for qid, rels in qrels.items():\n",
        "        for pid, rel in rels.items():\n",
        "            line = f\"{qid} Q0 {pid} {rel}\"\n",
        "            f.write(line + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-B0RWSNRD18h",
        "outputId": "0893dbfc-363e-437e-ebcd-d0e0fa38a3e0"
      },
      "outputs": [],
      "source": [
        "ndocs = 40\n",
        "collection = []\n",
        "for i in range(1, ndocs + 1):\n",
        "    with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
        "        doc = json.load(f)\n",
        "        for psg in doc:\n",
        "            collection.append(\n",
        "                dict(text=psg[\"PassageID\"] + \" \" + psg[\"Passage\"], ID=psg[\"ID\"],\n",
        "                     DcoumentId=psg['DocumentID'],\n",
        "                     PassageId=psg['PassageID'],\n",
        "                     )\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13732"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sparse sintactic representation\n",
        "\n",
        "tokenized_corpus = [tokenizer(clean_text(doc['text'])) for doc in collection]\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)\n",
        "\n",
        "collection_array = np.array(collection)\n",
        "\n",
        "len(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "647145b9912e4ccbb6bd61f553171d78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/430 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Dense semantic representation\n",
        "\n",
        "embeddings = model.encode([i['text'] for i in collection_array],\n",
        "                          device='cuda',\n",
        "                          normalize_embeddings=True,\n",
        "                          show_progress_bar=True,\n",
        "                          max_length=512,\n",
        "                          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sintactic_query(query:str):\n",
        "        tokenized_query = tokenizer(clean_text(query))\n",
        "        \n",
        "        scores = bm25.get_scores(tokenized_query)\n",
        "        \n",
        "        return scores\n",
        "    \n",
        "def semantic_query(query: str):\n",
        "    \n",
        "    cln_query = pattern_newline.sub(' ', query)\n",
        "    cln_query = pattern_multiple_spaces.sub(' ', cln_query).strip()\n",
        "    \n",
        "    query_emb = model.encode([cln_query], \n",
        "                             device='cuda',\n",
        "                             normalize_embeddings=True)\n",
        "    \n",
        "    scores = (query_emb@embeddings.T)[0]\n",
        "    \n",
        "    return scores\n",
        "\n",
        "\n",
        "def hybrid_query_rrf(query: str, k: int = 60):\n",
        "    sintactic_scores = sintactic_query(query)\n",
        "    semantic_scores = semantic_query(query)\n",
        "    \n",
        "    # Obtener los rankings inversos (mayor puntaje = rango 1)\n",
        "    # Añadimos 1 para que los rangos empiecen en 1\n",
        "    sintactic_ranks = sintactic_scores.argsort()[::-1].argsort() + 1 \n",
        "    semantic_ranks = semantic_scores.argsort()[::-1].argsort() + 1\n",
        "    \n",
        "    # Calcular los puntajes RRF para cada documento\n",
        "    rrf_scores = (1 / (k + sintactic_ranks)) + (1 / (k + semantic_ranks))\n",
        "    \n",
        "    return rrf_scores\n",
        "\n",
        "    \n",
        "def hybrid_query_avg(query: str, alpha: float = 0.5):\n",
        "    \n",
        "    sintactic_scores = sintactic_query(query)\n",
        "    sintactic_scores = (sintactic_scores - sintactic_scores.min()) / (sintactic_scores.max() - sintactic_scores.min())\n",
        "    \n",
        "    semantic_scores = semantic_query(query)\n",
        "    semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min())\n",
        "    \n",
        "    scores = alpha*semantic_scores + (1-alpha)*sintactic_scores\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2786/2786 [04:57<00:00,  9.35it/s]\n"
          ]
        }
      ],
      "source": [
        "retrieved = {}\n",
        "top_n = 10\n",
        "with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
        "    data = json.load(f)\n",
        "    for e in tqdm(data):\n",
        "        query = e['Question']\n",
        "        \n",
        "        scores = hybrid_query_avg(query)\n",
        "        \n",
        "        top_k = np.argpartition(-scores, top_n)[:top_n]\n",
        "        # Ordenamos correctamente los índices según sus puntuaciones\n",
        "        top_k = top_k[np.argsort(-scores[top_k])]\n",
        "\n",
        "        # Recuperamos los documentos correspondientes\n",
        "        top_docs = collection_array[top_k]\n",
        "\n",
        "        # Si deseas incluir las puntuaciones en los resultados\n",
        "        top_scores = scores[top_k]\n",
        "        top_results = [{**doc, 'score': score} for doc, score in zip(top_docs, top_scores)]\n",
        "        \n",
        "        retrieved[e[\"QuestionID\"]] = top_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"rankings.trec\", \"w\") as f:\n",
        "  for qid, hits in retrieved.items():\n",
        "    for i, hit in enumerate(hits):\n",
        "      line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} bm25\"\n",
        "      f.write(line + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVWcEMRRQfWZ",
        "outputId": "8ec08829-4340-4167-8e0d-fe6c9de74505"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall_10             \tall\t0.8024\n",
            "map_cut_10            \tall\t0.6637\n"
          ]
        }
      ],
      "source": [
        "!trec_eval/trec_eval -m recall.10 -m map_cut.10 ./qrels ./rankings.trec"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "alpha = 0.5\n",
        "recall_10             \tall\t0.8024\n",
        "map_cut_10            \tall\t0.6637"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## BaseLine \n",
        "\n",
        "recall_10             \tall\t0.7611\n",
        "\n",
        "map_cut_10            \tall\t0.6237"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
