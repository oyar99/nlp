{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: la ruta de destino 'trec_eval' ya existe y no es un directorio vacío.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/usnistgov/trec_eval.git && cd trec_eval && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "from tqdm import tqdm\n",
        "from re import sub\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "n1K8SXctD0Uw"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convierte el texto a minúsculas\n",
        "    cln_text = text.lower()\n",
        "    \n",
        "    # Elimina saltos de línea, tabulaciones y caracteres no deseados como \\u200e\n",
        "    cln_text = sub(r'[\\n\\t\\u200e]', ' ', cln_text)\n",
        "    \n",
        "    # remove all non-alphanumeric characters\n",
        "    cln_text = sub(r'[^a-z0-9]', ' ', cln_text)\n",
        "    \n",
        "    # Filtra las stopwords usando un conjunto\n",
        "    cln_text = ' '.join([stemmer.stem(word) for word in cln_text.split() if word not in stop_words])\n",
        "    \n",
        "    # Elimina múltiples espacios consecutivos\n",
        "    cln_text = sub(r' +', ' ', cln_text).strip()\n",
        "    \n",
        "    return cln_text\n",
        "\n",
        "# create funcion that tokenize in unigrams and bigrams\n",
        "def tokenizer(text):\n",
        "    tokens = text.split()\n",
        "    unigrams = tokens\n",
        "    bigrams = [f\"{tokens[i]} {tokens[i + 1]}\" for i in range(len(tokens) - 1)]\n",
        "    return unigrams + bigrams\n",
        "\n",
        "def load_qrels(docs_dir: str, fqrels: str) -> Dict[str, Dict[str, int]]:\n",
        "    ndocs = 40\n",
        "    docs = []\n",
        "    for i in range(1, ndocs + 1):\n",
        "        with open(os.path.join(docs_dir, f\"{i}.json\")) as f:\n",
        "            doc = json.load(f)\n",
        "            docs.append(doc)\n",
        "\n",
        "    did2pid2id: Dict[str, Dict[str, str]] = {}\n",
        "    for doc in docs:\n",
        "        for psg in doc:\n",
        "            did2pid2id.setdefault(psg[\"DocumentID\"], {})\n",
        "            assert psg[\"ID\"] not in did2pid2id[psg[\"DocumentID\"]]\n",
        "            did2pid2id[psg[\"DocumentID\"]].setdefault(psg[\"PassageID\"], psg[\"ID\"])\n",
        "\n",
        "    with open(fqrels) as f:\n",
        "        data = json.load(f)\n",
        "    qrels = {}\n",
        "    for e in data:\n",
        "        qid = e[\"QuestionID\"]\n",
        "        for psg in e[\"Passages\"]:\n",
        "            qrels.setdefault(qid, {})\n",
        "            pid = did2pid2id[psg[\"DocumentID\"]][psg[\"PassageID\"]]\n",
        "            qrels[qid][pid] = 1\n",
        "    return qrels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "RSVrz2DPD4Sz"
      },
      "outputs": [],
      "source": [
        "qrels = load_qrels(\"ObliQADataset/StructuredRegulatoryDocuments\", \"ObliQADataset/ObliQA_test.json\")\n",
        "with open(\"qrels\", \"w\") as f:\n",
        "    for qid, rels in qrels.items():\n",
        "        for pid, rel in rels.items():\n",
        "            line = f\"{qid} Q0 {pid} {rel}\"\n",
        "            f.write(line + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-B0RWSNRD18h",
        "outputId": "0893dbfc-363e-437e-ebcd-d0e0fa38a3e0"
      },
      "outputs": [],
      "source": [
        "ndocs = 40\n",
        "collection = []\n",
        "for i in range(1, ndocs + 1):\n",
        "    with open(os.path.join(\"ObliQADataset/StructuredRegulatoryDocuments\", f\"{i}.json\")) as f:\n",
        "        doc = json.load(f)\n",
        "        for psg in doc:\n",
        "            collection.append(\n",
        "                dict(text=psg[\"PassageID\"] + \" \" + psg[\"Passage\"], ID=psg[\"ID\"],\n",
        "                     DcoumentId=psg['DocumentID'],\n",
        "                     PassageId=psg['PassageID'],\n",
        "                     )\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13732"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_corpus = [tokenizer(clean_text(doc['text'])) for doc in collection]\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)\n",
        "\n",
        "collection_array = np.array(collection)\n",
        "\n",
        "len(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2786/2786 [03:26<00:00, 13.51it/s]\n"
          ]
        }
      ],
      "source": [
        "retrieved = {}\n",
        "with open(\"ObliQADataset/ObliQA_test.json\") as f:\n",
        "    data = json.load(f)\n",
        "    for e in tqdm(data):\n",
        "        query = e['Question']\n",
        "        tokenized_query = tokenizer(clean_text(query))\n",
        "        \n",
        "        doc_scores = bm25.get_scores(tokenized_query)\n",
        "        \n",
        "        # Obtenemos los índices de los 10 puntajes más altos\n",
        "        top_10_indices = np.argpartition(-doc_scores, 10)[:10]\n",
        "        # Ordenamos correctamente los 10 mejores índices\n",
        "        top_10_indices = top_10_indices[np.argsort(-doc_scores[top_10_indices])]\n",
        "        top_10_scores = doc_scores[top_10_indices]\n",
        "        \n",
        "        # Recuperamos los documentos correspondientes y añadimos el puntaje\n",
        "        top_10_docs = [{**collection_array[i], 'score': score} for i, score in zip(top_10_indices, top_10_scores)]\n",
        "        \n",
        "        retrieved[e[\"QuestionID\"]] = top_10_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"rankings.trec\", \"w\") as f:\n",
        "  for qid, hits in retrieved.items():\n",
        "    for i, hit in enumerate(hits):\n",
        "      line = f\"{qid} 0 {hit['ID']} {i+1} {hit['score']} bm25\"\n",
        "      f.write(line + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVWcEMRRQfWZ",
        "outputId": "8ec08829-4340-4167-8e0d-fe6c9de74505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall_10             \tall\t0.7771\n",
            "map_cut_10            \tall\t0.6338\n"
          ]
        }
      ],
      "source": [
        "!trec_eval/trec_eval -m recall.10 -m map_cut.10 ./qrels ./rankings.trec"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## BaseLine \n",
        "\n",
        "recall_10             \tall\t0.7611\n",
        "\n",
        "map_cut_10            \tall\t0.6237"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
