{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Del punto dos\n",
    "def process_text(documents):\n",
    "    \"\"\"\n",
    "    Limpia y procesa los datos de texto crudo.\n",
    "\n",
    "    Parámetros:\n",
    "    - documents (dict): Un diccionario de IDs de documentos y texto crudo.\n",
    "\n",
    "    Devuelve:\n",
    "    - documents_dict (dict): Un diccionario de IDs de documentos y tokens procesados.\n",
    "    \"\"\"\n",
    "    documents_dict = {}\n",
    "\n",
    "    for key, value in documents.items():\n",
    "        # Convierte a minúsculas\n",
    "        text = value.lower()\n",
    "        \n",
    "        # Tokeniza en palabras\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Elimina tokens no alfabéticos, como puntos y comas\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        \n",
    "        # Elimina palabras de parada\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Aplica stemming\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        documents_dict[key] = tokens\n",
    "\n",
    "    return documents_dict\n",
    "\n",
    "def crear_tf_idf_matrix(inverted_index: dict[dict[int]], \n",
    "                        tf_log_scale: bool = True,\n",
    "                        normalize_matrix: bool = True) -> tuple[np.array, list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Calcula la matriz TF-IDF a partir de un índice invertido.\n",
    "\n",
    "    Esta función toma un índice invertido que mapea términos a documentos y sus frecuencias, \n",
    "    y calcula la matriz TF-IDF para cada término en cada documento. La opción de usar escala \n",
    "    logarítmica para la frecuencia de términos (TF) es configurable, así como la opción de \n",
    "    normalizar la matriz resultante.\n",
    "\n",
    "    Args:\n",
    "        inverted_index (dict[dict[int]]): \n",
    "            Un diccionario donde las claves son términos (str) y los valores son diccionarios.\n",
    "            Estos diccionarios internos mapean el ID del documento (int) a la frecuencia de ese \n",
    "            término en el documento.\n",
    "\n",
    "        tf_log_scale (bool, opcional): \n",
    "            Indica si se debe aplicar escala logarítmica al cálculo de la frecuencia de términos (TF).\n",
    "            Por defecto es True, lo que significa que se usará la fórmula `log10(1 + frecuencia)`. Si \n",
    "            se establece en False, se usará la frecuencia sin escala logarítmica.\n",
    "\n",
    "        normalize_matrix (bool, opcional): \n",
    "            Indica si se debe normalizar la matriz TF-IDF resultante. La normalización se realiza por \n",
    "            filas, lo que significa que cada vector de documento se ajusta para que su norma sea 1. \n",
    "            Por defecto es True.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.array, list[str], list[int]]: \n",
    "            Retorna una tupla que contiene:\n",
    "            - tf_idf_matrix (np.array): Un array bidimensional donde cada fila representa un documento \n",
    "              y cada columna representa un término. Los valores en la matriz son los pesos TF-IDF \n",
    "              correspondientes.\n",
    "            - terms (list[str]): Una lista ordenada de los términos presentes en el índice invertido.\n",
    "            - docs (list[int]): Una lista ordenada de los IDs de documentos únicos en el corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtiene y ordena los términos únicos en el índice invertido.\n",
    "    terms = sorted(list(inverted_index.keys()))\n",
    "\n",
    "    # Extrae y ordena los IDs de documentos únicos en el corpus.\n",
    "    docs = {doc for docs_freq in inverted_index.values() for doc in docs_freq}\n",
    "    docs = sorted(list(docs))\n",
    "\n",
    "    # Calcula el Inverse Document Frequency (IDF) para cada término.\n",
    "    # IDF = log10(N / df), donde N es el número total de documentos y df es la frecuencia de documentos \n",
    "    # que contienen el término.\n",
    "    idf = {term: np.log10(len(docs) / len(inverted_index[term]))\n",
    "           for term in terms}\n",
    "\n",
    "    # Calcula el Term Frequency (TF) para cada término en cada documento.\n",
    "    # Si se aplica escala logarítmica, se usa la fórmula: TF = log10(1 + frecuencia).\n",
    "    # Si no, se utiliza la frecuencia directa.\n",
    "    if tf_log_scale:\n",
    "        tf = {doc: {term: np.log10(1 + inverted_index[term].get(doc, 0)) \n",
    "                    for term in inverted_index.keys()} for doc in docs}\n",
    "    else:\n",
    "        tf = {doc: {term: inverted_index[term].get(\n",
    "            doc, 0) for term in inverted_index.keys()} for doc in docs}\n",
    "\n",
    "    # Calcula el TF-IDF para cada término en cada documento.\n",
    "    # TF-IDF = TF * IDF para cada término en cada documento.\n",
    "    tf_idf = {doc: {term: tf[doc][term] * idf[term]\n",
    "                    for term in terms} for doc in docs}\n",
    "\n",
    "    # Convierte el diccionario de TF-IDF en una matriz numpy para facilitar el procesamiento posterior.\n",
    "    # Cada fila de la matriz representa un documento y cada columna un término.\n",
    "    tf_idf_matrix = np.array([[tf_idf[doc][term] for term in terms] for doc in docs])\n",
    "    \n",
    "    # Normaliza la matriz TF-IDF, de manera que la norma de cada vector de documento sea 1.\n",
    "    if normalize_matrix:\n",
    "        tf_idf_matrix = tf_idf_matrix / np.linalg.norm(tf_idf_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    # Retorna la matriz TF-IDF, la lista de términos y la lista de documentos en el orden correspondiente.\n",
    "    return tf_idf_matrix, terms, docs, idf\n",
    "\n",
    "\n",
    "def cosine_simi(v1: np.array, v2: np.array, asume_norm_1: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la similitud coseno entre dos vectores.\n",
    "\n",
    "    Esta función calcula la similitud coseno entre dos vectores `v1` y `v2`. \n",
    "    La similitud coseno es una medida de la similitud entre dos vectores \n",
    "    en un espacio vectorial que mide el coseno del ángulo entre ellos. \n",
    "    Se utiliza comúnmente en análisis de texto y en tareas de recuperación de información.\n",
    "\n",
    "    Args:\n",
    "        v1 (np.array): \n",
    "            El primer vector (como un array de NumPy) con el que se calculará la similitud.\n",
    "        \n",
    "        v2 (np.array): \n",
    "            El segundo vector (como un array de NumPy) con el que se calculará la similitud.\n",
    "        \n",
    "        asume_norm_1 (bool, opcional): \n",
    "            Si se establece en True, la función asume que ambos vectores `v1` y `v2` \n",
    "            ya están normalizados (es decir, su norma es 1). Esto permite omitir el \n",
    "            cálculo de las normas, mejorando la eficiencia. Por defecto es False.\n",
    "\n",
    "    Returns:\n",
    "        float: \n",
    "            Un valor de tipo float que representa la similitud coseno entre los dos vectores.\n",
    "            Un valor cercano a 1 indica que los vectores son muy similares (paralelos), \n",
    "            mientras que un valor cercano a 0 indica que son ortogonales (no relacionados).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if asume_norm_1:\n",
    "        # Si se asume que los vectores ya están normalizados (norma = 1), \n",
    "        # la similitud coseno se reduce al producto punto entre ellos.\n",
    "        return np.dot(v1, v2)\n",
    "    else:\n",
    "        # Si los vectores no están normalizados, se calcula la similitud coseno \n",
    "        # como el producto punto dividido por el producto de las normas de los vectores.\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = json.loads(open('./output/inverted_index.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix, terms, docs, corpus_idf = crear_tf_idf_matrix(inverted_index, \n",
    "                                                             tf_log_scale=True,\n",
    "                                                             normalize_matrix=True\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms == list(corpus_idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'hey you! i am a physician physician physician'\n",
    "\n",
    "def crear_vector_tf_idf(text:str,\n",
    "                        tf_log_scale: bool = True,\n",
    "                        normalize_vector: bool = True)-> np.array:\n",
    "    \n",
    "    text_cln = process_text({'0': text})['0']\n",
    "\n",
    "    if tf_log_scale:\n",
    "        txt_vector = np.array([\n",
    "            np.log10(1 + text_cln.count(term)) * corpus_idf[term] if term in text_cln else 0\n",
    "            for term in terms])\n",
    "    else:\n",
    "        txt_vector = np.array([\n",
    "            text_cln.count(term) * corpus_idf[term] if term in text_cln else 0\n",
    "            for term in terms])\n",
    "\n",
    "    if normalize_vector:\n",
    "        txt_vector = txt_vector / np.linalg.norm(txt_vector)\n",
    "        \n",
    "    return txt_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crear_vector_tf_idf(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
