{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"1-MetricasDeEvaluacionDeIR.ipynb\"\n",
    "%run \"2-BusquedaBinariaUsandoIndiceInvertido.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción matriz/vector tf-idf a partir del índice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_tf_idf_matrix(inverted_index: dict[dict[int]], \n",
    "                        tf_log_scale: bool = True,\n",
    "                        normalize_matrix: bool = True) -> tuple[np.array, list[str], list[int], dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Calcula la matriz TF-IDF a partir de un índice invertido.\n",
    "\n",
    "    Esta función toma un índice invertido que mapea términos a documentos y sus frecuencias, \n",
    "    y calcula la matriz TF-IDF para cada término en cada documento. La opción de usar escala \n",
    "    logarítmica para la frecuencia de términos (TF) es configurable, así como la opción de \n",
    "    normalizar la matriz resultante.\n",
    "\n",
    "    Args:\n",
    "        inverted_index (dict[dict[int]]): \n",
    "            Un diccionario donde las claves son términos (str) y los valores son diccionarios.\n",
    "            Estos diccionarios internos mapean el ID del documento (int) a la frecuencia de ese \n",
    "            término en el documento.\n",
    "\n",
    "        tf_log_scale (bool, opcional): \n",
    "            Indica si se debe aplicar escala logarítmica al cálculo de la frecuencia de términos (TF).\n",
    "            Por defecto es True, lo que significa que se usará la fórmula `log10(1 + frecuencia)`. Si \n",
    "            se establece en False, se usará la frecuencia sin escala logarítmica.\n",
    "\n",
    "        normalize_matrix (bool, opcional): \n",
    "            Indica si se debe normalizar la matriz TF-IDF resultante. La normalización se realiza por \n",
    "            filas, lo que significa que cada vector de documento se ajusta para que su norma sea 1. \n",
    "            Por defecto es True.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.array, list[str], list[int], dict[str, float]]: \n",
    "            Retorna una tupla que contiene:\n",
    "            - tf_idf_matrix (np.array): Un array bidimensional donde cada fila representa un documento \n",
    "              y cada columna representa un término. Los valores en la matriz son los pesos TF-IDF \n",
    "              correspondientes.\n",
    "            - terms (list[str]): Una lista ordenada de los términos presentes en el índice invertido.\n",
    "            - docs (list[int]): Una lista ordenada de los IDs de documentos únicos en el corpus.\n",
    "            - corpus_idf (dict[str, float]): Un diccionario que mapea cada término a su valor IDF \n",
    "              calculado para el corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convierte y ordena los términos únicos en un array de NumPy.\n",
    "    terms = np.array(sorted(list(inverted_index.keys())))\n",
    "\n",
    "    # Extrae y ordena los IDs de documentos únicos en el corpus, y los convierte en un array de NumPy.\n",
    "    docs = {doc for docs_freq in inverted_index.values() for doc in docs_freq}\n",
    "    docs = np.array(sorted(list(docs)))\n",
    "\n",
    "    # Calcula el Inverse Document Frequency (IDF) para cada término y lo almacena en un diccionario.\n",
    "    # IDF = log10(N / df), donde N es el número total de documentos y df es la frecuencia de documentos \n",
    "    # que contienen el término.\n",
    "    corpus_idf = {term: np.log10(len(docs) / len(inverted_index[term])) for term in terms}\n",
    "\n",
    "    # Inicializa una matriz TF-IDF de ceros con dimensiones (número de documentos, número de términos).\n",
    "    tf_idf = np.zeros((len(docs), len(terms)))\n",
    "\n",
    "    # Llena la matriz TF-IDF iterando sobre los términos y documentos.\n",
    "    for iterm, term in enumerate(terms):\n",
    "        for idoc, doc in enumerate(docs):\n",
    "            if doc in inverted_index[term]:\n",
    "                # Calcula TF usando la escala logarítmica si está activada.\n",
    "                if tf_log_scale:\n",
    "                    tf_idf[idoc, iterm] = np.log10(1 + inverted_index[term][doc])\n",
    "                else:\n",
    "                    # Utiliza la frecuencia directa si no se aplica la escala logarítmica.\n",
    "                    tf_idf[idoc, iterm] = inverted_index[term][doc]\n",
    "                \n",
    "                # Multiplica el TF calculado por el IDF correspondiente.\n",
    "                tf_idf[idoc, iterm] *= corpus_idf[term]\n",
    "    \n",
    "    # Normaliza la matriz TF-IDF, de manera que la norma de cada vector de documento sea 1.\n",
    "    if normalize_matrix:\n",
    "        tf_idf = tf_idf / np.linalg.norm(tf_idf, axis=1, keepdims=True)\n",
    "    \n",
    "    # Retorna la matriz TF-IDF, la lista de términos, la lista de documentos, y el diccionario de IDF.\n",
    "    # Redondea a 7 decimales para evitar errores de precisión.\n",
    "    return tf_idf.round(7), terms, docs, corpus_idf\n",
    "\n",
    "\n",
    "\n",
    "def crear_vector_tf_idf(text: str,\n",
    "                        terms: list[str],\n",
    "                        corpus_idf: dict[str, float],\n",
    "                        tf_log_scale: bool = True,\n",
    "                        normalize_vector: bool = True) -> np.array:\n",
    "    \"\"\"\n",
    "    Crea un vector TF-IDF a partir de un texto dado.\n",
    "\n",
    "    Esta función toma un texto, una lista de términos y un diccionario de valores \n",
    "    IDF precomputados para un corpus, y genera un vector TF-IDF para el texto \n",
    "    proporcionado. La opción de usar escala logarítmica para la frecuencia de términos \n",
    "    (TF) y la normalización del vector resultante es configurable.\n",
    "\n",
    "    Args:\n",
    "        text (str): \n",
    "            El texto a partir del cual se generará el vector TF-IDF.\n",
    "        \n",
    "        terms (list[str]): \n",
    "            Una lista de términos que se consideran en el corpus. Cada término de la lista \n",
    "            corresponde a una columna en el vector TF-IDF.\n",
    "\n",
    "        corpus_idf (dict[str, float]): \n",
    "            Un diccionario que mapea cada término a su valor IDF (Inverse Document Frequency) \n",
    "            precomputado en el corpus. \n",
    "\n",
    "        tf_log_scale (bool, opcional): \n",
    "            Indica si se debe aplicar escala logarítmica al cálculo de la frecuencia de términos (TF).\n",
    "            Por defecto es True, lo que significa que se usará la fórmula `log10(1 + frecuencia)`. \n",
    "            Si se establece en False, se usará la frecuencia sin escala logarítmica.\n",
    "\n",
    "        normalize_vector (bool, opcional): \n",
    "            Indica si se debe normalizar el vector TF-IDF resultante. La normalización ajusta \n",
    "            el vector para que su norma sea 1, lo que facilita la comparación entre textos.\n",
    "            Por defecto es True.\n",
    "\n",
    "    Returns:\n",
    "        np.array: \n",
    "            Un array de NumPy que representa el vector TF-IDF del texto dado. Cada posición en el \n",
    "            vector corresponde a un término en la lista `terms`, y su valor es el peso TF-IDF \n",
    "            correspondiente a ese término en el texto.\n",
    "    \"\"\"    \n",
    "\n",
    "    # Procesa el texto para limpiarlo y normalizarlo, preparando los datos para el análisis.\n",
    "    # process_text fue definido en el notebook 2-BusquedaBinariaUsandoIndiceInvertido.ipynb\n",
    "    text_cln = process_text({'0': text})['0']\n",
    "\n",
    "    # Calcula el vector TF-IDF, aplicando escala logarítmica si se especifica.\n",
    "    if tf_log_scale:\n",
    "        txt_vector = np.array([\n",
    "            np.log10(1 + text_cln.count(term)) * corpus_idf[term] if term in text_cln else 0\n",
    "            for term in terms])\n",
    "    else:\n",
    "        # Si no se aplica escala logarítmica, se utiliza la frecuencia directa.\n",
    "        txt_vector = np.array([\n",
    "            text_cln.count(term) * corpus_idf[term] if term in text_cln else 0\n",
    "            for term in terms])\n",
    "\n",
    "    # Normaliza el vector TF-IDF, de manera que su norma sea 1 si la opción está activada.\n",
    "    if normalize_vector:\n",
    "        txt_vector = txt_vector / np.linalg.norm(txt_vector)\n",
    "    \n",
    "    # Redondea los valores del vector a 7 decimales para evitar errores de precisión.\n",
    "    return txt_vector.round(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de la similitud coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_simi(v1: np.array, v2: np.array, asume_norm_1: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la similitud coseno entre dos vectores.\n",
    "\n",
    "    Esta función calcula la similitud coseno entre dos vectores `v1` y `v2`. \n",
    "    La similitud coseno es una medida de la similitud entre dos vectores \n",
    "    en un espacio vectorial que mide el coseno del ángulo entre ellos. \n",
    "    Se utiliza comúnmente en análisis de texto y en tareas de recuperación de información.\n",
    "\n",
    "    Args:\n",
    "        v1 (np.array): \n",
    "            El primer vector (como un array de NumPy) con el que se calculará la similitud.\n",
    "        \n",
    "        v2 (np.array): \n",
    "            El segundo vector (como un array de NumPy) con el que se calculará la similitud.\n",
    "        \n",
    "        asume_norm_1 (bool, opcional): \n",
    "            Si se establece en True, la función asume que ambos vectores `v1` y `v2` \n",
    "            ya están normalizados (es decir, su norma es 1). Esto permite omitir el \n",
    "            cálculo de las normas, mejorando la eficiencia. Por defecto es False.\n",
    "\n",
    "    Returns:\n",
    "        float: \n",
    "            Un valor de tipo float que representa la similitud coseno entre los dos vectores.\n",
    "            Un valor cercano a 1 indica que los vectores son muy similares (paralelos), \n",
    "            mientras que un valor cercano a 0 indica que son ortogonales (no relacionados).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if asume_norm_1:\n",
    "        # Si se asume que los vectores ya están normalizados (norma = 1), \n",
    "        # la similitud coseno se reduce al producto punto entre ellos.\n",
    "        res = np.dot(v1, v2)\n",
    "    else:\n",
    "        # Si los vectores no están normalizados, se calcula la similitud coseno \n",
    "        # como el producto punto dividido por el producto de las normas de los vectores.\n",
    "        res = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    # Redondea a 4 decimales para evitar errores de precisión\n",
    "    return res.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar consultas para obtener documentos relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_docs(text: str, \n",
    "                           terms: np.array, \n",
    "                           docs: np.array, \n",
    "                           corpus_idf: dict[str, float], \n",
    "                           tf_idf_matrix: np.array, \n",
    "                           relevance_treshold: float = 0,\n",
    "                           tf_log_scale: bool = True,\n",
    "                           normalize_vector: bool = True\n",
    "                           ) -> np.array:\n",
    "    \"\"\"\n",
    "    Obtiene los documentos más relevantes en función de una consulta de texto.\n",
    "\n",
    "    Esta función toma un texto de consulta, calcula su vector TF-IDF, y luego compara \n",
    "    este vector con una matriz TF-IDF de un corpus utilizando la similitud coseno. \n",
    "    Los documentos cuya similitud coseno con la consulta es mayor que un umbral \n",
    "    especificado se consideran relevantes y se retornan en orden de relevancia.\n",
    "\n",
    "    Args:\n",
    "        text (str): \n",
    "            El texto de la consulta para el cual se desean encontrar los documentos más relevantes.\n",
    "        \n",
    "        terms (np.array[str]): \n",
    "            Un array de términos relevantes en el corpus, utilizado para construir el vector TF-IDF \n",
    "            de la consulta.\n",
    "        \n",
    "        docs (np.array[str]): \n",
    "            Un array de IDs o nombres de documentos en el corpus, donde cada documento se corresponde \n",
    "            con una fila en la matriz `tf_idf_matrix`.\n",
    "        \n",
    "        corpus_idf (dict[str, float]): \n",
    "            Un diccionario que mapea cada término a su valor IDF (Inverse Document Frequency) \n",
    "            precomputado en el corpus.\n",
    "\n",
    "        tf_idf_matrix (np.array[float]): \n",
    "            Una matriz TF-IDF donde cada fila representa un documento y cada columna representa un \n",
    "            término del corpus. Esta matriz se usa para comparar la consulta con los documentos existentes.\n",
    "\n",
    "        relevance_treshold (float, opcional): \n",
    "            Un umbral de relevancia. Solo se retornarán los documentos cuya similitud coseno con \n",
    "            la consulta sea mayor que este valor. Por defecto es 0, lo que significa que se incluirán \n",
    "            todos los documentos con una similitud positiva.\n",
    "            \n",
    "        tf_log_scale (bool, opcional):\n",
    "            Indica si se debe aplicar escala logarítmica al cálculo de la frecuencia de términos (TF).\n",
    "            Por defecto es True, lo que significa que se usará la fórmula `log10(1 + frecuencia)`. \n",
    "            Si se establece en False, se usará la frecuencia sin escala logarítmica.\n",
    "        \n",
    "        normalize_vector (bool, opcional):\n",
    "            Indica si se debe normalizar el vector TF-IDF de la consulta. La normalización ajusta \n",
    "            el vector para que su norma sea 1, lo que facilita la comparación con los vectores \n",
    "            de documentos. Por defecto es True.\n",
    "\n",
    "    Returns:\n",
    "        np.array[str]: \n",
    "            Un array de IDs o nombres de documentos ordenados por relevancia, desde el más relevante \n",
    "            hasta el menos relevante, según la similitud coseno con la consulta.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear el vector TF-IDF para la consulta de texto dada, utilizando el corpus y la lista de términos.\n",
    "    v_query = crear_vector_tf_idf(text, terms, corpus_idf, \n",
    "                                  tf_log_scale=tf_log_scale, \n",
    "                                  normalize_vector=normalize_vector)\n",
    "\n",
    "    # Calcular la similitud coseno entre el vector de la consulta y la matriz TF-IDF del corpus.\n",
    "    # Se asume que los vectores en la matriz TF-IDF ya están normalizados.\n",
    "    cosine_similarities = cosine_simi(v_query, tf_idf_matrix.T, asume_norm_1=True)\n",
    "\n",
    "    # Identificar los índices de los documentos cuya similitud coseno es mayor que el umbral de relevancia.\n",
    "    indices = np.where(cosine_similarities > relevance_treshold)[0]\n",
    "\n",
    "    # Ordenar los índices de los documentos por similitud coseno en orden descendente.\n",
    "    sorted_indices = indices[np.argsort(-cosine_similarities[indices])]\n",
    "\n",
    "    # Obtener los documentos correspondientes a los índices ordenados por relevancia.\n",
    "    sorted_docs = [(docs[i], cosine_similarities[i]) for i in sorted_indices]\n",
    "    \n",
    "    return sorted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear matriz tf-idf a partir del índice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = json.loads(open('./output/inverted_index.json').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La creación de la matriz TF-IDF a partir del índice invertido puede ser un proceso costoso en términos de tiempo, ya que requiere iterar sobre todos los documentos para calcular las frecuencias asociadas a cada término. Este paso es crucial porque, al generar los vectores TF-IDF para los documentos, es necesario que la representación tenga una dimensión consistente con el vocabulario completo, asegurando que las posiciones de los términos sean coherentes en todos los documentos. Esto permite que la matriz TF-IDF tenga los documentos como filas y los términos como columnas, facilitando su uso en análisis posteriores.\n",
    "\n",
    "Una mejora que podría incrementar la organización y modularidad de la implementación sería estructurar el vectorizador como una clase. Esta clase podría almacenar en sus atributos los términos, los documentos, la matriz TF-IDF y otros valores relevantes para cálculos futuros. Además, se podrían definir métodos específicos para el cálculo de los vectores TF-IDF y la similitud coseno. Este enfoque no solo evitaría la repetición de parámetros y el 'drilling' en las funciones independientes, sino que también garantizaría una mayor consistencia y facilidad de mantenimiento.\n",
    "\n",
    "Finalmente, aunque esta función se ejecuta solo una vez al inicio y sobre un corpus relativamente pequeño, el tiempo de ejecución no es crítico. Sin embargo, para mejorar la eficiencia y escalabilidad de la solución, sería recomendable implementar estas mejoras en el diseño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix, terms, docs, corpus_idf = crear_tf_idf_matrix(inverted_index, \n",
    "                                                             tf_log_scale=True,\n",
    "                                                             normalize_matrix=True\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/processed_queries.json') as f:\n",
    "    processed_queries = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_res = {}\n",
    "with open(\"./output/RRDV/RRDV-consultas_resultados.tsv\", \"w\") as f:\n",
    "    for q,t in processed_queries.items():\n",
    "        \n",
    "        rel_docs = get_most_relevant_docs(\n",
    "                                ' '.join(t), \n",
    "                                terms, \n",
    "                                docs, \n",
    "                                corpus_idf, \n",
    "                                tf_idf_matrix, \n",
    "                                relevance_treshold=0,\n",
    "                                tf_log_scale=True,\n",
    "                                normalize_vector=True\n",
    "                                )\n",
    "        \n",
    "        queries_res[q] = rel_docs\n",
    "        rel_docs = ','.join([f'{d[0]}:{d[1]}' for d in rel_docs])\n",
    "        \n",
    "        f.write(f\"{q}\\t{rel_docs}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular metricas de evaluacion\n",
    "file_path = 'data/relevance-judgments.tsv'\n",
    "\n",
    "queries = {}\n",
    "\n",
    "# Abrir el archivo de relevance-judgments\n",
    "with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    \n",
    "    for row in reader:\n",
    "        # la primera columna es el id del query\n",
    "        query_id = row[0]\n",
    "        \n",
    "        # Extraer el id de los documentos relevantes y su score\n",
    "        document_scores = row[1].split(',')\n",
    "        \n",
    "        documents = {}\n",
    "        \n",
    "        for document_score in document_scores:\n",
    "            # Obtener el id del documento y su puntaje\n",
    "            document_id, score = document_score.split(':')\n",
    "            \n",
    "            score = int(score)\n",
    "            documents[document_id] = score\n",
    "        \n",
    "        queries[query_id] = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos los vectores de relevancia binaria para cada query\n",
    "relevances = {}\n",
    "\n",
    "for query, documents in queries.items():\n",
    "    binary_array = []\n",
    "    for d in queries_res[query]:\n",
    "        binary_array.append(1 if d[0] in documents else 0)\n",
    "    relevances[query] = binary_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P@M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'output/RRDV/P@M.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for query, documents in queries.items():\n",
    "        precision = precision_at_k(relevances[query], len(documents))\n",
    "        file.write(f\"{query}: {precision}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R@M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'output/RRDV/R@M.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for query, documents in queries.items():\n",
    "        recall = recall_at_k(relevances[query], len(documents), len(documents))\n",
    "        file.write(f\"{query}: {recall}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7402507734672369"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map = mean_average_precision(list(relevances.values()))\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NDCG@M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos los vectores de relevancia para cada query\n",
    "relevances_ranked = {}\n",
    "\n",
    "for query, documents in queries.items():\n",
    "    binary_array = []\n",
    "    for d in queries_res[query]:\n",
    "        binary_array.append(documents.get(d[0],0))\n",
    "    relevances_ranked[query] = binary_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'output/RRDV/NDCG@M.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for query, documents in queries.items():\n",
    "        ndcg = ndcg_at_k(relevances_ranked[query], len(documents))\n",
    "        file.write(f\"{query}: {ndcg}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
