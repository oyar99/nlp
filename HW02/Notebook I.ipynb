{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8afa2b-90f9-40ee-b1c7-9dcddefba398",
   "metadata": {},
   "source": [
    "# Tarea 2 – ISIS 4221, Notebook I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33740950-a73e-4aaa-91ac-0ee480a992b4",
   "metadata": {},
   "source": [
    "## Paso 0: Importación de Librerías Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cfe6f-6952-42aa-a56c-ca8c15137b4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.12.3' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from re import compile, DOTALL, findall\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pandas import DataFrame, Series\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475ae5f2-3105-4863-8f29-ef36fcbb9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define los directorios que se deben crear\n",
    "ngram_models_dir = './data/ngram_models/'\n",
    "train_test_dir = './data/train_test/'\n",
    "\n",
    "# Crea los directorios si no existen\n",
    "os.makedirs(ngram_models_dir, exist_ok=True)\n",
    "os.makedirs(train_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc636ae-4da0-4107-a3d1-8be464466306",
   "metadata": {},
   "source": [
    "## Paso 1: Definición de las Funciones Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53e3286-1636-4c0a-bf3e-29050ce08a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee el contenido de un archivo y maneja posibles errores de codificación.\n",
    "\n",
    "    Args:\n",
    "        f (str): La ruta al archivo que se desea leer.\n",
    "\n",
    "    Returns:\n",
    "        str: El contenido del archivo como una cadena de texto.\n",
    "\n",
    "    Notas:\n",
    "        - La función intenta leer el archivo con codificación 'utf-8' por defecto.\n",
    "        - Si ocurre un error de decodificación (UnicodeDecodeError), se reintenta con la codificación 'latin1'.\n",
    "        - Este enfoque es útil cuando se manejan archivos de texto con codificaciones mixtas o desconocidas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f, 'r', encoding='utf-8') as file:\n",
    "            txt = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(f, 'r', encoding='latin1') as file:\n",
    "            txt = file.read()\n",
    "    \n",
    "    return txt\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando correos electrónicos, etiquetas HTML, caracteres no alfanuméricos,\n",
    "    y otros elementos no deseados.\n",
    "\n",
    "    Args:\n",
    "        txt (str): El texto que se desea limpiar.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto limpio y normalizado.\n",
    "\n",
    "    Pasos:\n",
    "        - Convertir el texto a minúsculas para normalizarlo.\n",
    "        - Eliminar correos electrónicos, etiquetas HTML, y ciertas palabras clave usando expresiones regulares.\n",
    "        - Reemplazar secuencias de puntos múltiples con un solo espacio.\n",
    "        - Eliminar repeticiones de caracteres consecutivos (más de dos veces) que no sean necesarias.\n",
    "        - Reemplazar guiones por espacios para evitar palabras concatenadas por guiones.\n",
    "        - Sustituir signos de puntuación específicos (?, !, :) por un punto.\n",
    "        - Eliminar caracteres no alfanuméricos, excepto puntos, espacios y saltos de línea.\n",
    "        - Sustituir números por la palabra 'NUM' para normalizar las secuencias numéricas.\n",
    "        - Reemplazar múltiples saltos de línea consecutivos con un solo punto.\n",
    "        - Convertir saltos de línea individuales en espacios.\n",
    "        - Reemplazar múltiples espacios consecutivos por un solo espacio.\n",
    "\n",
    "    Notas:\n",
    "        - El uso de expresiones regulares precompiladas mejora la eficiencia al aplicar la limpieza a múltiples textos.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compila las expresiones regulares fuera de la función para mejorar la eficiencia\n",
    "    email_re = compile(r'\\S*@\\S*\\s?|from: |re: |subject: |urllink|maxaxaxaxaxaxaxaxaxaxaxaxaxaxax')\n",
    "    punctuation_re = compile(r'[?!:]')\n",
    "    non_alphanumeric_re = compile(r'[^A-Za-z0-9. \\n]')\n",
    "    numbers_re = compile(r'\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b')\n",
    "    multiple_newlines_re = compile(r'\\n{2,}')\n",
    "    single_newline_re = compile(r'\\n')\n",
    "    multiple_dots_re = compile(r'\\.\\.+')\n",
    "    multiple_spaces_re = compile(r'\\s+')\n",
    "    multiple_char_repetition_re = compile(r'(.)\\1{2,}')\n",
    "    \n",
    "    # Convierte el texto a minúsculas\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # Elimina correos electrónicos, etiquetas dentro de <>, y ciertas palabras clave\n",
    "    txt = email_re.sub('', txt)\n",
    "\n",
    "    # Reemplaza secuencias de puntos múltiples con un solo espacio\n",
    "    txt = multiple_dots_re.sub(' ', txt)\n",
    "\n",
    "    # Elimina repeticiones de caracteres consecutivos (más de dos veces)\n",
    "    txt = multiple_char_repetition_re.sub(r'\\1', txt)\n",
    "\n",
    "    # Reemplaza guiones por espacios\n",
    "    txt = txt.replace('-', ' ')\n",
    "\n",
    "    # Sustituye signos de puntuación específicos por un punto\n",
    "    txt = punctuation_re.sub('.', txt)\n",
    "\n",
    "    # Elimina caracteres no alfanuméricos excepto puntos, espacios y saltos de línea\n",
    "    txt = non_alphanumeric_re.sub('', txt)\n",
    "\n",
    "    # Sustituye números por 'NUM'\n",
    "    txt = numbers_re.sub('NUM', txt)\n",
    "\n",
    "    # Reemplaza múltiples saltos de línea consecutivos por un solo punto\n",
    "    txt = multiple_newlines_re.sub('.', txt)\n",
    "\n",
    "    # Convierte saltos de línea individuales en espacios\n",
    "    txt = single_newline_re.sub(' ', txt)\n",
    "\n",
    "    # Reemplaza múltiples espacios consecutivos por un solo espacio\n",
    "    txt = multiple_spaces_re.sub(' ', txt)\n",
    "\n",
    "    # Elimina espacios en blanco al inicio y final del texto\n",
    "    return txt.strip()\n",
    "\n",
    "def extract_and_process_text_from_xml(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Extrae y procesa el texto contenido entre etiquetas <post> en un archivo XML.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): La ruta al archivo XML del cual se extraerá el texto.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de diccionarios, donde cada diccionario representa una oración extraída y procesada\n",
    "              con las claves:\n",
    "              - 'text': La oración procesada.\n",
    "              - 'length': La longitud de la oración en número de palabras.\n",
    "\n",
    "    Notas:\n",
    "        - La función primero lee el archivo XML utilizando `read_file`.\n",
    "        - Luego extrae el contenido de todas las etiquetas <post> usando expresiones regulares.\n",
    "        - Cada fragmento de texto extraído se limpia utilizando la función `clean_text`.\n",
    "        - El texto limpio se divide en oraciones, que se formatean y almacenan en una lista de diccionarios.\n",
    "        - Cada oración es rodeada por etiquetas <s> y </s> para indicar su inicio y fin.\n",
    "        - Las oraciones que contienen una sola palabra o están vacías se descartan.\n",
    "    \"\"\"\n",
    "    # Lee el contenido del archivo XML\n",
    "    xml_content = read_file(file_path)\n",
    "    \n",
    "    # Extrae el contenido entre las etiquetas <post> y </post> utilizando expresiones regulares\n",
    "    post_matches = findall(r'<post>(.*?)</post>', xml_content, DOTALL)\n",
    "    \n",
    "    df_rows = []\n",
    "    \n",
    "    for post in post_matches:\n",
    "        # Limpia el texto extraído utilizando la función clean_text\n",
    "        cleaned_post = clean_text(post.strip())\n",
    "        \n",
    "        # Divide el texto limpio en oraciones\n",
    "        sentences = [f'<s> {s.strip()} </s>' for s in sent_tokenize(cleaned_post) if len(s.strip().split()) > 1]\n",
    "        \n",
    "        # Crea un diccionario para cada oración con su texto y longitud\n",
    "        df_rows.extend([{\n",
    "            'text': s,\n",
    "            'source': file_path,\n",
    "            'length': len(s.split())\n",
    "        } for s in sentences])\n",
    "\n",
    "    return df_rows\n",
    "\n",
    "def create_ngrams(sentence: str, n: int, unique_tokens: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Genera n-gramas a partir de una oración dada.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): La oración de entrada de la cual se generarán los n-gramas.\n",
    "        n (int): La longitud de los n-gramas a generar.\n",
    "        unique_tokens (dict, opcional): Un diccionario que mapea ciertos tokens a un valor único,\n",
    "            típicamente utilizado para reemplazar tokens poco frecuentes por un marcador como '<UNK>'.\n",
    "            Por defecto es None.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de n-gramas, donde cada n-grama se representa como una tupla de palabras.\n",
    "\n",
    "    Notas:\n",
    "        - Si se proporciona `unique_tokens`, cualquier palabra en la oración que aparezca en \n",
    "          `unique_tokens` será reemplazada según el mapeo en `unique_tokens`.\n",
    "        - La función devuelve n-gramas en forma de un generador para ahorrar memoria, \n",
    "          especialmente útil al procesar grandes corpus.\n",
    "    \"\"\"\n",
    "    words = sentence.split()  # Divide la oración en palabras\n",
    "    \n",
    "    if unique_tokens:\n",
    "        # Genera n-gramas usando un generador para reemplazar las palabras según el diccionario unique_tokens\n",
    "        return (tuple([unique_tokens.get((w,), w) for w in words[i:i+n]]) \n",
    "                for i in range(len(words) - n + 1))\n",
    "    \n",
    "    # Genera los n-gramas usando un generador sin realizar reemplazos\n",
    "    return (tuple(words[i:i+n]) for i in range(len(words) - n + 1) if len(words[i:i+n]) == n)\n",
    "\n",
    "def create_ngram_model(n_gram: int, text_corpus: Series)-> tuple[Counter[int], defaultdict[int]]:\n",
    "    \"\"\"\n",
    "    Crea un modelo de n-gramas a partir de un corpus de texto.\n",
    "\n",
    "    Args:\n",
    "        n_gram (int): El tamaño de los n-gramas a generar.\n",
    "        text_corpus (pd.Series): Una serie de Pandas que contiene el corpus de texto, \n",
    "                                 donde cada entrada es una oración o texto.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dos elementos:\n",
    "            - ngram_counts (Counter): Un contador que almacena las frecuencias de los n-gramas en el corpus.\n",
    "            - final_unigram (defaultdict): Un diccionario con los (n-1)-gramas más frecuentes y su conteo, \n",
    "                                           con un marcador especial `<UNK>` para los menos frecuentes.\n",
    "\n",
    "    Notas:\n",
    "        - La función primero cuenta los (n-1)-gramas para identificar los tokens únicos que serán \n",
    "          reemplazados por `<UNK>`.\n",
    "        - Luego, se cuentan los n-gramas completos utilizando los reemplazos identificados.\n",
    "    \"\"\"    \n",
    "\n",
    "    # Cuenta las frecuencias de (n-1)-gramas en el corpus de texto\n",
    "    unigram_counts = Counter(chain.from_iterable(\n",
    "        text_corpus.apply(lambda x: create_ngrams(x, n=n_gram-1, unique_tokens=None))\n",
    "    ))\n",
    "\n",
    "    final_unigram = defaultdict(int)\n",
    "    unique_tokens = defaultdict(int)\n",
    "\n",
    "    # Identifica tokens únicos y construir el diccionario final de 'unigrama'\n",
    "    for ngram, count in unigram_counts.items():\n",
    "        if '' in ngram:\n",
    "            continue\n",
    "        if count < 2:\n",
    "            unique_tokens[ngram] = ('<UNK>',) * (n_gram-1)\n",
    "        else:\n",
    "            final_unigram[ngram] = count\n",
    "\n",
    "    final_unigram[('<UNK>',)*(n_gram-1)] = len(unique_tokens)\n",
    "\n",
    "    # Cuenta las frecuencias de los n-gramas completos usando los reemplazos identificados\n",
    "    ngram_counts = Counter(chain.from_iterable(\n",
    "        text_corpus.apply(lambda x: create_ngrams(x, n=n_gram, unique_tokens=unique_tokens))\n",
    "    ))\n",
    "\n",
    "    return ngram_counts, final_unigram\n",
    "\n",
    "def save_ngram_model(ngram_counts, final_unigram, model_name):\n",
    "    \"\"\"\n",
    "    Guarda el modelo de n-gramas en un archivo JSON.\n",
    "\n",
    "    Args:\n",
    "        ngram_counts (Counter): Los n-gramas y sus conteos.\n",
    "        model_name (str): Nombre del archivo donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    with open(f'./data/ngram_models/{model_name}.json', 'w') as f:\n",
    "        json.dump({ \n",
    "            'ngram_counts': { ' '.join(str(w) for w in k): v for k, v in ngram_counts.items() },\n",
    "            'final_unigram': { ' '.join(str(w) for w in k): v for k, v in final_unigram.items() }\n",
    "            }, f)\n",
    "\n",
    "    print(f\"{model_name} saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e93c22-aa58-405b-9ac8-1ffb67ff1ab5",
   "metadata": {},
   "source": [
    "## Paso 2: Procesamiento y Almacenamiento de 20 Newsgroups (20N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae385720-17cc-4d1e-b78c-b1d4a372cda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de archivos encontrados: 18828\n"
     ]
    }
   ],
   "source": [
    "# Obtiene una lista de archivos en el directorio './raw_data/20news-18828/' que contiene subdirectorios para cada newsgroup\n",
    "# NOTA: Actualizar segun donde se ubique el dataset\n",
    "files_20n = glob('./raw_data/20news-18828/*/*')\n",
    "\n",
    "# Cuenta el número total de archivos encontrados\n",
    "print(f\"Número total de archivos encontrados: {len(files_20n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb8ea45-4fa1-468e-9e23-f86a65307118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18828/18828 [04:24<00:00, 71.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt; mathew alt.atheism faq. &lt;/s&gt;</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt; atheist resources.archive name. &lt;/s&gt;</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt; atheismresources alt atheism archive name....</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt; resources last modified. &lt;/s&gt;</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt; NUM december 1992 version. &lt;/s&gt;</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;s&gt; atheist resources. &lt;/s&gt;</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;s&gt; addresses of atheist organizations. &lt;/s&gt;</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;s&gt; usa.freedom from religion foundation.darwi...</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;s&gt; ffrf p.o. &lt;/s&gt;</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;s&gt; box NUM madison wi 53701. telephone. &lt;/s&gt;</td>\n",
       "      <td>./raw_data/20news-18828\\alt.atheism\\49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                   <s> mathew alt.atheism faq. </s>   \n",
       "1           <s> atheist resources.archive name. </s>   \n",
       "2  <s> atheismresources alt atheism archive name....   \n",
       "3                  <s> resources last modified. </s>   \n",
       "4                <s> NUM december 1992 version. </s>   \n",
       "5                        <s> atheist resources. </s>   \n",
       "6       <s> addresses of atheist organizations. </s>   \n",
       "7  <s> usa.freedom from religion foundation.darwi...   \n",
       "8                                 <s> ffrf p.o. </s>   \n",
       "9      <s> box NUM madison wi 53701. telephone. </s>   \n",
       "\n",
       "                                      source     category  length  \n",
       "0  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       5  \n",
       "1  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       5  \n",
       "2  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       7  \n",
       "3  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       5  \n",
       "4  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       6  \n",
       "5  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       4  \n",
       "6  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       6  \n",
       "7  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism      26  \n",
       "8  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       4  \n",
       "9  ./raw_data/20news-18828\\alt.atheism\\49960  alt.atheism       8  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa una lista para almacenar las filas procesadas\n",
    "df_news_rows = []\n",
    "\n",
    "# Procesa cada archivo encontrado\n",
    "for f in tqdm(files_20n):\n",
    "    # Extrae la categoría (nombre del subdirectorio) a partir de la ruta del archivo\n",
    "    category = os.path.basename(os.path.dirname(f))\n",
    "    \n",
    "    # Lee el contenido del archivo\n",
    "    txt = read_file(f)\n",
    "    \n",
    "    # Limpia el texto leído\n",
    "    txt_cln = clean_text(txt)\n",
    "    \n",
    "    # Divide el texto limpio en oraciones, añadiendo etiquetas de inicio y fin de oración\n",
    "    sentences = [f'<s> {s.strip()} </s>' for s in sent_tokenize(txt_cln) if len(s.strip().split()) > 1]\n",
    "    \n",
    "    # Crea un diccionario para cada oración con el texto, la fuente del archivo, la categoría y la longitud de la oración\n",
    "    df_news_rows.extend([{\n",
    "        'text': s,\n",
    "        'source': f,\n",
    "        'category': category,  # Añade la categoría como una columna adicional\n",
    "        'length': len(s.split())\n",
    "    } for s in sentences])\n",
    "\n",
    "# Crea un DataFrame a partir de las filas procesadas\n",
    "df_news = DataFrame(df_news_rows)\n",
    "\n",
    "# Guarda el DataFrame en un archivo Parquet para su almacenamiento eficiente\n",
    "df_news.to_parquet('./data/20news.parquet', index=False)\n",
    "\n",
    "df_news.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ea9594-1f67-4c27-8398-5af58ff96762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide aleatoriamente el DataFrame en conjuntos de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "df_news_train = df_news.sample(frac=0.8, random_state=42)\n",
    "df_news_test = df_news.drop(df_news_train.index)\n",
    "\n",
    "# Guarda los conjuntos de entrenamiento y prueba en archivos Parquet separados\n",
    "df_news_train.to_parquet('./data/train_test/20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_training.parquet', index=False)\n",
    "df_news_test.to_parquet('./data/train_test/20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_testing.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894c3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera los modelos de N-gramas\n",
    "unigram_counts_20n, unigram_20n = create_ngram_model(1, df_news['text'])\n",
    "bigram_counts_20n, bigram_20n = create_ngram_model(2, df_news['text'])\n",
    "trigram_counts_20n, trigram_20n = create_ngram_model(3, df_news['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f9f704d-5acf-47f1-8f6d-95d7031e66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_unigrams saved.\n",
      "20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_bigrams saved.\n",
      "20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_trigrams saved.\n"
     ]
    }
   ],
   "source": [
    "# Guarda los modelos de N-gramas\n",
    "save_ngram_model(unigram_counts_20n, unigram_20n, '20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_unigrams')\n",
    "save_ngram_model(bigram_counts_20n, bigram_20n, '20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_bigrams')\n",
    "save_ngram_model(trigram_counts_20n, trigram_20n, '20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1643a7-bb89-4b96-bdd0-c60da1cd10ef",
   "metadata": {},
   "source": [
    "## Paso 3: Procesamiento y Almacenamiento de Blog Authorship Corpus (BAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e11e87-f882-4716-a270-5210016184a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19320"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtiene una lista de archivos XML en el directorio './raw_data/blogs/'\n",
    "# Nota: Actualizar ruta segun corresponda\n",
    "files_bac = glob('./raw_data/blogs/*')\n",
    "\n",
    "# Cuenta el número total de archivos encontrados\n",
    "len(files_bac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb2c7f4-5fe5-4c5b-88d0-432793267173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19320/19320 [13:32<00:00, 23.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  <s> well everyone got up and going this mornin...   \n",
      "1  <s> its still raining but thats okay with me. ...   \n",
      "2                    <s> sort of suits my mood. </s>   \n",
      "3  <s> i could easily have stayed home in bed wit...   \n",
      "4       <s> this has been a lot of rain though. </s>   \n",
      "\n",
      "                                              source  length  \n",
      "0  ./raw_data/blogs\\1000331.female.37.indUnk.Leo.xml      10  \n",
      "1  ./raw_data/blogs\\1000331.female.37.indUnk.Leo.xml      10  \n",
      "2  ./raw_data/blogs\\1000331.female.37.indUnk.Leo.xml       7  \n",
      "3  ./raw_data/blogs\\1000331.female.37.indUnk.Leo.xml      16  \n",
      "4  ./raw_data/blogs\\1000331.female.37.indUnk.Leo.xml      10  \n"
     ]
    }
   ],
   "source": [
    "# Inicializa una lista para almacenar las filas procesadas\n",
    "df_bac_rows = []\n",
    "\n",
    "# Procesa cada archivo XML encontrado\n",
    "for f in tqdm(files_bac):\n",
    "    # Extraer y procesar el texto del archivo XML, y agregar las filas al DataFrame\n",
    "    df_bac_rows.extend(extract_and_process_text_from_xml(f))\n",
    "\n",
    "# Crea un DataFrame a partir de las filas procesadas\n",
    "df_bac = DataFrame(df_bac_rows)\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "print(df_bac.head())\n",
    "\n",
    "# Guarda el DataFrame en un archivo Parquet para su almacenamiento eficiente\n",
    "df_bac.to_parquet('./data/bac.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08ec9c4e-ecd7-453d-afc0-70724100a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide el DataFrame en conjuntos de entrenamiento (80%) y prueba (20%) de manera aleatoria\n",
    "df_bac_train = df_bac.sample(frac=0.8, random_state=42)\n",
    "df_bac_test = df_bac.drop(df_bac_train.index)\n",
    "\n",
    "# Guarda los conjuntos de entrenamiento y prueba en archivos Parquet separados\n",
    "df_bac_train.to_parquet('./data/train_test/BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_training.parquet', index=False)\n",
    "df_bac_test.to_parquet('./data/train_test/BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_testing.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7abcf0d-a688-4479-a74e-ab5d59991938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera los modelos de N-gramas\n",
    "unigram_counts_bac, unigram_bac = create_ngram_model(1, df_bac['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51210499",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts_bac, bigram_bac = create_ngram_model(2, df_bac['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3dd4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_counts_bac, trigram_bac = create_ngram_model(3, df_bac['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3322e3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_unigrams saved.\n",
      "BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_bigrams saved.\n",
      "BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_trigrams saved.\n"
     ]
    }
   ],
   "source": [
    "# Guarda los modelos de N-gramas\n",
    "save_ngram_model(unigram_counts_bac, unigram_bac, 'BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_unigrams')\n",
    "save_ngram_model(bigram_counts_bac, bigram_bac, 'BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_bigrams')\n",
    "save_ngram_model(trigram_counts_bac, trigram_bac, 'BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_trigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293b7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
