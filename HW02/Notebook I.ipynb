{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8afa2b-90f9-40ee-b1c7-9dcddefba398",
   "metadata": {},
   "source": [
    "# Tarea 2 – ISIS 4221, Notebook I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33740950-a73e-4aaa-91ac-0ee480a992b4",
   "metadata": {},
   "source": [
    "## Paso 0: Importación de Librerías Necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbd441-6d22-4918-8d72-bb01b68b9be4",
   "metadata": {},
   "source": [
    "<span style=\"color: gray;\">Añadí import statements para `nltk`.</span>\n",
    "\n",
    "<span style=\"color: gray;\">Añadí código para crear los directorios necesarios, si no existen.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cfe6f-6952-42aa-a56c-ca8c15137b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from importlib import import_module\n",
    "from itertools import chain\n",
    "from re import compile, DOTALL, findall, split\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk   # Para tokenizar mejor\n",
    "from nltk.tokenize import sent_tokenize   # Para tokenizar mejor\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')   # Para usar con nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ae5f2-3105-4863-8f29-ef36fcbb9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define los directorios que se deben crear\n",
    "ngram_models_dir = './data/ngram_models/'\n",
    "train_test_dir = './data/train_test/'\n",
    "\n",
    "# Crea los directorios si no existen\n",
    "os.makedirs(ngram_models_dir, exist_ok=True)\n",
    "os.makedirs(train_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc636ae-4da0-4107-a3d1-8be464466306",
   "metadata": {},
   "source": [
    "## Paso 1: Definición de las Funciones Necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73b70b-7e3c-48ea-a61a-6be4adb0d799",
   "metadata": {},
   "source": [
    "<span style=\"color: gray;\">Sugerí crear `sentences` utilizando el tokenizador de oraciones de `nltk` para una mejor precisión (por ejemplo, para capturar signos de puntuación como \"!\" o \"?\" y no separar erróneamente oraciones como \"e.g.\".</span>\n",
    "\n",
    "<span style=\"color: gray;\">Sugerí usar una expresión regular diferentes con `numbers_re` para manejar números con comas (por ejemplo, 1,000) y decimales (por ejemplo, 1.50).</span>\n",
    "\n",
    "<span style=\"color: gray;\">Cambié el signo `==` por el signo `>=` en el generador que devuelve la función `create_ngrams` para garantizar que las oraciones tengan suficientes palabras (i.e., si la longitud de la oración es menor a `n`, no se generarán n-gramas).</span>\n",
    "\n",
    "<span style=\"color: gray;\">Añadí la función `save_ngram_model`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e3286-1636-4c0a-bf3e-29050ce08a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee el contenido de un archivo y maneja posibles errores de codificación.\n",
    "\n",
    "    Args:\n",
    "        f (str): La ruta al archivo que se desea leer.\n",
    "\n",
    "    Returns:\n",
    "        str: El contenido del archivo como una cadena de texto.\n",
    "\n",
    "    Notas:\n",
    "        - La función intenta leer el archivo con codificación 'utf-8' por defecto.\n",
    "        - Si ocurre un error de decodificación (UnicodeDecodeError), se reintenta con la codificación 'latin1'.\n",
    "        - Este enfoque es útil cuando se manejan archivos de texto con codificaciones mixtas o desconocidas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f, 'r', encoding='utf-8') as file:\n",
    "            txt = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(f, 'r', encoding='latin1') as file:\n",
    "            txt = file.read()\n",
    "    \n",
    "    return txt\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando correos electrónicos, etiquetas HTML, caracteres no alfanuméricos,\n",
    "    y otros elementos no deseados.\n",
    "\n",
    "    Args:\n",
    "        txt (str): El texto que se desea limpiar.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto limpio y normalizado.\n",
    "\n",
    "    Pasos:\n",
    "        - Convertir el texto a minúsculas para normalizarlo.\n",
    "        - Eliminar correos electrónicos, etiquetas HTML, y ciertas palabras clave usando expresiones regulares.\n",
    "        - Reemplazar secuencias de puntos múltiples con un solo espacio.\n",
    "        - Eliminar repeticiones de caracteres consecutivos (más de dos veces) que no sean necesarias.\n",
    "        - Reemplazar guiones por espacios para evitar palabras concatenadas por guiones.\n",
    "        - Sustituir signos de puntuación específicos (?, !, :) por un punto.\n",
    "        - Eliminar caracteres no alfanuméricos, excepto puntos, espacios y saltos de línea.\n",
    "        - Sustituir números por la palabra 'NUM' para normalizar las secuencias numéricas.\n",
    "        - Reemplazar múltiples saltos de línea consecutivos con un solo punto.\n",
    "        - Convertir saltos de línea individuales en espacios.\n",
    "        - Reemplazar múltiples espacios consecutivos por un solo espacio.\n",
    "\n",
    "    Notas:\n",
    "        - El uso de expresiones regulares precompiladas mejora la eficiencia al aplicar la limpieza a múltiples textos.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compila las expresiones regulares fuera de la función para mejorar la eficiencia\n",
    "    email_re = compile(r'\\S*@\\S*\\s?|from: |re: |subject: |urllink|maxaxaxaxaxaxaxaxaxaxaxaxaxaxax')\n",
    "    punctuation_re = compile(r'[?!:]')\n",
    "    non_alphanumeric_re = compile(r'[^A-Za-z0-9. \\n]')\n",
    "    # Línea original de Raúl: numbers_re = compile(r'\\b\\d+\\b')\n",
    "    # Mi sugerencia:\n",
    "    numbers_re = compile(r'\\b\\d{1,3}(,\\d{3})*(\\.\\d+)?\\b')\n",
    "    multiple_newlines_re = compile(r'\\n{2,}')\n",
    "    single_newline_re = compile(r'\\n')\n",
    "    multiple_dots_re = compile(r'\\.\\.+')\n",
    "    multiple_spaces_re = compile(r'\\s+')\n",
    "    multiple_char_repetition_re = compile(r'(.)\\1{2,}')\n",
    "    \n",
    "    # Convierte el texto a minúsculas\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # Elimina correos electrónicos, etiquetas dentro de <>, y ciertas palabras clave\n",
    "    txt = email_re.sub('', txt)\n",
    "\n",
    "    # Reemplaza secuencias de puntos múltiples con un solo espacio\n",
    "    txt = multiple_dots_re.sub(' ', txt)\n",
    "\n",
    "    # Elimina repeticiones de caracteres consecutivos (más de dos veces)\n",
    "    txt = multiple_char_repetition_re.sub(r'\\1', txt)\n",
    "\n",
    "    # Reemplaza guiones por espacios\n",
    "    txt = txt.replace('-', ' ')\n",
    "\n",
    "    # Sustituye signos de puntuación específicos por un punto\n",
    "    txt = punctuation_re.sub('.', txt)\n",
    "\n",
    "    # Elimina caracteres no alfanuméricos excepto puntos, espacios y saltos de línea\n",
    "    txt = non_alphanumeric_re.sub('', txt)\n",
    "\n",
    "    # Sustituye números por 'NUM'\n",
    "    txt = numbers_re.sub('NUM', txt)\n",
    "\n",
    "    # Reemplaza múltiples saltos de línea consecutivos por un solo punto\n",
    "    txt = multiple_newlines_re.sub('.', txt)\n",
    "\n",
    "    # Convierte saltos de línea individuales en espacios\n",
    "    txt = single_newline_re.sub(' ', txt)\n",
    "\n",
    "    # Reemplaza múltiples espacios consecutivos por un solo espacio\n",
    "    txt = multiple_spaces_re.sub(' ', txt)\n",
    "\n",
    "    # Elimina espacios en blanco al inicio y final del texto\n",
    "    return txt.strip()\n",
    "\n",
    "def extract_and_process_text_from_xml(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Extrae y procesa el texto contenido entre etiquetas <post> en un archivo XML.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): La ruta al archivo XML del cual se extraerá el texto.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de diccionarios, donde cada diccionario representa una oración extraída y procesada\n",
    "              con las claves:\n",
    "              - 'text': La oración procesada.\n",
    "              - 'length': La longitud de la oración en número de palabras.\n",
    "\n",
    "    Notas:\n",
    "        - La función primero lee el archivo XML utilizando `read_file`.\n",
    "        - Luego extrae el contenido de todas las etiquetas <post> usando expresiones regulares.\n",
    "        - Cada fragmento de texto extraído se limpia utilizando la función `clean_text`.\n",
    "        - El texto limpio se divide en oraciones, que se formatean y almacenan en una lista de diccionarios.\n",
    "        - Cada oración es rodeada por etiquetas <s> y </s> para indicar su inicio y fin.\n",
    "        - Las oraciones que contienen una sola palabra o están vacías se descartan.\n",
    "    \"\"\"\n",
    "    # Lee el contenido del archivo XML\n",
    "    xml_content = read_file(file_path)\n",
    "    \n",
    "    # Extrae el contenido entre las etiquetas <post> y </post> utilizando expresiones regulares\n",
    "    post_matches = findall(r'<post>(.*?)</post>', xml_content, DOTALL)\n",
    "    \n",
    "    df_rows = []\n",
    "    \n",
    "    for post in post_matches:\n",
    "        # Limpia el texto extraído utilizando la función clean_text\n",
    "        cleaned_post = clean_text(post.strip())\n",
    "        \n",
    "        # Divide el texto limpio en oraciones\n",
    "        # Línea original de Raúl: sentences = [f'<s> {s.strip()} </s>' for s in split(r'\\.\\s*', cleaned_post) if len(s.strip().split()) > 1]\n",
    "        # Mi sugerencia:\n",
    "        sentences = [f'<s> {s.strip()} </s>' for s in sent_tokenize(cleaned_post) if len(s.strip().split()) > 1]\n",
    "        \n",
    "        # Crea un diccionario para cada oración con su texto y longitud\n",
    "        df_rows.extend([{\n",
    "            'text': s,\n",
    "            'source': file_path,\n",
    "            'length': len(s.split())\n",
    "        } for s in sentences])\n",
    "\n",
    "    return df_rows\n",
    "\n",
    "def create_ngrams(sentence: str, n: int, unique_tokens: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Genera n-gramas a partir de una oración dada.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): La oración de entrada de la cual se generarán los n-gramas.\n",
    "        n (int): La longitud de los n-gramas a generar.\n",
    "        unique_tokens (dict, opcional): Un diccionario que mapea ciertos tokens a un valor único,\n",
    "            típicamente utilizado para reemplazar tokens poco frecuentes por un marcador como '<UNK>'.\n",
    "            Por defecto es None.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de n-gramas, donde cada n-grama se representa como una tupla de palabras.\n",
    "\n",
    "    Notas:\n",
    "        - Si se proporciona `unique_tokens`, cualquier palabra en la oración que aparezca en \n",
    "          `unique_tokens` será reemplazada según el mapeo en `unique_tokens`.\n",
    "        - La función devuelve n-gramas en forma de un generador para ahorrar memoria, \n",
    "          especialmente útil al procesar grandes corpus.\n",
    "    \"\"\"\n",
    "    words = sentence.split()  # Divide la oración en palabras\n",
    "    \n",
    "    if unique_tokens:\n",
    "        # Genera n-gramas usando un generador para reemplazar las palabras según el diccionario unique_tokens\n",
    "        return (tuple([unique_tokens.get((w,), w) for w in words[i:i+n]]) \n",
    "                for i in range(len(words) - n + 1))\n",
    "    \n",
    "    # Genera los n-gramas usando un generador sin realizar reemplazos\n",
    "    return (tuple(words[i:i+n]) for i in range(len(words) - n + 1) if len(words[i:i+n]) == n)\n",
    "\n",
    "def create_ngram_model(n_gram: int, text_corpus: Series)-> tuple[Counter[int], defaultdict[int]]:\n",
    "    \"\"\"\n",
    "    Crea un modelo de n-gramas a partir de un corpus de texto.\n",
    "\n",
    "    Args:\n",
    "        n_gram (int): El tamaño de los n-gramas a generar.\n",
    "        text_corpus (pd.Series): Una serie de Pandas que contiene el corpus de texto, \n",
    "                                 donde cada entrada es una oración o texto.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dos elementos:\n",
    "            - ngram_counts (Counter): Un contador que almacena las frecuencias de los n-gramas en el corpus.\n",
    "            - final_unigram (defaultdict): Un diccionario con los (n-1)-gramas más frecuentes y su conteo, \n",
    "                                           con un marcador especial `<UNK>` para los menos frecuentes.\n",
    "\n",
    "    Notas:\n",
    "        - La función primero cuenta los (n-1)-gramas para identificar los tokens únicos que serán \n",
    "          reemplazados por `<UNK>`.\n",
    "        - Luego, se cuentan los n-gramas completos utilizando los reemplazos identificados.\n",
    "    \"\"\"    \n",
    "\n",
    "    # Cuenta las frecuencias de (n-1)-gramas en el corpus de texto\n",
    "    unigram_counts = Counter(chain.from_iterable(\n",
    "        text_corpus.apply(lambda x: create_ngrams(x, n=n_gram-1, unique_tokens=None))\n",
    "    ))\n",
    "\n",
    "    final_unigram = defaultdict(int)\n",
    "    unique_tokens = defaultdict(int)\n",
    "\n",
    "    # Identifica tokens únicos y construir el diccionario final de 'unigrama'\n",
    "    for ngram, count in unigram_counts.items():\n",
    "        if '' in ngram:\n",
    "            continue\n",
    "        if count < 2:\n",
    "            unique_tokens[ngram] = ('<UNK>',) * (n_gram-1)\n",
    "        else:\n",
    "            final_unigram[ngram] = count\n",
    "\n",
    "    final_unigram[('<UNK>',)*(n_gram-1)] = len(unique_tokens)\n",
    "\n",
    "    # Cuenta las frecuencias de los n-gramas completos usando los reemplazos identificados\n",
    "    ngram_counts = Counter(chain.from_iterable(\n",
    "        text_corpus.apply(lambda x: create_ngrams(x, n=n_gram, unique_tokens=unique_tokens))\n",
    "    ))\n",
    "\n",
    "    return ngram_counts, final_unigram\n",
    "\n",
    "def save_ngram_model(ngram_counts, model_name):\n",
    "    \"\"\"\n",
    "    Guarda el modelo de n-gramas en un archivo JSON.\n",
    "\n",
    "    Args:\n",
    "        ngram_counts (Counter): Los n-gramas y sus conteos.\n",
    "        model_name (str): Nombre del archivo donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    with open(f'./data/ngram_models/{model_name}.json', 'w') as f:\n",
    "        json.dump(ngram_counts, f)\n",
    "    print(f\"{model_name} saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d18dc-320e-482b-8eaa-eb61d8af4eb0",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">No entiendo para qué funciona o cómo debo usar la siguiente función. En mi extensión del código de Raúl no la uso.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1e6cb-dfae-4fda-a254-6f08acf7f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(token_text: list[str], n_gram: int, \n",
    "                         final_unigram: dict, ngram_counts: dict) -> float:\n",
    "    \"\"\"\n",
    "    Estima la probabilidad de un n-grama dado usando un modelo de n-gramas.\n",
    "\n",
    "    Args:\n",
    "        token_text (list[str]): La secuencia de tokens para la cual se desea estimar la probabilidad.\n",
    "        n_gram (int): El tamaño del n-grama.\n",
    "        final_unigram (dict): El diccionario que contiene las frecuencias de los (n-1)-gramas.\n",
    "        ngram_counts (dict): El diccionario que contiene las frecuencias de los n-gramas.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si el número de tokens en `token_text` no coincide con `n_gram`.\n",
    "\n",
    "    Returns:\n",
    "        float: La probabilidad estimada del n-grama dado.\n",
    "\n",
    "    Notas:\n",
    "        - La función utiliza suavizado de Laplace para evitar probabilidades de cero.\n",
    "        - Si el prefijo (n-1)-grama no se encuentra en `final_unigram`, se reemplaza con (`<UNK>`,)* (n_gram-1).\n",
    "    \"\"\"    \n",
    "    \n",
    "    if len(token_text) != n_gram:\n",
    "        raise ValueError(f'El texto de entrada debe tener {n_gram} tokens')\n",
    "    \n",
    "    # Reemplaza el prefijo con `<UNK>` si no se encuentra en final_unigram\n",
    "    if not final_unigram.get(tuple(token_text[:n_gram-1])):\n",
    "        token_text = list(token_text)\n",
    "        token_text[:n_gram] = ('<UNK>',) * (n_gram-1)\n",
    "        token_text = tuple(token_text)\n",
    "\n",
    "    # Suavizado de Laplace\n",
    "    # Probabilidad del (n-1)-grama más tamaño del vocabulario\n",
    "    p_wi = final_unigram.get(tuple(token_text[:n_gram-1]), 0) + len(final_unigram)\n",
    "    \n",
    "    # Probabilidad del n-grama completo dado su (n-1)-grama\n",
    "    p_w_wi = (ngram_counts.get(tuple(token_text), 1)) / p_wi\n",
    "        \n",
    "    return p_w_wi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb02d6-3b02-4544-ad4f-50b296b1cf08",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">No entiendo para qué funciona o cómo debo la función de arriba. En mi extensión del código de Raúl no la uso.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e93c22-aa58-405b-9ac8-1ffb67ff1ab5",
   "metadata": {},
   "source": [
    "## Paso 2: Procesamiento y Almacenamiento de 20 Newsgroups (20N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f372079-d8c5-4b73-937d-cd0110c03ea1",
   "metadata": {},
   "source": [
    "<span style=\"color: gray;\">Añadí (i) una columna `category` que extrae el nombre del subdirectorio para que cada documento quede categorizado de acuerdo con el *newsgroup* del que proviene, y (ii) una descripción en el comentario inicial sobre la organización de los archivos dentro de subdirectorios que representan las diferentes categorías (*newsgroups*).</span>\n",
    "\n",
    "<span style=\"color: gray;\">Sugerí crear `sentences` utilizando el tokenizador de oraciones de `nltk` para una mejor precisión (por ejemplo, para capturar signos de puntuación como \"!\" o \"?\" y no separar erróneamente tokens como \"e.g.\".</span>\n",
    "\n",
    "<span style=\"color: gray;\">Añadí código para guardar con los nombres correctos los conjuntos de entrenamiento y prueba.</span>\n",
    "\n",
    "<span style=\"color: gray;\">Añadí código para generar y guardar con los nombres correctos los modelos de N-gramas.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae385720-17cc-4d1e-b78c-b1d4a372cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene una lista de archivos en el directorio './raw_data/20news-18828/' que contiene subdirectorios para cada newsgroup\n",
    "files_20n = glob('./raw_data/20news-18828/*/*')\n",
    "\n",
    "# Cuenta el número total de archivos encontrados\n",
    "print(f\"Número total de archivos encontrados: {len(files_20n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8ea45-4fa1-468e-9e23-f86a65307118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa una lista para almacenar las filas procesadas\n",
    "df_news_rows = []\n",
    "\n",
    "# Procesa cada archivo encontrado\n",
    "for f in tqdm(files_20n):\n",
    "    # Extrae la categoría (nombre del subdirectorio) a partir de la ruta del archivo\n",
    "    category = os.path.basename(os.path.dirname(f))\n",
    "    \n",
    "    # Lee el contenido del archivo\n",
    "    txt = read_file(f)\n",
    "    \n",
    "    # Limpia el texto leído\n",
    "    txt_cln = clean_text(txt)\n",
    "    \n",
    "    # Divide el texto limpio en oraciones, añadiendo etiquetas de inicio y fin de oración\n",
    "    # Línea original de Raúl: sentences = [f'<s> {s.strip()} </s>' for s in re.split(r'\\.\\s*', txt_cln) if len(s.strip().split()) > 1]\n",
    "    # Mi sugerencia:\n",
    "    sentences = [f'<s> {s.strip()} </s>' for s in sent_tokenize(txt_cln) if len(s.strip().split()) > 1]\n",
    "    \n",
    "    # Crea un diccionario para cada oración con el texto, la fuente del archivo, la categoría y la longitud de la oración\n",
    "    df_news_rows.extend([{\n",
    "        'text': s,\n",
    "        'source': f,\n",
    "        'category': category,  # Añade la categoría como una columna adicional\n",
    "        'length': len(s.split())\n",
    "    } for s in sentences])\n",
    "\n",
    "# Crea un DataFrame a partir de las filas procesadas\n",
    "df_news = pd.DataFrame(df_news_rows)\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "print(df_news.head())\n",
    "\n",
    "# Guarda el DataFrame en un archivo Parquet para su almacenamiento eficiente\n",
    "df_news.to_parquet('./data/20news.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea9594-1f67-4c27-8398-5af58ff96762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide aleatoriamente el DataFrame en conjuntos de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "df_news_train = df_news.sample(frac=0.8, random_state=42)\n",
    "df_news_test = df_news.drop(df_news_train.index)\n",
    "\n",
    "# Guarda los conjuntos de entrenamiento y prueba en archivos Parquet separados\n",
    "df_news_train.to_parquet('./data/train_test/20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_training.parquet', index=False)\n",
    "df_news_test.to_parquet('./data/train_test/20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_testing.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f704d-5acf-47f1-8f6d-95d7031e66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera los modelos de N-gramas\n",
    "unigram_counts_20n, unigram_20n = create_ngram_model(1, df_news['text'])\n",
    "bigram_counts_20n, bigram_20n = create_ngram_model(2, df_news['text'])\n",
    "trigram_counts_20n, trigram_20n = create_ngram_model(3, df_news['text'])\n",
    "\n",
    "# Guarda los modelos de N-gramas\n",
    "save_ngram_model(unigram_counts_20n, './data/ngram_models/20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_unigrams')\n",
    "save_ngram_model(bigram_counts_20n, './data/ngram_models/20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_bigrams')\n",
    "save_ngram_model(trigram_counts_20n, './data/ngram_models/20N_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1643a7-bb89-4b96-bdd0-c60da1cd10ef",
   "metadata": {},
   "source": [
    "## Paso 3: Procesamiento y Almacenamiento de Blog Authorship Corpus (BAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31e053-0d97-4f97-b7d9-8f29c640c9e4",
   "metadata": {},
   "source": [
    "<span style=\"color: gray;\">Añadí código para guardar con los nombres correctos los conjuntos de entrenamiento y prueba.</span>\n",
    "\n",
    "<span style=\"color: gray;\">Añadí código para generar y guardar con los nombres correctos los modelos de N-gramas.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e11e87-f882-4716-a270-5210016184a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene una lista de archivos XML en el directorio './raw_data/blogs/'\n",
    "files_bac = glob('./raw_data/blogs/*')\n",
    "\n",
    "# Cuenta el número total de archivos encontrados\n",
    "len(files_bac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2c7f4-5fe5-4c5b-88d0-432793267173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa una lista para almacenar las filas procesadas\n",
    "df_bac_rows = []\n",
    "\n",
    "# Procesa cada archivo XML encontrado\n",
    "for f in tqdm(files_bac):\n",
    "    # Extraer y procesar el texto del archivo XML, y agregar las filas al DataFrame\n",
    "    df_bac_rows.extend(extract_and_process_text_from_xml(f))\n",
    "\n",
    "# Crea un DataFrame a partir de las filas procesadas\n",
    "df_bac = DataFrame(df_bac_rows)\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "print(df_bac.head())\n",
    "\n",
    "# Guarda el DataFrame en un archivo Parquet para su almacenamiento eficiente\n",
    "df_bac.to_parquet('./data/bac.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec9c4e-ecd7-453d-afc0-70724100a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide el DataFrame en conjuntos de entrenamiento (80%) y prueba (20%) de manera aleatoria\n",
    "df_bac_train = df_bac.sample(frac=0.8, random_state=42)\n",
    "df_bac_test = df_bac.drop(df_bac_train.index)\n",
    "\n",
    "# Guarda los conjuntos de entrenamiento y prueba en archivos Parquet separados\n",
    "df_bac_train.to_parquet('./data/train_test/BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_training.parquet', index=False)\n",
    "df_bac_test.to_parquet('./data/train_test/BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_testing.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7abcf0d-a688-4479-a74e-ab5d59991938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera los modelos de N-gramas\n",
    "unigram_counts_bac, unigram_bac = create_ngram_model(1, df_bac['text'])\n",
    "bigram_counts_bac, bigram_bac = create_ngram_model(2, df_bac['text'])\n",
    "trigram_counts_bac, trigram_bac = create_ngram_model(3, df_bac['text'])\n",
    "\n",
    "# Guarda los modelos de N-gramas\n",
    "save_ngram_model(unigram_counts_bac, './data/ngram_models/BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_unigrams')\n",
    "save_ngram_model(bigram_counts_bac, './data/ngram_models/BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_bigrams')\n",
    "save_ngram_model(trigram_counts_bac, './data/ngram_models/BAC_CarlosRaulDeLaRosaPeredoJhonStewarRayoMosqueraMarioGarridoCordoba_trigrams')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
