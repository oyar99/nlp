{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Este notebook presenta modelos para analisis de sentimientos para varios dominios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "from functions import create_sentiment_dataset, build_preprocess_pipeline, build_preprocess_pipeline_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del data set - Cambiar segun sea necesario\n",
    "files = glob('data/Multi Domain Sentiment/processed_acl/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>folder</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avid:1 your:1 horrible_book:1 wasted:1 use_it:...</td>\n",
       "      <td>negative</td>\n",
       "      <td>avid  your  horrible book  wasted  use it  the...</td>\n",
       "      <td>books</td>\n",
       "      <td>negative.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to_use:1 shallow:1 found:1 he_castigates:1 cas...</td>\n",
       "      <td>negative</td>\n",
       "      <td>to use  shallow  found  he castigates  castiga...</td>\n",
       "      <td>books</td>\n",
       "      <td>negative.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avid:1 your:1 horrible_book:1 wasted:1 use_it:...</td>\n",
       "      <td>negative</td>\n",
       "      <td>avid  your  horrible book  wasted  use it  the...</td>\n",
       "      <td>books</td>\n",
       "      <td>negative.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book_seriously:1 we:1 days_couldn't:1 me_tell:...</td>\n",
       "      <td>negative</td>\n",
       "      <td>book seriously  we  days couldn't  me tell  st...</td>\n",
       "      <td>books</td>\n",
       "      <td>negative.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mass:1 only:1 he:2 help:1 \"jurisfiction\":1 lik...</td>\n",
       "      <td>negative</td>\n",
       "      <td>mass  only  he  help  \"jurisfiction\"  like  wa...</td>\n",
       "      <td>books</td>\n",
       "      <td>negative.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27672</th>\n",
       "      <td>the_last:1 well:1 gets:1 the_next:1 come:1 chi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>the last  well  gets  the next  come  china an...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>unlabeled.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27673</th>\n",
       "      <td>through:1 them_ordered:1 so_cookies:1 won't_be...</td>\n",
       "      <td>positive</td>\n",
       "      <td>through  them ordered  so cookies  won't be  o...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>unlabeled.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27674</th>\n",
       "      <td>i:1 is_great:1 god-daughter:1 get:1 cooking_it...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i  is great  god-daughter  get  cooking it's  ...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>unlabeled.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>steel:5 the_edge:1 just_a:1 only_slightly:1 st...</td>\n",
       "      <td>negative</td>\n",
       "      <td>steel  the edge  just a  only slightly  straig...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>unlabeled.review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>pay:1 \"coupler\")_after:1 use_i:1 &lt;num&gt;_months:...</td>\n",
       "      <td>negative</td>\n",
       "      <td>pay  \"coupler\") after  use i  &lt;num&gt; months  my...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>unlabeled.review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27677 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                raw_text     label  \\\n",
       "0      avid:1 your:1 horrible_book:1 wasted:1 use_it:...  negative   \n",
       "1      to_use:1 shallow:1 found:1 he_castigates:1 cas...  negative   \n",
       "2      avid:1 your:1 horrible_book:1 wasted:1 use_it:...  negative   \n",
       "3      book_seriously:1 we:1 days_couldn't:1 me_tell:...  negative   \n",
       "4      mass:1 only:1 he:2 help:1 \"jurisfiction\":1 lik...  negative   \n",
       "...                                                  ...       ...   \n",
       "27672  the_last:1 well:1 gets:1 the_next:1 come:1 chi...  positive   \n",
       "27673  through:1 them_ordered:1 so_cookies:1 won't_be...  positive   \n",
       "27674  i:1 is_great:1 god-daughter:1 get:1 cooking_it...  positive   \n",
       "27675  steel:5 the_edge:1 just_a:1 only_slightly:1 st...  negative   \n",
       "27676  pay:1 \"coupler\")_after:1 use_i:1 <num>_months:...  negative   \n",
       "\n",
       "                                                    text   folder  \\\n",
       "0      avid  your  horrible book  wasted  use it  the...    books   \n",
       "1      to use  shallow  found  he castigates  castiga...    books   \n",
       "2      avid  your  horrible book  wasted  use it  the...    books   \n",
       "3      book seriously  we  days couldn't  me tell  st...    books   \n",
       "4      mass  only  he  help  \"jurisfiction\"  like  wa...    books   \n",
       "...                                                  ...      ...   \n",
       "27672  the last  well  gets  the next  come  china an...  kitchen   \n",
       "27673  through  them ordered  so cookies  won't be  o...  kitchen   \n",
       "27674  i  is great  god-daughter  get  cooking it's  ...  kitchen   \n",
       "27675  steel  the edge  just a  only slightly  straig...  kitchen   \n",
       "27676  pay  \"coupler\") after  use i  <num> months  my...  kitchen   \n",
       "\n",
       "                   file  \n",
       "0       negative.review  \n",
       "1       negative.review  \n",
       "2       negative.review  \n",
       "3       negative.review  \n",
       "4       negative.review  \n",
       "...                 ...  \n",
       "27672  unlabeled.review  \n",
       "27673  unlabeled.review  \n",
       "27674  unlabeled.review  \n",
       "27675  unlabeled.review  \n",
       "27676  unlabeled.review  \n",
       "\n",
       "[27677 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_sentiment_dataset(files)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>file</th>\n",
       "      <th>negative.review</th>\n",
       "      <th>positive.review</th>\n",
       "      <th>unlabeled.review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>folder</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>books</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>4465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dvd</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>3586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electronics</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitchen</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "file         negative.review  positive.review  unlabeled.review\n",
       "folder                                                         \n",
       "books                   1000             1000              4465\n",
       "dvd                     1000             1000              3586\n",
       "electronics             1000             1000              5681\n",
       "kitchen                 1000             1000              5945"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agrupar los reviews por categoria\n",
    "df.groupby(['folder','file']).size().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos de entrenamiento consisten de los reviews que estan marcados como positivos o negativos\n",
    "train_data = df[df.file!='unlabeled.review'].reset_index(drop=True)\n",
    "# El conjunto de pruebas consiste de los reviews que no estan marcados\n",
    "test_data = df[df.file=='unlabeled.review'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Clasificador por categoria\n",
    "\n",
    "En esta seccion se va a construir un clasificador por cada una de las 4 categorias (Books/DVD/electronics/kitchen)\n",
    "\n",
    "### TF - IDF\n",
    "\n",
    "En los siguientes clasificadores se utiliza `tf-idf` para vectorizar el texto.\n",
    "\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:08<00:26,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de clasificación para la categoría books:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.82      0.82      2201\n",
      "    positive       0.82      0.83      0.83      2264\n",
      "\n",
      "    accuracy                           0.82      4465\n",
      "   macro avg       0.82      0.82      0.82      4465\n",
      "weighted avg       0.82      0.82      0.82      4465\n",
      "\n",
      "Características más importantes para la categoría books:\n",
      "highly: 1.3807\n",
      "favorite: 1.3809\n",
      "recommend: 1.4730\n",
      "love: 1.5758\n",
      "loved: 1.5821\n",
      "best: 1.7067\n",
      "wonderful: 1.8561\n",
      "easy: 1.8678\n",
      "excellent: 2.5359\n",
      "great: 2.5664\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:17<00:17,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de clasificación para la categoría dvd:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.79      0.82      1779\n",
      "    positive       0.81      0.86      0.84      1807\n",
      "\n",
      "    accuracy                           0.83      3586\n",
      "   macro avg       0.83      0.83      0.83      3586\n",
      "weighted avg       0.83      0.83      0.83      3586\n",
      "\n",
      "Características más importantes para la categoría dvd:\n",
      "fun: 1.3184\n",
      "enjoy: 1.4308\n",
      "season: 1.4876\n",
      "family: 1.5869\n",
      "loved: 1.6472\n",
      "wonderful: 1.6526\n",
      "excellent: 1.9895\n",
      "love: 2.1912\n",
      "best: 2.5460\n",
      "great: 3.4329\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:24<00:08,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de clasificación para la categoría electronics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.84      0.84      2824\n",
      "    positive       0.84      0.85      0.85      2857\n",
      "\n",
      "    accuracy                           0.84      5681\n",
      "   macro avg       0.84      0.84      0.84      5681\n",
      "weighted avg       0.84      0.84      0.84      5681\n",
      "\n",
      "Características más importantes para la categoría electronics:\n",
      "fast: 1.5724\n",
      "highly: 1.9789\n",
      "works: 2.0644\n",
      "easy: 2.0722\n",
      "good: 2.1233\n",
      "best: 2.1383\n",
      "perfect: 2.3296\n",
      "excellent: 2.8242\n",
      "price: 3.1311\n",
      "great: 4.6393\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:30<00:00,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de clasificación para la categoría kitchen:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.87      0.86      2991\n",
      "    positive       0.86      0.85      0.86      2954\n",
      "\n",
      "    accuracy                           0.86      5945\n",
      "   macro avg       0.86      0.86      0.86      5945\n",
      "weighted avg       0.86      0.86      0.86      5945\n",
      "\n",
      "Características más importantes para la categoría kitchen:\n",
      "clean: 1.6883\n",
      "little: 1.9548\n",
      "works: 1.9760\n",
      "ve: 2.0059\n",
      "perfect: 2.2109\n",
      "excellent: 2.2450\n",
      "best: 2.6713\n",
      "love: 3.6070\n",
      "easy: 4.0915\n",
      "great: 4.4956\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Por cada categoría se crea un modelo y se analizan las características más importantes\n",
    "for cate in tqdm(train_data['folder'].unique()):\n",
    "    cate_train_data = train_data[train_data['folder'] == cate]\n",
    "    cate_test_data = test_data[test_data['folder'] == cate]\n",
    "    \n",
    "    # Construir y ajustar el pipeline de preprocesamiento\n",
    "    tfidf_pipeline = build_preprocess_pipeline('tfidf').fit(cate_train_data['text'])\n",
    "    X_train_tfidf_transformed = tfidf_pipeline.transform(cate_train_data['text'])\n",
    "    \n",
    "    # Clasificador de regresión logística\n",
    "    logistic_estimator = LogisticRegression(n_jobs=-1, random_state=42, solver='saga')\n",
    "    cate_lr = logistic_estimator.fit(X_train_tfidf_transformed, cate_train_data['label'])\n",
    "    \n",
    "    # Probar el modelo\n",
    "    X_test_tfidf_transformed = tfidf_pipeline.transform(cate_test_data['text'])\n",
    "    y_pred = cate_lr.predict(X_test_tfidf_transformed)\n",
    "    print(f\"Resultados de clasificación para la categoría {cate}:\")\n",
    "    print(classification_report(cate_test_data['label'], y_pred))\n",
    "    \n",
    "    # Obtener los nombres de las características del vectorizador\n",
    "    feature_names = tfidf_pipeline.named_steps['vectorizer'].get_feature_names_out()\n",
    "    coef = cate_lr.coef_[0]  # Coeficiente del modelo entrenado para la primera clase (positiva o negativa)\n",
    "\n",
    "    # Obtener las 10 características más importantes\n",
    "    top_features_indices = np.argsort(coef)[-10:]\n",
    "    top_features = [(feature_names[i], coef[i]) for i in top_features_indices]\n",
    "\n",
    "    print(f\"Características más importantes para la categoría {cate}:\")\n",
    "    for feature, weight in top_features:\n",
    "        print(f\"{feature}: {weight:.4f}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:09<00:29,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* books *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.82      0.82      2201\n",
      "    positive       0.83      0.82      0.82      2264\n",
      "\n",
      "    accuracy                           0.82      4465\n",
      "   macro avg       0.82      0.82      0.82      4465\n",
      "weighted avg       0.82      0.82      0.82      4465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:19<00:19,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* dvd *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.83      0.83      1779\n",
      "    positive       0.83      0.82      0.83      1807\n",
      "\n",
      "    accuracy                           0.83      3586\n",
      "   macro avg       0.83      0.83      0.83      3586\n",
      "weighted avg       0.83      0.83      0.83      3586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:26<00:08,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* electronics *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.84      0.84      2824\n",
      "    positive       0.84      0.84      0.84      2857\n",
      "\n",
      "    accuracy                           0.84      5681\n",
      "   macro avg       0.84      0.84      0.84      5681\n",
      "weighted avg       0.84      0.84      0.84      5681\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:32<00:00,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* kitchen *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.85      2991\n",
      "    positive       0.84      0.86      0.85      2954\n",
      "\n",
      "    accuracy                           0.85      5945\n",
      "   macro avg       0.85      0.85      0.85      5945\n",
      "weighted avg       0.85      0.85      0.85      5945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Por cada categoria se crea un modelo\n",
    "for cate in tqdm(train_data['folder'].unique()):\n",
    "    cate_train_data = train_data[train_data['folder']==cate]\n",
    "    cate_test_data = test_data[test_data['folder']==cate]\n",
    "    \n",
    "    # Pipeline de preprocesamiento de datos usado tambien para el notebook de 20N\n",
    "    tfidf_pipeline = build_preprocess_pipeline('tfidf').fit(cate_train_data['text'])\n",
    "    X_train_tfidf_transformed = tfidf_pipeline.transform(cate_train_data['text'])\n",
    "    \n",
    "    # Clasificador multinomial de naive bayes\n",
    "    nb_estimator = MultinomialNB(alpha=1.0)\n",
    "\n",
    "    cate_nb = nb_estimator.fit(X_train_tfidf_transformed, cate_train_data['label'])\n",
    "    \n",
    "    ## Probar el modelo y obtener las metricas\n",
    "    X_test_transformed_tfidf = tfidf_pipeline.transform(cate_test_data['text'])\n",
    "    y_pred = cate_nb.predict(X_test_transformed_tfidf)\n",
    "    \n",
    "    print(f'************* {cate} *************')\n",
    "    print(classification_report(cate_test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa un resultado postivo para cada uno de las categorias pues en todos la precision es mayor al 80% para ambos clasificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF\n",
    "\n",
    "Para los siguientes clasificadores se utiliza la frecuencia de los terminos para vectorizar el texto para usar como entrada a los modelos\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:10<00:30, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* books *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.82      0.83      2201\n",
      "    positive       0.83      0.85      0.84      2264\n",
      "\n",
      "    accuracy                           0.83      4465\n",
      "   macro avg       0.83      0.83      0.83      4465\n",
      "weighted avg       0.83      0.83      0.83      4465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:18<00:18,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* dvd *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.77      0.80      1779\n",
      "    positive       0.79      0.85      0.82      1807\n",
      "\n",
      "    accuracy                           0.81      3586\n",
      "   macro avg       0.81      0.81      0.81      3586\n",
      "weighted avg       0.81      0.81      0.81      3586\n",
      "\n",
      "************* electronics *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:26<00:08,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.85      0.86      2824\n",
      "    positive       0.85      0.86      0.86      2857\n",
      "\n",
      "    accuracy                           0.86      5681\n",
      "   macro avg       0.86      0.86      0.86      5681\n",
      "weighted avg       0.86      0.86      0.86      5681\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:33<00:00,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* kitchen *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86      2991\n",
      "    positive       0.85      0.86      0.86      2954\n",
      "\n",
      "    accuracy                           0.86      5945\n",
      "   macro avg       0.86      0.86      0.86      5945\n",
      "weighted avg       0.86      0.86      0.86      5945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:33<00:00,  8.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Por cada categoria se crea un modelo\n",
    "for cate in tqdm(train_data['folder'].unique()):\n",
    "    \n",
    "    cate_train_data = train_data[train_data['folder']==cate]\n",
    "    cate_test_data = test_data[test_data['folder']==cate]\n",
    "    \n",
    "    # Pipeline de preprocesamiento de datos usado tambien para el notebook de 20N\n",
    "    cnt_pipeline = build_preprocess_pipeline('count').fit(cate_train_data['text'])\n",
    "    X_train_cnt_transformed = cnt_pipeline.transform(cate_train_data['text'])\n",
    "    \n",
    "    # Clasificador de regresion logistica\n",
    "    logistic_estimator = LogisticRegression(n_jobs=-1, random_state=42, \n",
    "                                            class_weight=None, solver='saga',\n",
    "                                            max_iter=1000, penalty='l2',\n",
    "                                            tol=1e-2, C=1\n",
    "                                            )\n",
    "\n",
    "    cate_lr = logistic_estimator.fit(X_train_cnt_transformed, cate_train_data['label'])\n",
    "    \n",
    "    ## Probar el modelo usando el conjunto de pruebas\n",
    "    X_test_transformed_cnt = cnt_pipeline.transform(cate_test_data['text'])\n",
    "    y_pred = cate_lr.predict(X_test_transformed_cnt)\n",
    "    \n",
    "    print(f'************* {cate} *************')\n",
    "    print(classification_report(cate_test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:10<00:30, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* books *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.49      0.50      2201\n",
      "    positive       0.52      0.53      0.52      2264\n",
      "\n",
      "    accuracy                           0.51      4465\n",
      "   macro avg       0.51      0.51      0.51      4465\n",
      "weighted avg       0.51      0.51      0.51      4465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:19<00:18,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* dvd *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.54      0.52      1779\n",
      "    positive       0.50      0.45      0.47      1807\n",
      "\n",
      "    accuracy                           0.49      3586\n",
      "   macro avg       0.49      0.49      0.49      3586\n",
      "weighted avg       0.49      0.49      0.49      3586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:26<00:08,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* electronics *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.52      0.50      2824\n",
      "    positive       0.49      0.45      0.47      2857\n",
      "\n",
      "    accuracy                           0.49      5681\n",
      "   macro avg       0.49      0.49      0.49      5681\n",
      "weighted avg       0.49      0.49      0.49      5681\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:32<00:00,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* kitchen *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.84      0.86      2991\n",
      "    positive       0.84      0.87      0.86      2954\n",
      "\n",
      "    accuracy                           0.86      5945\n",
      "   macro avg       0.86      0.86      0.86      5945\n",
      "weighted avg       0.86      0.86      0.86      5945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:32<00:00,  8.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Por cada categoria se crea un modelo\n",
    "for cate in tqdm(train_data['folder'].unique()):\n",
    "    \n",
    "    cate_train_data = train_data[train_data['folder']==cate]\n",
    "    cate_test_data = test_data[test_data['folder']==cate]\n",
    "    \n",
    "    # Pipeline de preprocesamiento de datos usado tambien para el notebook de 20N\n",
    "    cnt_pipeline = build_preprocess_pipeline('count').fit(cate_train_data['text'])\n",
    "    X_train_cnt_transformed = cnt_pipeline.transform(cate_train_data['text'])\n",
    "\n",
    "    # Clasificador de Naive Bayes multinomial\n",
    "    nb_estimator = MultinomialNB(alpha=1.0)\n",
    "\n",
    "    cate_nb = nb_estimator.fit(X_train_tfidf_transformed, cate_train_data['label'])\n",
    "    \n",
    "    ## Probar el modelo con el conjunto de pruebas\n",
    "    \n",
    "    X_test_transformed_cnt = cnt_pipeline.transform(cate_test_data['text'])\n",
    "    y_pred = cate_nb.predict(X_test_transformed_cnt)\n",
    "    \n",
    "    print(f'************* {cate} *************')\n",
    "    print(classification_report(cate_test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa un peor rendimiento usando `tf` especialmente usando Naive Bayes para la categoria de electronics y dvd. Lo que podria explicarse porque la terminologia usada para las resenas de estos productos no logra ser suficiente para el analisis de sentimientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los modelos siguientes se utiliza una representacion usando un puntaje de positivo/negativo a partir de un lexicon y en base al texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresion lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:04<00:14,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* books *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.64      0.62      2201\n",
      "    positive       0.62      0.57      0.60      2264\n",
      "\n",
      "    accuracy                           0.61      4465\n",
      "   macro avg       0.61      0.61      0.61      4465\n",
      "weighted avg       0.61      0.61      0.61      4465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:09<00:09,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* dvd *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.62      0.61      1779\n",
      "    positive       0.62      0.59      0.60      1807\n",
      "\n",
      "    accuracy                           0.61      3586\n",
      "   macro avg       0.61      0.61      0.61      3586\n",
      "weighted avg       0.61      0.61      0.61      3586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:12<00:04,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* electronics *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.66      0.62      2824\n",
      "    positive       0.62      0.56      0.59      2857\n",
      "\n",
      "    accuracy                           0.61      5681\n",
      "   macro avg       0.61      0.61      0.61      5681\n",
      "weighted avg       0.61      0.61      0.61      5681\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:16<00:00,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* kitchen *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.67      0.65      2991\n",
      "    positive       0.65      0.62      0.63      2954\n",
      "\n",
      "    accuracy                           0.64      5945\n",
      "   macro avg       0.64      0.64      0.64      5945\n",
      "weighted avg       0.64      0.64      0.64      5945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Por cada categoría se crea un modelo\n",
    "for cate in tqdm(train_data['folder'].unique()):\n",
    "    \n",
    "    cate_train_data = train_data[train_data['folder'] == cate]\n",
    "    cate_test_data = test_data[test_data['folder'] == cate]\n",
    "    \n",
    "    # Construir y ajustar el pipeline de preprocesamiento\n",
    "    pipeline = build_preprocess_pipeline_lexicon('data/lexicon/SentiWordNet_3.0.0.txt')\n",
    "    X_train_sentiment = pipeline.fit_transform(cate_train_data['text'])\n",
    "\n",
    "    # Clasificador de regresion logistica\n",
    "    logistic_estimator = LogisticRegression()\n",
    "    cate_lr = logistic_estimator.fit(X_train_sentiment, cate_train_data['label'])\n",
    "    \n",
    "    # Probar el modelo con el conjunto de pruebas\n",
    "    X_test_sentiment = pipeline.transform(cate_test_data['text'])\n",
    "    y_pred = cate_lr.predict(X_test_sentiment)\n",
    "    \n",
    "    print(f'************* {cate} *************')\n",
    "    print(classification_report(cate_test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:04<00:14,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* books *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.66      0.62      2201\n",
      "    positive       0.62      0.55      0.58      2264\n",
      "\n",
      "    accuracy                           0.60      4465\n",
      "   macro avg       0.60      0.60      0.60      4465\n",
      "weighted avg       0.60      0.60      0.60      4465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:09<00:09,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* dvd *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.71      0.64      1779\n",
      "    positive       0.63      0.49      0.55      1807\n",
      "\n",
      "    accuracy                           0.60      3586\n",
      "   macro avg       0.60      0.60      0.59      3586\n",
      "weighted avg       0.60      0.60      0.59      3586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:13<00:04,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* electronics *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.71      0.64      2824\n",
      "    positive       0.63      0.49      0.55      2857\n",
      "\n",
      "    accuracy                           0.60      5681\n",
      "   macro avg       0.60      0.60      0.59      5681\n",
      "weighted avg       0.60      0.60      0.59      5681\n",
      "\n",
      "************* kitchen *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:16<00:00,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.70      0.66      2991\n",
      "    positive       0.65      0.58      0.62      2954\n",
      "\n",
      "    accuracy                           0.64      5945\n",
      "   macro avg       0.64      0.64      0.64      5945\n",
      "weighted avg       0.64      0.64      0.64      5945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Por cada categoría se crea un modelo\n",
    "for cate in tqdm(train_data['folder'].unique()):\n",
    "    # Extraer los datos de entrenamiento y prueba para la categoría específica\n",
    "    cate_train_data = train_data[train_data['folder'] == cate]\n",
    "    cate_test_data = test_data[test_data['folder'] == cate]\n",
    "    \n",
    "    # Construir y ajustar el pipeline de preprocesamiento basado en léxicos\n",
    "    pipeline = build_preprocess_pipeline_lexicon('data/lexicon/SentiWordNet_3.0.0.txt')\n",
    "    X_train_lex_transformed = pipeline.fit_transform(cate_train_data['text'])\n",
    "    y_train = cate_train_data['label']\n",
    "    \n",
    "    # Entrenar el modelo Naive Bayes\n",
    "    nb_estimator = GaussianNB()\n",
    "    cate_nb = nb_estimator.fit(X_train_lex_transformed, y_train)\n",
    "    \n",
    "    # Transformar los datos de prueba y realizar predicciones\n",
    "    X_test_lex_transformed = pipeline.transform(cate_test_data['text'])\n",
    "    y_test = cate_test_data['label']\n",
    "    y_pred = cate_nb.predict(X_test_lex_transformed)\n",
    "    \n",
    "    # Imprimir el reporte de clasificación para cada categoría\n",
    "    print(f'************* {cate} *************')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Clasificador para todas las categorias\n",
    "\n",
    "Ahora construimos un solo clasificador para todas las categorias donde se determina si el review es positivo o negativo unicamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se construye el pipeline de procesamiento para todo el conjunto de datos de entrenamiento\n",
    "tfidf_pipeline = build_preprocess_pipeline('tfidf').fit(train_data['text'])\n",
    "X_train_tfidf_transformed = tfidf_pipeline.transform(train_data['text'])\n",
    "\n",
    "cnt_pipeline = build_preprocess_pipeline('count').fit(train_data['text'])\n",
    "X_train_cnt_transformed = cnt_pipeline.transform(train_data['text'])\n",
    "\n",
    "lex_pipeline = build_preprocess_pipeline_lexicon('data/lexicon/SentiWordNet_3.0.0.txt')\n",
    "X_train_lex_transformed = lex_pipeline.fit_transform(train_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF\n",
    "\n",
    "En esta seccion usamos `tf-idf` como metodo de vectorizacion del texto\n",
    "\n",
    "### Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85      9795\n",
      "    positive       0.85      0.85      0.85      9882\n",
      "\n",
      "    accuracy                           0.85     19677\n",
      "   macro avg       0.85      0.85      0.85     19677\n",
      "weighted avg       0.85      0.85      0.85     19677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se utiliza clasificador de regresion logistica\n",
    "logistic_estimator = LogisticRegression(n_jobs=-1, random_state=42, \n",
    "                                        class_weight=None, solver='saga',\n",
    "                                        max_iter=1000, penalty='l2',\n",
    "                                        tol=1e-2, C=1\n",
    "                                        )\n",
    "\n",
    "cate_lr = logistic_estimator.fit(X_train_tfidf_transformed, train_data['label'])\n",
    "\n",
    "## Se prueba el modelo y arrojan los resultados \n",
    "X_test_transformed_tfidf = tfidf_pipeline.transform(test_data['text'])\n",
    "y_pred = cate_lr.predict(X_test_transformed_tfidf)\n",
    "\n",
    "print(classification_report(test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características más importantes para clasificar como positivo:\n",
      "wonderful: 2.9428\n",
      "works: 3.0035\n",
      "highly: 3.0108\n",
      "price: 3.2014\n",
      "perfect: 3.7345\n",
      "love: 4.4127\n",
      "easy: 4.7173\n",
      "best: 5.0155\n",
      "excellent: 5.6955\n",
      "great: 7.1480\n",
      "\n",
      "Características más importantes para clasificar como negativo:\n",
      "waste: -4.8423\n",
      "bad: -4.5380\n",
      "disappointed: -4.2791\n",
      "worst: -4.1176\n",
      "poor: -4.0052\n",
      "boring: -3.6060\n",
      "disappointing: -3.3968\n",
      "terrible: -3.1828\n",
      "disappointment: -3.0579\n",
      "return: -2.9255\n"
     ]
    }
   ],
   "source": [
    "# Obtener los nombres de las características del vectorizador\n",
    "feature_names = tfidf_pipeline.named_steps['vectorizer'].get_feature_names_out()\n",
    "coef = cate_lr.coef_[0]  # Hay un solo conjunto de coeficientes para clasificación binaria\n",
    "\n",
    "# Obtener las 10 características más importantes (positivas y negativas)\n",
    "top_positive_indices = np.argsort(coef)[-10:]  # Las características más importantes para la clase positiva\n",
    "top_negative_indices = np.argsort(coef)[:10]   # Las características más importantes para la clase negativa\n",
    "\n",
    "top_positive_features = [(feature_names[i], coef[i]) for i in top_positive_indices]\n",
    "top_negative_features = [(feature_names[i], coef[i]) for i in top_negative_indices]\n",
    "\n",
    "print(\"Características más importantes para clasificar como positivo:\")\n",
    "for feature, weight in top_positive_features:\n",
    "    print(f\"{feature}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nCaracterísticas más importantes para clasificar como negativo:\")\n",
    "for feature, weight in top_negative_features:\n",
    "    print(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85      9795\n",
      "    positive       0.85      0.85      0.85      9882\n",
      "\n",
      "    accuracy                           0.85     19677\n",
      "   macro avg       0.85      0.85      0.85     19677\n",
      "weighted avg       0.85      0.85      0.85     19677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clasificador de Naive Bayes multinomial\n",
    "nb_estimator = MultinomialNB(alpha=1.0)\n",
    "\n",
    "cate_nb = nb_estimator.fit(X_train_tfidf_transformed, train_data['label'])\n",
    "\n",
    "## Se prueba el modelo y se imprimen los resultados\n",
    "y_pred = cate_lr.predict(X_test_transformed_tfidf)\n",
    "\n",
    "print(classification_report(test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos modelos arrojan modelos muy buenos donde la precision es del `0.85`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF\n",
    "\n",
    "Ahora se utiliza una matriz con la frecuencia de los terminos como entrada de los modelos\n",
    "\n",
    "### Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.84      0.85      9795\n",
      "    positive       0.84      0.86      0.85      9882\n",
      "\n",
      "    accuracy                           0.85     19677\n",
      "   macro avg       0.85      0.85      0.85     19677\n",
      "weighted avg       0.85      0.85      0.85     19677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clasificador de regresion logistica\n",
    "logistic_estimator = LogisticRegression(n_jobs=-1, random_state=42, \n",
    "                                        class_weight=None, solver='saga',\n",
    "                                        max_iter=1000, penalty='l2',\n",
    "                                        tol=1e-2, C=1\n",
    "                                        )\n",
    "\n",
    "cate_lr = logistic_estimator.fit(X_train_cnt_transformed, train_data['label'])\n",
    "\n",
    "## Probar el modelo\n",
    "X_test_transformed_cnt = cnt_pipeline.transform(test_data['text'])\n",
    "y_pred = cate_lr.predict(X_test_transformed_cnt)\n",
    "\n",
    "print(classification_report(test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.82      0.82      9795\n",
      "    positive       0.83      0.83      0.83      9882\n",
      "\n",
      "    accuracy                           0.83     19677\n",
      "   macro avg       0.83      0.83      0.83     19677\n",
      "weighted avg       0.83      0.83      0.83     19677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clasificador de Naive Bayes multinomial\n",
    "logistic_estimator = MultinomialNB(alpha=1.0)\n",
    "\n",
    "cate_lr = logistic_estimator.fit(X_train_cnt_transformed, train_data['label'])\n",
    "\n",
    "## Test the model\n",
    "y_pred = cate_lr.predict(X_test_transformed_cnt)\n",
    "\n",
    "print(classification_report(test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Naive Bayes se obtiene un resultado ligeramente peor comparado con el resto de los modelos, aunque no se evidencia la misma dificultad que al clasificar por categoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicons\n",
    "\n",
    "Ahora usamos caracteristicas extraidas del lexicon que corresponden a un puntaje de positivo/negativo para cada review.\n",
    "\n",
    "### Regresion logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.58      0.61      9795\n",
      "    positive       0.62      0.66      0.64      9882\n",
      "\n",
      "    accuracy                           0.62     19677\n",
      "   macro avg       0.62      0.62      0.62     19677\n",
      "weighted avg       0.62      0.62      0.62     19677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clasificador de regresion logistica\n",
    "logistic_estimator = LogisticRegression(n_jobs=-1, random_state=42, \n",
    "                                        class_weight=None, solver='saga',\n",
    "                                        max_iter=1000, penalty='l2',\n",
    "                                        tol=1e-2, C=1\n",
    "                                        )\n",
    "\n",
    "cate_lr = logistic_estimator.fit(X_train_lex_transformed, train_data['label'])\n",
    "\n",
    "## Probar el modelo\n",
    "X_test_transformed_lex = lex_pipeline.transform(test_data['text'])\n",
    "y_pred = cate_lr.predict(X_test_transformed_lex)\n",
    "\n",
    "print(classification_report(test_data['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el feature de score que se extrae del texto a partir del lexicon es bastante sencillo, no se obtiene los mismos resultados que usando bolsa de palabras, pero si existe la posibilidad de mejorar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Clasificador Consolidado *************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.70      0.64      9795\n",
      "    positive       0.64      0.52      0.57      9882\n",
      "\n",
      "    accuracy                           0.61     19677\n",
      "   macro avg       0.61      0.61      0.61     19677\n",
      "weighted avg       0.61      0.61      0.61     19677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construir y ajustar el pipeline de preprocesamiento basado en léxicos para el conjunto consolidado\n",
    "pipeline = build_preprocess_pipeline_lexicon('data/lexicon/SentiWordNet_3.0.0.txt')\n",
    "X_train_lex_transformed = pipeline.fit_transform(train_data['text'])\n",
    "y_train = train_data['label']\n",
    "\n",
    "# Entrenar el modelo Naive Bayes en todo el conjunto de datos de entrenamiento\n",
    "nb_estimator = GaussianNB()\n",
    "consolidated_nb = nb_estimator.fit(X_train_lex_transformed, y_train)\n",
    "\n",
    "# Transformar los datos de prueba y realizar predicciones\n",
    "X_test_lex_transformed = pipeline.transform(test_data['text'])\n",
    "y_test = test_data['label']\n",
    "y_pred = consolidated_nb.predict(X_test_lex_transformed)\n",
    "\n",
    "# Imprimir el reporte de clasificación para el conjunto consolidado\n",
    "print('************* Clasificador Consolidado *************')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
